#!/usr/bin/env python3
"""
URLåŒ¹é…æ¨¡å—
å®ç°URLç›¸ä¼¼åº¦åŒ¹é…ç®—æ³•ï¼Œæ”¯æŒå¤šç§åŒ¹é…ç­–ç•¥
"""

import re
from typing import Dict, List, Tuple, Optional
from urllib.parse import urlparse, parse_qs
from difflib import SequenceMatcher


class URLMatcher:
    """URLåŒ¹é…å™¨ï¼Œæ”¯æŒå¤šç§åŒ¹é…ç­–ç•¥"""

    def __init__(self):
        self.similarity_threshold = 0.6  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆè°ƒæ•´åé€‚åˆæ–°çš„æƒé‡åˆ†é…ï¼‰
        self.base_url_weight = 0.6      # åŸºç¡€URLæƒé‡
        self.params_weight = 0.4        # å‚æ•°æƒé‡

    def calculate_string_similarity(self, str1: str, str2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦
        ä½¿ç”¨SequenceMatcherç®—æ³•ï¼ˆåŸºäºRatcliff/Obershelpç®—æ³•ï¼‰

        Args:
            str1: å­—ç¬¦ä¸²1
            str2: å­—ç¬¦ä¸²2

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0.0 - 1.0)
        """
        if not str1 or not str2:
            return 0.0

        # æ ‡å‡†åŒ–å­—ç¬¦ä¸²ï¼ˆè½¬å°å†™ï¼Œå»é™¤å¤šä½™ç©ºæ ¼ï¼‰
        str1_norm = str1.lower().strip()
        str2_norm = str2.lower().strip()

        if str1_norm == str2_norm:
            return 1.0

        # ä½¿ç”¨SequenceMatcherè®¡ç®—ç›¸ä¼¼åº¦
        matcher = SequenceMatcher(None, str1_norm, str2_norm)
        return matcher.ratio()

    def calculate_jaccard_similarity(self, str1: str, str2: str) -> float:
        """
        è®¡ç®—Jaccardç›¸ä¼¼åº¦ï¼ˆåŸºäºå­—ç¬¦é›†åˆï¼‰

        Args:
            str1: å­—ç¬¦ä¸²1
            str2: å­—ç¬¦ä¸²2

        Returns:
            Jaccardç›¸ä¼¼åº¦åˆ†æ•° (0.0 - 1.0)
        """
        if not str1 or not str2:
            return 0.0

        set1 = set(str1.lower())
        set2 = set(str2.lower())

        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))

        return intersection / union if union > 0 else 0.0

    def calculate_levenshtein_similarity(self, str1: str, str2: str) -> float:
        """
        è®¡ç®—åŸºäºç¼–è¾‘è·ç¦»çš„ç›¸ä¼¼åº¦

        Args:
            str1: å­—ç¬¦ä¸²1
            str2: å­—ç¬¦ä¸²2

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0.0 - 1.0)
        """
        if not str1 or not str2:
            return 0.0

        if str1 == str2:
            return 1.0

        # è®¡ç®—ç¼–è¾‘è·ç¦»
        len1, len2 = len(str1), len(str2)
        dp = [[0] * (len2 + 1) for _ in range(len1 + 1)]

        for i in range(len1 + 1):
            dp[i][0] = i
        for j in range(len2 + 1):
            dp[0][j] = j

        for i in range(1, len1 + 1):
            for j in range(1, len2 + 1):
                if str1[i-1] == str2[j-1]:
                    dp[i][j] = dp[i-1][j-1]
                else:
                    dp[i][j] = min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]) + 1

        max_len = max(len1, len2)
        return 1.0 - (dp[len1][len2] / max_len) if max_len > 0 else 0.0

    def extract_base_url(self, url: str) -> str:
        """
        æå–URLä¸­é—®å·å‰çš„éƒ¨åˆ†ï¼ˆåŸºç¡€URLï¼‰

        Args:
            url: å®Œæ•´URL

        Returns:
            åŸºç¡€URLï¼ˆé—®å·å‰çš„éƒ¨åˆ†ï¼‰
        """
        if '?' in url:
            return url.split('?')[0]
        return url

    def parse_url_components(self, url: str) -> Dict[str, any]:
        """
        è§£æURLç»„ä»¶

        Args:
            url: å®Œæ•´URL

        Returns:
            URLç»„ä»¶å­—å…¸
        """
        parsed = urlparse(url)

        return {
            'scheme': parsed.scheme,
            'netloc': parsed.netloc,
            'path': parsed.path,
            'params': parsed.params,
            'query': parsed.query,
            'fragment': parsed.fragment,
            'base_url': f"{parsed.scheme}://{parsed.netloc}{parsed.path}",
            'query_params': parse_qs(parsed.query) if parsed.query else {}
        }

    def match_base_url_exact(self, url1: str, url2: str) -> bool:
        """
        æ£€æŸ¥ä¸¤ä¸ªURLçš„åŸºç¡€éƒ¨åˆ†ï¼ˆé—®å·å‰ï¼‰æ˜¯å¦å®Œå…¨åŒ¹é…

        Args:
            url1: URL 1
            url2: URL 2

        Returns:
            æ˜¯å¦å®Œå…¨åŒ¹é…
        """
        base1 = self.extract_base_url(url1)
        base2 = self.extract_base_url(url2)
        return base1 == base2

    def calculate_url_similarity(self, url1: str, url2: str) -> Dict[str, float]:
        """
        è®¡ç®—ä¸¤ä¸ªURLçš„ç»¼åˆç›¸ä¼¼åº¦

        Args:
            url1: URL 1
            url2: URL 2

        Returns:
            ç›¸ä¼¼åº¦ç»“æœå­—å…¸
        """
        # è§£æURLç»„ä»¶
        comp1 = self.parse_url_components(url1)
        comp2 = self.parse_url_components(url2)

        # 1. åŸºç¡€URLç›¸ä¼¼åº¦ï¼ˆå¤šç§ç®—æ³•ï¼‰
        base_similarity_seq = self.calculate_string_similarity(comp1['base_url'], comp2['base_url'])
        base_similarity_jaccard = self.calculate_jaccard_similarity(comp1['base_url'], comp2['base_url'])
        base_similarity_levenshtein = self.calculate_levenshtein_similarity(comp1['base_url'], comp2['base_url'])

        # åŸºç¡€URLç»¼åˆç›¸ä¼¼åº¦ï¼ˆå–å¹³å‡å€¼ï¼‰
        base_similarity = (base_similarity_seq + base_similarity_jaccard + base_similarity_levenshtein) / 3

        # 2. æŸ¥è¯¢å‚æ•°ç›¸ä¼¼åº¦ï¼ˆçº¯å­—ç¬¦ä¸²å¯¹æ¯”ï¼Œç”¨äºè¾…åŠ©å‚è€ƒï¼‰
        query_similarity = self.calculate_string_similarity(comp1['query'], comp2['query'])

        # 2.1 è¯­ä¹‰åŒ–æŸ¥è¯¢ç›¸ä¼¼åº¦ï¼šå¿½ç•¥æ˜“å˜å‚æ•°åï¼Œè¯„ä¼°é”®é‡åˆä¸å€¼ä¸€è‡´
        import re
        params1 = comp1['query_params'] or {}
        params2 = comp2['query_params'] or {}

        # å¿½ç•¥æ˜“å˜å‚æ•°ï¼ˆé€šç”¨æ­£åˆ™ï¼Œä¸åšé“¶è¡Œå®šåˆ¶ï¼‰
        volatile_key_patterns = [
            r"(?i)(^|_|\b)(ts|time|timestamp|session|sid|sessionid|nonce|rand|random)(_|\b|$)",
            r"(?i)^utm_[a-z0-9_]+$",
            r"(?i)^cachebust$",
        ]
        def is_volatile(key: str) -> bool:
            k = key or ""
            for pat in volatile_key_patterns:
                if re.search(pat, k):
                    return True
            return False

        keys1 = {k for k in params1.keys() if not is_volatile(k)}
        keys2 = {k for k in params2.keys() if not is_volatile(k)}
        union_keys = keys1 | keys2
        inter_keys = keys1 & keys2

        # é”®é‡åˆåº¦
        key_overlap = (len(inter_keys) / len(union_keys)) if union_keys else (1.0 if not comp1['query'] and not comp2['query'] else 0.0)

        # å€¼ä¸€è‡´åº¦ï¼ˆå¯¹äº¤é›†é”®ï¼Œæ¯”è¾ƒå€¼åˆ—è¡¨çš„é›†åˆæ˜¯å¦ç›¸ç­‰ï¼Œä½¿ç”¨â€œå€¼å¤æ‚åº¦åŠ æƒâ€ï¼‰
        def _norm_vals(v):
            try:
                if isinstance(v, list):
                    return tuple(sorted(str(x) for x in v))
                return (str(v),)
            except Exception:
                return (str(v),)

        def _estimate_pair_weight(vals_tuple_1, vals_tuple_2) -> float:
            def _estimate_vals_weight(vals_tuple) -> float:
                # ä»¥â€œæœ€é•¿å…ƒç´ é•¿åº¦ + è¯å…ƒæ•°â€ä¼°è®¡å¤æ‚åº¦æƒé‡ï¼ŒèŒƒå›´å¤§è‡´åœ¨[1, 3]
                try:
                    elements = list(vals_tuple)
                except Exception:
                    elements = [str(vals_tuple)]
                if not elements:
                    return 1.0
                max_len = 0
                token_count = 0
                for s in elements:
                    s = str(s)
                    if len(s) > max_len:
                        max_len = len(s)
                    token_count += len(re.findall(r"[A-Za-z0-9]+", s))
                # é•¿åº¦åˆ†ï¼šmax_len/20 ä¸Šé™ 1.0ï¼›è¯å…ƒåˆ†ï¼štoken_count/10 ä¸Šé™ 1.0ï¼›åŸºçº¿ 1.0
                length_component = min(1.0, max_len / 20.0)
                token_component = min(1.0, token_count / 10.0)
                return 1.0 + length_component + token_component

            w1 = _estimate_vals_weight(vals_tuple_1)
            w2 = _estimate_vals_weight(vals_tuple_2)
            return max(w1, w2)

        total_weight = 0.0
        equal_weight = 0.0
        for k in inter_keys:
            v1 = _norm_vals(params1.get(k))
            v2 = _norm_vals(params2.get(k))
            pair_weight = _estimate_pair_weight(v1, v2)
            total_weight += pair_weight
            if v1 == v2:
                equal_weight += pair_weight
        if inter_keys and total_weight > 0:
            value_match = equal_weight / total_weight
        else:
            value_match = 1.0 if not comp1['query'] and not comp2['query'] else 0.0

        # 3. å®Œæ•´URLç›¸ä¼¼åº¦
        full_similarity_seq = self.calculate_string_similarity(url1, url2)
        full_similarity_jaccard = self.calculate_jaccard_similarity(url1, url2)
        full_similarity_levenshtein = self.calculate_levenshtein_similarity(url1, url2)

        full_similarity = (full_similarity_seq + full_similarity_jaccard + full_similarity_levenshtein) / 3

        # 4. åŸºç¡€URLå®Œå…¨åŒ¹é…æ£€æŸ¥
        base_exact_match = self.match_base_url_exact(url1, url2)

        # 5. ç»¼åˆè¯„åˆ† - ğŸ¯ ä¿®å¤ï¼šé™ä½åŸºç¡€URLæƒé‡ï¼Œæé«˜æŸ¥è¯¢å‚æ•°æƒé‡
        if base_exact_match:
            # åŸºç¡€URLä¸€è‡´ï¼š
            # - æ— queryï¼šé«˜åŸºçº¿
            # - æœ‰queryï¼šä¾æ®é”®é‡åˆä¸å€¼ä¸€è‡´åº¦è¿›è¡Œè¯­ä¹‰è¯„åˆ†ï¼Œå¿½ç•¥æ˜“å˜å‚æ•°
            if not comp1['query'] and not comp2['query']:
                composite_score = 0.9
            else:
                composite_score = 0.2 + (0.4 * key_overlap) + (0.4 * value_match) + (0.2 * query_similarity)
                # æ•´ä½“æŸ¥è¯¢é•¿åº¦å·®å¼‚æƒ©ç½šï¼ˆè¶Šä¸ä¸€è‡´ï¼Œæ‰£åˆ†è¶Šå¤šï¼Œæœ€å¤šæ‰£ 0.15ï¼‰
                qlen1 = len(comp1['query']) if comp1['query'] else 0
                qlen2 = len(comp2['query']) if comp2['query'] else 0
                if qlen1 > 0 and qlen2 > 0:
                    length_disparity = abs(qlen1 - qlen2) / max(qlen1, qlen2)
                else:
                    length_disparity = 0.0
                composite_score -= 0.15 * length_disparity
                if composite_score < 0.0:
                    composite_score = 0.0
                if composite_score > 1.0:
                    composite_score = 1.0
        else:
            # å¦åˆ™ä½¿ç”¨åŠ æƒå¹³å‡
            composite_score = (base_similarity * self.base_url_weight +
                             query_similarity * self.params_weight)

        # å‘½ä¸­åˆ¤æ–­ï¼šä»…ä»¥ç»¼åˆåˆ†ä¸é˜ˆå€¼åˆ¤æ–­
        is_match_flag = (composite_score >= self.similarity_threshold)

        return {
            'base_similarity': base_similarity,
            'query_similarity': query_similarity,
            'full_similarity': full_similarity,
            'base_exact_match': base_exact_match,
            'composite_score': composite_score,
            'is_match': is_match_flag,
            'details': {
                'base_similarity_seq': base_similarity_seq,
                'base_similarity_jaccard': base_similarity_jaccard,
                'base_similarity_levenshtein': base_similarity_levenshtein,
                'full_similarity_seq': full_similarity_seq,
                'full_similarity_jaccard': full_similarity_jaccard,
                'full_similarity_levenshtein': full_similarity_levenshtein,
                'key_overlap': key_overlap,
                'value_match': value_match
            }
        }

    def find_best_match(self, target_url: str, candidate_urls: List[str]) -> Optional[Tuple[str, Dict[str, float]]]:
        """
        åœ¨å€™é€‰URLåˆ—è¡¨ä¸­æ‰¾åˆ°æœ€ä½³åŒ¹é…

        Args:
            target_url: ç›®æ ‡URL
            candidate_urls: å€™é€‰URLåˆ—è¡¨

        Returns:
            æœ€ä½³åŒ¹é…çš„URLå’Œç›¸ä¼¼åº¦ç»“æœï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…è¿”å›None
        """
        best_match = None
        best_score = 0.0
        best_result = None

        for candidate_url in candidate_urls:
            result = self.calculate_url_similarity(target_url, candidate_url)

            if result['composite_score'] > best_score:
                best_score = result['composite_score']
                best_match = candidate_url
                best_result = result

        # åªè¿”å›è¶…è¿‡é˜ˆå€¼çš„åŒ¹é…
        if best_score >= self.similarity_threshold:
            return best_match, best_result

        return None

    def set_similarity_threshold(self, threshold: float):
        """è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼"""
        self.similarity_threshold = max(0.0, min(1.0, threshold))

    def set_weights(self, base_url_weight: float, params_weight: float):
        """è®¾ç½®æƒé‡"""
        total = base_url_weight + params_weight
        if total > 0:
            self.base_url_weight = base_url_weight / total
            self.params_weight = params_weight / total


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    matcher = URLMatcher()

    # æµ‹è¯•URL
    url1 = "https://www.cmbwinglungbank.com/ibanking/McpCSReqServlet?dse_operationName=NbMsgMsgboxDspProc&dse_processorState=initial&dse_nextEventName=initial.logon&dse_sessionId=AAARCXAYDPFGIYHQBQAJCIBUHGDGIJHZABFNAQHD&mcp_language=cn&dse_pageId=1&dse_parentContextName="
    url2 = "https://www.cmbwinglungbank.com/ibanking/McpCSReqServlet?dse_operationName=NbMsgMsgboxDspProc&dse_processorState=initial&dse_nextEventName=initial.logon&dse_sessionId=DIFFERENT_SESSION_ID&mcp_language=cn&dse_pageId=1&dse_parentContextName="
    url3 = "https://www.cmbwinglungbank.com/ibanking/McpCSReqServlet?jspName=/accountmanagement/NbBkgWel.jsp&dse_sessionId=ANOTHER_SESSION&mcp_funcId=Acm&mcp_language=cn"
    url4 = "https://example.com/api/different"

    print("ğŸ§ª URLåŒ¹é…æµ‹è¯•")
    print("=" * 50)

    # æµ‹è¯•1: ç›¸åŒåŸºç¡€URLï¼Œä¸åŒå‚æ•°
    result = matcher.calculate_url_similarity(url1, url2)
    print(f"æµ‹è¯•1 - ç›¸åŒåŸºç¡€URLï¼Œä¸åŒå‚æ•°:")
    print(f"  åŸºç¡€URLåŒ¹é…: {result['base_exact_match']}")
    print(f"  ç»¼åˆè¯„åˆ†: {result['composite_score']:.3f}")
    print(f"  æ˜¯å¦åŒ¹é…: {result['is_match']}")
    print()

    # æµ‹è¯•2: ç›¸åŒåŸºç¡€URLï¼Œå®Œå…¨ä¸åŒå‚æ•°
    result = matcher.calculate_url_similarity(url1, url3)
    print(f"æµ‹è¯•2 - ç›¸åŒåŸºç¡€URLï¼Œå®Œå…¨ä¸åŒå‚æ•°:")
    print(f"  åŸºç¡€URLåŒ¹é…: {result['base_exact_match']}")
    print(f"  ç»¼åˆè¯„åˆ†: {result['composite_score']:.3f}")
    print(f"  æ˜¯å¦åŒ¹é…: {result['is_match']}")
    print()

    # æµ‹è¯•3: å®Œå…¨ä¸åŒURL
    result = matcher.calculate_url_similarity(url1, url4)
    print(f"æµ‹è¯•3 - å®Œå…¨ä¸åŒURL:")
    print(f"  åŸºç¡€URLåŒ¹é…: {result['base_exact_match']}")
    print(f"  ç»¼åˆè¯„åˆ†: {result['composite_score']:.3f}")
    print(f"  æ˜¯å¦åŒ¹é…: {result['is_match']}")
    print()

    # æµ‹è¯•4: æœ€ä½³åŒ¹é…æŸ¥æ‰¾
    candidates = [url2, url3, url4]
    best_match = matcher.find_best_match(url1, candidates)
    if best_match:
        print(f"æœ€ä½³åŒ¹é…: {best_match[0]}")
        print(f"åŒ¹é…è¯„åˆ†: {best_match[1]['composite_score']:.3f}")
    else:
        print("æœªæ‰¾åˆ°åŒ¹é…çš„URL")
