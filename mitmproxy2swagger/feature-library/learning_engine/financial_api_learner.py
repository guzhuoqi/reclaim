#!/usr/bin/env python3
"""
é‡‘èAPIå¢å¼ºå­¦ä¹ å¼•æ“
Financial API Enhanced Learning Engine

åŸºäºç°æœ‰ç‰¹å¾åº“ï¼Œé€šè¿‡æ¨¡å¼åŒ¹é…å’Œé‚»å±…åˆ†æï¼Œå­¦ä¹ æ–°çš„é‡‘èAPIç‰¹å¾
å¹¶åŠ¨æ€æ›´æ–°ç‰¹å¾åº“ï¼Œæé«˜APIè¯†åˆ«çš„å‡†ç¡®æ€§å’Œè¦†ç›–ç‡ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. å®½æ¾å‰ç½®æ‰«æ - é™ä½é˜ˆå€¼ï¼Œæ”¶é›†æ›´å¤šå€™é€‰API
2. é‚»å±…æŠ¥æ–‡åˆ†æ - åŸºäºæ—¶é—´åºåˆ—å’Œè°ƒç”¨é“¾åˆ†æAPIå…³ç³»
3. æ¨¡å¼å­¦ä¹  - ä»æˆåŠŸæ¡ˆä¾‹ä¸­æå–æ–°çš„URLå’Œå“åº”æ¨¡å¼
4. ç‰¹å¾åº“æ›´æ–° - å°†å­¦ä¹ åˆ°çš„æ–°çŸ¥è¯†è¡¥å……åˆ°ç‰¹å¾åº“
5. ç¡®å®šæ€§å¢å¼º - é€šè¿‡å¤šç»´åº¦éªŒè¯æé«˜å­¦ä¹ çš„å¯é æ€§
"""

import json
import re
import sys
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from urllib.parse import urlparse, parse_qs
import difflib

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥ç°æœ‰æ¨¡å—
sys.path.append('mitmproxy2swagger/mitmproxy2swagger')
sys.path.append('mitmproxy2swagger/feature-library/ai_analysis_features')

from financial_api_analyzer import FinancialAPIAnalyzer


@dataclass
class APICandidate:
    """APIå€™é€‰å¯¹è±¡"""
    url: str
    method: str
    status_code: int
    response_content: str
    request_body: str
    timestamp: datetime
    domain: str
    confidence_score: float = 0.0
    learned_patterns: List[str] = None
    neighbor_context: Dict[str, Any] = None

    def __post_init__(self):
        if self.learned_patterns is None:
            self.learned_patterns = []
        if self.neighbor_context is None:
            self.neighbor_context = {}


@dataclass
class LearnedPattern:
    """å­¦ä¹ åˆ°çš„æ¨¡å¼"""
    pattern_type: str  # url_pattern, response_pattern, sequence_pattern
    pattern_value: str
    confidence: float
    source_apis: List[str]
    validation_count: int = 0
    institution: str = ""
    category: str = ""


class FinancialAPILearner:
    """é‡‘èAPIå¢å¼ºå­¦ä¹ å¼•æ“"""

    def __init__(self, feature_library_path: str = None):
        """åˆå§‹åŒ–å­¦ä¹ å¼•æ“"""
        self.logger = logging.getLogger(__name__)

        # åŠ è½½ç°æœ‰ç‰¹å¾åº“
        if feature_library_path is None:
            feature_library_path = 'mitmproxy2swagger/feature-library/ai_analysis_features/financial_api_features.json'

        self.feature_library_path = feature_library_path
        self.feature_library = self._load_feature_library()

        # åˆå§‹åŒ–åˆ†æå™¨
        self.analyzer = FinancialAPIAnalyzer()

        # å­¦ä¹ é…ç½® - ä¼˜åŒ–åçš„å‚æ•°
        self.config = {
            'loose_scan_threshold': 0.05,  # é™ä½å®½æ¾æ‰«æé˜ˆå€¼ (0.1 -> 0.05)
            'modern_api_bonus': 0.15,       # ç°ä»£APIé¢å¤–åŠ åˆ†
            'json_response_bonus': 0.1,     # JSONå“åº”é¢å¤–åŠ åˆ†
            'large_response_bonus': 0.05,   # å¤§å“åº”é¢å¤–åŠ åˆ†
            'neighbor_time_window': 300,    # é‚»å±…åˆ†ææ—¶é—´çª—å£(ç§’)
            'min_confidence_for_learning': 0.4,  # é™ä½å­¦ä¹ çš„æœ€å°ç½®ä¿¡åº¦ (0.6 -> 0.4)
            'pattern_validation_threshold': 2,   # æ¨¡å¼éªŒè¯é˜ˆå€¼
            'max_candidates_per_domain': 100,   # å¢åŠ æ¯ä¸ªåŸŸåæœ€å¤§å€™é€‰æ•° (50 -> 100)
            'enable_modern_api_detection': True  # å¯ç”¨ç°ä»£APIæ£€æµ‹
        }

        # å­¦ä¹ çŠ¶æ€
        self.learned_patterns: List[LearnedPattern] = []
        self.api_candidates: List[APICandidate] = []
        self.learning_stats = {
            'total_scanned': 0,
            'candidates_found': 0,
            'patterns_learned': 0,
            'feature_library_updates': 0
        }

    def _load_feature_library(self) -> Dict[str, Any]:
        """åŠ è½½ç‰¹å¾åº“"""
        try:
            with open(self.feature_library_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.error(f"åŠ è½½ç‰¹å¾åº“å¤±è´¥: {e}")
            return {}

    def _save_feature_library(self):
        """ä¿å­˜ç‰¹å¾åº“"""
        try:
            # å¤‡ä»½åŸæ–‡ä»¶
            backup_path = f"{self.feature_library_path}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            with open(self.feature_library_path, 'r', encoding='utf-8') as f:
                backup_content = f.read()
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(backup_content)

            # ä¿å­˜æ›´æ–°åçš„ç‰¹å¾åº“
            with open(self.feature_library_path, 'w', encoding='utf-8') as f:
                json.dump(self.feature_library, f, indent=2, ensure_ascii=False)

            self.logger.info(f"ç‰¹å¾åº“å·²æ›´æ–°ï¼Œå¤‡ä»½ä¿å­˜è‡³: {backup_path}")
            return True
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç‰¹å¾åº“å¤±è´¥: {e}")
            return False

    def loose_scan_apis(self, flows: List[Dict[str, Any]]) -> List[APICandidate]:
        """å®½æ¾å‰ç½®æ‰«æ - æ”¶é›†æ‰€æœ‰å¯èƒ½çš„é‡‘èAPIå€™é€‰"""
        candidates = []

        for flow in flows:
            try:
                url = flow.get('url', '')
                method = flow.get('method', '').upper()
                status_code = flow.get('status_code', 0)
                response_body = flow.get('response_body', '')
                request_body = flow.get('request_body', '')

                # åŸºæœ¬è¿‡æ»¤
                if not url or status_code != 200:
                    continue

                # è§£æåŸŸå
                parsed_url = urlparse(url)
                domain = parsed_url.netloc.lower()

                # è·³è¿‡æ˜æ˜¾çš„é™æ€èµ„æº
                if self._is_static_resource(url):
                    continue

                # å®½æ¾çš„é‡‘èæœºæ„è¯†åˆ«
                if self._is_potential_financial_domain(domain):
                    # å®½æ¾çš„APIæ¨¡å¼åŒ¹é…
                    confidence = self._calculate_loose_confidence(url, response_body, domain)

                    if confidence >= self.config['loose_scan_threshold']:
                        candidate = APICandidate(
                            url=url,
                            method=method,
                            status_code=status_code,
                            response_content=response_body,
                            request_body=request_body,
                            timestamp=datetime.now(),  # å®é™…åº”è¯¥ä»flowä¸­è·å–
                            domain=domain,
                            confidence_score=confidence
                        )
                        candidates.append(candidate)

                self.learning_stats['total_scanned'] += 1

            except Exception as e:
                self.logger.warning(f"å¤„ç†flowæ—¶å‡ºé”™: {e}")
                continue

        # æŒ‰åŸŸåé™åˆ¶å€™é€‰æ•°é‡
        candidates = self._limit_candidates_per_domain(candidates)

        # ä¿å­˜å€™é€‰APIåˆ°å®ä¾‹å˜é‡
        self.api_candidates = candidates

        self.learning_stats['candidates_found'] = len(candidates)
        self.logger.info(f"å®½æ¾æ‰«æå®Œæˆï¼Œå‘ç° {len(candidates)} ä¸ªå€™é€‰API")

        return candidates

    def _is_static_resource(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé™æ€èµ„æº"""
        static_extensions = ['.js', '.css', '.png', '.jpg', '.gif', '.ico', '.woff', '.ttf', '.svg']
        static_keywords = ['google', 'doubleclick', 'analytics', 'facebook', 'twitter']

        url_lower = url.lower()

        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        if any(url_lower.endswith(ext) for ext in static_extensions):
            return True

        # æ£€æŸ¥ç¬¬ä¸‰æ–¹æœåŠ¡
        if any(keyword in url_lower for keyword in static_keywords):
            return True

        return False

    def _is_potential_financial_domain(self, domain: str) -> bool:
        """å®½æ¾çš„é‡‘èæœºæ„åŸŸåè¯†åˆ«"""
        # å·²çŸ¥é‡‘èæœºæ„åŸŸå
        known_financial_domains = set()
        for category in self.feature_library.values():
            if isinstance(category, dict) and 'institutions' in category:
                for inst_data in category['institutions'].values():
                    if 'domains' in inst_data:
                        known_financial_domains.update(inst_data['domains'])

        if domain in known_financial_domains:
            return True

        # é‡‘èå…³é”®å­—åŒ¹é… - å¢å¼ºç‰ˆ
        financial_keywords = [
            # ä¼ ç»Ÿé“¶è¡Œ
            'bank', 'banking', 'finance', 'investment', 'wealth', 'credit',
            'hsbc', 'citibank', 'jpmorgan', 'goldman', 'morgan', 'wells',
            'boc', 'icbc', 'ccb', 'abc', 'cmb', 'cib', 'spdb', 'citic',
            'hangseng', 'bochk', 'dbs', 'ocbc', 'uob', 'maybank',
            # ä¸­å›½é“¶è¡Œç‰¹æ®ŠåŸŸå
            'mybank', 'epass', 'winglungbank', 'cmbwinglungbank',
            # ç°ä»£åˆ¸å•†
            'lbkrs', 'longbridge', 'longport', 'futu', 'futuhk', 'futunn',
            'tiger', 'tigerbrokers', 'itiger', 'webull', 'moomoo',
            'securities', 'brokerage', 'trading', 'broker',
            # æ”¯ä»˜å’Œé‡‘èç§‘æŠ€
            'pay', 'payment', 'wallet', 'fintech', 'alipay', 'wechatpay'
        ]

        if any(keyword in domain for keyword in financial_keywords):
            return True

        # ç‰¹æ®ŠåŸŸåæ¨¡å¼åŒ¹é…
        special_patterns = [
            r'.*bank.*\.com',      # åŒ…å«bankçš„åŸŸå
            r'.*securities.*\.com', # åŒ…å«securitiesçš„åŸŸå
            r'.*trading.*\.com',   # åŒ…å«tradingçš„åŸŸå
            r'.*finance.*\.com',   # åŒ…å«financeçš„åŸŸå
            r'.*\.com\.cn$',       # ä¸­å›½åŸŸå (.com.cn)
            r'.*\.com\.hk$'        # é¦™æ¸¯åŸŸå (.com.hk)
        ]

        import re
        for pattern in special_patterns:
            if re.match(pattern, domain):
                return True

        return False

    def _calculate_loose_confidence(self, url: str, response_content: str, domain: str) -> float:
        """è®¡ç®—å®½æ¾ç½®ä¿¡åº¦åˆ†æ•° - å¢å¼ºç‰ˆ"""
        confidence = 0.0

        # URLæ¨¡å¼åŒ¹é… (æƒé‡: 0.3)
        url_score = self._score_url_patterns(url)
        confidence += url_score * 0.3

        # å“åº”å†…å®¹åŒ¹é… (æƒé‡: 0.3)
        content_score = self._score_response_content(response_content)
        confidence += content_score * 0.3

        # åŸŸååŒ¹é… (æƒé‡: 0.2)
        domain_score = self._score_domain(domain)
        confidence += domain_score * 0.2

        # ç°ä»£APIæ¨¡å¼åŠ åˆ† (æƒé‡: 0.2)
        modern_api_score = self._score_modern_api_patterns(url, response_content, domain)
        confidence += modern_api_score * 0.2

        return min(confidence, 1.0)

    def _score_url_patterns(self, url: str) -> float:
        """URLæ¨¡å¼è¯„åˆ†"""
        url_lower = url.lower()
        score = 0.0

        # é‡‘èAPIå…³é”®å­—
        financial_keywords = [
            'account', 'balance', 'transaction', 'transfer', 'payment',
            'portfolio', 'investment', 'wealth', 'asset', 'deposit',
            'loan', 'credit', 'card', 'statement', 'history',
            'dashboard', 'overview', 'summary', 'detail', 'info',
            'api', 'service', 'data', 'query', 'get', 'fetch'
        ]

        for keyword in financial_keywords:
            if keyword in url_lower:
                score += 0.1

        # APIè·¯å¾„æ¨¡å¼ - å¢å¼ºç‰ˆ
        api_patterns = ['/api/', '/service/', '/rest/', '/v1/', '/v2/', '/data/']
        for pattern in api_patterns:
            if pattern in url_lower:
                score += 0.2

        # ç°ä»£åˆ¸å•†APIæ¨¡å¼
        modern_patterns = ['/api/forward/', '/api/third_party/', '/api/v1/', '/api/v2/']
        for pattern in modern_patterns:
            if pattern in url_lower:
                score += 0.3  # ç°ä»£APIæ¨¡å¼ç»™æ›´é«˜åˆ†

        # ä¼ ç»Ÿé“¶è¡ŒServletæ¨¡å¼
        servlet_patterns = ['servlet', '.do']
        for pattern in servlet_patterns:
            if pattern in url_lower:
                score += 0.15  # ä¼ ç»Ÿæ¨¡å¼ç»™é€‚ä¸­åˆ†æ•°

        return min(score, 1.0)

    def _score_response_content(self, content: str) -> float:
        """å“åº”å†…å®¹è¯„åˆ†"""
        if not content:
            return 0.0

        content_lower = content.lower()
        score = 0.0

        # JSONæ ¼å¼åŠ åˆ†
        if content.strip().startswith('{') or content.strip().startswith('['):
            score += 0.3

        # é‡‘èæ•°æ®å…³é”®å­—
        financial_data_keywords = [
            'balance', 'amount', 'currency', 'account', 'transaction',
            'portfolio', 'asset', 'investment', 'deposit', 'credit',
            'hkd', 'usd', 'cny', 'eur', 'gbp', 'jpy',
            'total', 'available', 'current', 'saving', 'checking'
        ]

        for keyword in financial_data_keywords:
            if keyword in content_lower:
                score += 0.05

        return min(score, 1.0)

    def _score_domain(self, domain: str) -> float:
        """åŸŸåè¯„åˆ†"""
        # æ£€æŸ¥æ˜¯å¦åœ¨å·²çŸ¥é‡‘èæœºæ„åˆ—è¡¨ä¸­
        for category in self.feature_library.values():
            if isinstance(category, dict) and 'institutions' in category:
                for inst_data in category['institutions'].values():
                    if 'domains' in inst_data and domain in inst_data['domains']:
                        return 1.0

        # é‡‘èå…³é”®å­—åŒ¹é…
        financial_keywords = ['bank', 'finance', 'investment', 'wealth']
        for keyword in financial_keywords:
            if keyword in domain:
                return 0.7

        return 0.0

    def _score_modern_api_patterns(self, url: str, response_content: str, domain: str) -> float:
        """ç°ä»£APIæ¨¡å¼è¯„åˆ† - ä¸“é—¨é’ˆå¯¹é•¿æ¡¥ç­‰ç°ä»£åˆ¸å•†"""
        score = 0.0
        url_lower = url.lower()

        # ç°ä»£åˆ¸å•†åŸŸåè¯†åˆ«
        modern_securities_domains = [
            'lbkrs.com', 'longbridge.com', 'futu5.com', 'futuhk.com',
            'tigerbrokers.com', 'itiger.com', 'webull.com', 'moomoo.com'
        ]

        if any(domain_pattern in domain for domain_pattern in modern_securities_domains):
            score += 0.4  # ç°ä»£åˆ¸å•†åŸŸåé«˜åˆ†

        # ç°ä»£APIè·¯å¾„æ¨¡å¼
        modern_api_patterns = [
            '/api/forward/',     # é•¿æ¡¥ç‰¹æœ‰æ¨¡å¼
            '/api/third_party/', # ç¬¬ä¸‰æ–¹è®¤è¯
            '/api/v1/',          # ç‰ˆæœ¬åŒ–API
            '/api/v2/',
            '/portfolio/',       # æŠ•èµ„ç»„åˆ
            '/account/',         # è´¦æˆ·ç®¡ç†
            '/auth/',           # è®¤è¯
            '/config/',         # é…ç½®
            '/member/'          # ä¼šå‘˜ä¿¡æ¯
        ]

        for pattern in modern_api_patterns:
            if pattern in url_lower:
                score += 0.2

        # JSONå“åº”æ ¼å¼åŠ åˆ†
        if response_content and (response_content.strip().startswith('{') or response_content.strip().startswith('[')):
            score += 0.2

        # å¤§å“åº”åŠ åˆ† (é€šå¸¸åŒ…å«æ›´å¤šæ•°æ®)
        if response_content and len(response_content) > 1000:
            score += 0.1

        # ç°ä»£é‡‘èæœ¯è¯­
        modern_financial_terms = [
            'available_cash', 'max_purchase', 'credit_limit', 'total_asset',
            'account_no', 'account_channel', 'member_info', 'portfolio',
            'position', 'trading', 'execution', 'order_id'
        ]

        if response_content:
            content_lower = response_content.lower()
            for term in modern_financial_terms:
                if term in content_lower:
                    score += 0.05

        return min(score, 1.0)

    def _limit_candidates_per_domain(self, candidates: List[APICandidate]) -> List[APICandidate]:
        """é™åˆ¶æ¯ä¸ªåŸŸåçš„å€™é€‰æ•°é‡"""
        domain_candidates = defaultdict(list)

        for candidate in candidates:
            domain_candidates[candidate.domain].append(candidate)

        limited_candidates = []
        for domain, domain_list in domain_candidates.items():
            # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œå–å‰Nä¸ª
            sorted_candidates = sorted(domain_list, key=lambda x: x.confidence_score, reverse=True)
            limited_candidates.extend(sorted_candidates[:self.config['max_candidates_per_domain']])

        return limited_candidates

    def analyze_neighbor_context(self, candidates: List[APICandidate], all_flows: List[Dict[str, Any]]) -> List[APICandidate]:
        """é‚»å±…æŠ¥æ–‡åˆ†æ - åŸºäºæ—¶é—´åºåˆ—å’Œè°ƒç”¨é“¾åˆ†æAPIå…³ç³»"""
        self.logger.info("å¼€å§‹é‚»å±…æŠ¥æ–‡åˆ†æ...")

        # ä¸ºæ¯ä¸ªå€™é€‰APIåˆ†æå…¶é‚»å±…ä¸Šä¸‹æ–‡
        for candidate in candidates:
            try:
                neighbor_context = self._extract_neighbor_context(candidate, all_flows)
                candidate.neighbor_context = neighbor_context

                # åŸºäºé‚»å±…ä¸Šä¸‹æ–‡è°ƒæ•´ç½®ä¿¡åº¦
                context_boost = self._calculate_context_boost(neighbor_context)
                candidate.confidence_score = min(candidate.confidence_score + context_boost, 1.0)

            except Exception as e:
                self.logger.warning(f"åˆ†æé‚»å±…ä¸Šä¸‹æ–‡æ—¶å‡ºé”™: {e}")
                continue

        return candidates

    def _extract_neighbor_context(self, candidate: APICandidate, all_flows: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æå–é‚»å±…ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context = {
            'preceding_apis': [],
            'following_apis': [],
            'same_domain_apis': [],
            'authentication_sequence': [],
            'business_flow_indicators': []
        }

        candidate_time = candidate.timestamp
        time_window = timedelta(seconds=self.config['neighbor_time_window'])

        for flow in all_flows:
            try:
                flow_url = flow.get('url', '')
                flow_domain = urlparse(flow_url).netloc.lower()
                flow_time = datetime.now()  # å®é™…åº”è¯¥ä»flowä¸­è§£ææ—¶é—´æˆ³

                # åŒåŸŸåAPI
                if flow_domain == candidate.domain and flow_url != candidate.url:
                    context['same_domain_apis'].append({
                        'url': flow_url,
                        'method': flow.get('method', ''),
                        'status': flow.get('status_code', 0)
                    })

                # æ—¶é—´çª—å£å†…çš„API
                time_diff = abs((flow_time - candidate_time).total_seconds())
                if time_diff <= self.config['neighbor_time_window']:
                    api_info = {
                        'url': flow_url,
                        'method': flow.get('method', ''),
                        'status': flow.get('status_code', 0),
                        'time_offset': (flow_time - candidate_time).total_seconds()
                    }

                    if flow_time < candidate_time:
                        context['preceding_apis'].append(api_info)
                    elif flow_time > candidate_time:
                        context['following_apis'].append(api_info)

                # è®¤è¯åºåˆ—æ£€æµ‹
                if self._is_authentication_api(flow_url):
                    context['authentication_sequence'].append({
                        'url': flow_url,
                        'time_offset': (flow_time - candidate_time).total_seconds()
                    })

            except Exception as e:
                continue

        # ä¸šåŠ¡æµç¨‹æŒ‡æ ‡
        context['business_flow_indicators'] = self._detect_business_flow_patterns(context)

        return context

    def _is_authentication_api(self, url: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºè®¤è¯API"""
        auth_keywords = ['login', 'auth', 'authenticate', 'signin', 'logon', 'verify', 'token']
        url_lower = url.lower()
        return any(keyword in url_lower for keyword in auth_keywords)

    def _detect_business_flow_patterns(self, context: Dict[str, Any]) -> List[str]:
        """æ£€æµ‹ä¸šåŠ¡æµç¨‹æ¨¡å¼"""
        patterns = []

        # æ£€æµ‹ç™»å½•->ä¸šåŠ¡APIæ¨¡å¼
        auth_before = any(api['time_offset'] < 0 for api in context['authentication_sequence'])
        if auth_before:
            patterns.append('post_authentication_api')

        # æ£€æµ‹æ‰¹é‡APIè°ƒç”¨æ¨¡å¼
        if len(context['same_domain_apis']) > 5:
            patterns.append('batch_api_calls')

        # æ£€æµ‹dashboardåŠ è½½æ¨¡å¼
        dashboard_keywords = ['dashboard', 'overview', 'summary', 'home']
        dashboard_apis = [api for api in context['same_domain_apis']
                         if any(keyword in api['url'].lower() for keyword in dashboard_keywords)]
        if dashboard_apis:
            patterns.append('dashboard_loading_sequence')

        return patterns

    def _calculate_context_boost(self, context: Dict[str, Any]) -> float:
        """åŸºäºä¸Šä¸‹æ–‡è®¡ç®—ç½®ä¿¡åº¦æå‡"""
        boost = 0.0

        # è®¤è¯åAPIåŠ åˆ†
        if 'post_authentication_api' in context['business_flow_indicators']:
            boost += 0.2

        # DashboardåŠ è½½åºåˆ—åŠ åˆ†
        if 'dashboard_loading_sequence' in context['business_flow_indicators']:
            boost += 0.15

        # åŒåŸŸåAPIæ•°é‡åŠ åˆ†
        same_domain_count = len(context['same_domain_apis'])
        if same_domain_count > 3:
            boost += min(same_domain_count * 0.02, 0.1)

        # æ‰¹é‡è°ƒç”¨æ¨¡å¼åŠ åˆ†
        if 'batch_api_calls' in context['business_flow_indicators']:
            boost += 0.1

        return boost

    def learn_patterns_from_candidates(self, candidates: List[APICandidate]) -> List[LearnedPattern]:
        """ä»å€™é€‰APIä¸­å­¦ä¹ æ–°æ¨¡å¼"""
        self.logger.info("å¼€å§‹ä»å€™é€‰APIå­¦ä¹ æ–°æ¨¡å¼...")

        learned_patterns = []

        # æŒ‰ç½®ä¿¡åº¦è¿‡æ»¤é«˜è´¨é‡å€™é€‰
        high_confidence_candidates = [
            c for c in candidates
            if c.confidence_score >= self.config['min_confidence_for_learning']
        ]

        self.logger.info(f"é«˜ç½®ä¿¡åº¦å€™é€‰APIæ•°é‡: {len(high_confidence_candidates)}")

        # å­¦ä¹ URLæ¨¡å¼
        url_patterns = self._learn_url_patterns(high_confidence_candidates)
        learned_patterns.extend(url_patterns)

        # å­¦ä¹ å“åº”æ¨¡å¼
        response_patterns = self._learn_response_patterns(high_confidence_candidates)
        learned_patterns.extend(response_patterns)

        # å­¦ä¹ åºåˆ—æ¨¡å¼
        sequence_patterns = self._learn_sequence_patterns(high_confidence_candidates)
        learned_patterns.extend(sequence_patterns)

        self.learned_patterns.extend(learned_patterns)
        self.learning_stats['patterns_learned'] = len(learned_patterns)

        self.logger.info(f"å­¦ä¹ åˆ° {len(learned_patterns)} ä¸ªæ–°æ¨¡å¼")

        return learned_patterns

    def _learn_url_patterns(self, candidates: List[APICandidate]) -> List[LearnedPattern]:
        """å­¦ä¹ URLæ¨¡å¼"""
        patterns = []

        # æŒ‰åŸŸååˆ†ç»„
        domain_groups = defaultdict(list)
        for candidate in candidates:
            domain_groups[candidate.domain].append(candidate)

        for domain, domain_candidates in domain_groups.items():
            if len(domain_candidates) < 2:  # è‡³å°‘éœ€è¦2ä¸ªæ ·æœ¬
                continue

            # æå–URLè·¯å¾„æ¨¡å¼
            paths = [urlparse(c.url).path for c in domain_candidates]
            common_patterns = self._extract_common_url_patterns(paths)

            for pattern in common_patterns:
                learned_pattern = LearnedPattern(
                    pattern_type='url_pattern',
                    pattern_value=pattern,
                    confidence=0.7,  # åˆå§‹ç½®ä¿¡åº¦
                    source_apis=[c.url for c in domain_candidates],
                    institution=self._identify_institution(domain),
                    category='financial_api'
                )
                patterns.append(learned_pattern)

        return patterns

    def _extract_common_url_patterns(self, paths: List[str]) -> List[str]:
        """æå–URLè·¯å¾„çš„å…¬å…±æ¨¡å¼"""
        patterns = []

        # ç®€å•çš„å…¬å…±å‰ç¼€/åç¼€æ£€æµ‹
        if len(paths) < 2:
            return patterns

        # æŸ¥æ‰¾å…¬å…±å‰ç¼€
        common_prefix = self._find_common_prefix(paths)
        if len(common_prefix) > 10:  # è‡³å°‘10ä¸ªå­—ç¬¦
            patterns.append(f"{common_prefix}*")

        # æŸ¥æ‰¾åŒ…å«ç‰¹å®šå…³é”®å­—çš„æ¨¡å¼
        financial_keywords = ['account', 'balance', 'transaction', 'portfolio', 'dashboard']
        for keyword in financial_keywords:
            matching_paths = [p for p in paths if keyword in p.lower()]
            if len(matching_paths) >= 2:
                patterns.append(f"*{keyword}*")

        return patterns

    def _find_common_prefix(self, strings: List[str]) -> str:
        """æŸ¥æ‰¾å­—ç¬¦ä¸²åˆ—è¡¨çš„å…¬å…±å‰ç¼€"""
        if not strings:
            return ""

        prefix = strings[0]
        for string in strings[1:]:
            while not string.startswith(prefix):
                prefix = prefix[:-1]
                if not prefix:
                    break

        return prefix

    def _learn_response_patterns(self, candidates: List[APICandidate]) -> List[LearnedPattern]:
        """å­¦ä¹ å“åº”å†…å®¹æ¨¡å¼"""
        patterns = []

        # æ”¶é›†æ‰€æœ‰JSONå“åº”
        json_responses = []
        for candidate in candidates:
            content = candidate.response_content.strip()
            if content.startswith('{') or content.startswith('['):
                try:
                    json_data = json.loads(content)
                    json_responses.append((candidate, json_data))
                except:
                    continue

        if len(json_responses) < 2:
            return patterns

        # åˆ†æJSONç»“æ„æ¨¡å¼
        common_keys = self._find_common_json_keys(json_responses)

        for key_pattern in common_keys:
            learned_pattern = LearnedPattern(
                pattern_type='response_pattern',
                pattern_value=key_pattern,
                confidence=0.6,
                source_apis=[c.url for c, _ in json_responses],
                category='json_structure'
            )
            patterns.append(learned_pattern)

        return patterns

    def _find_common_json_keys(self, json_responses: List[Tuple[APICandidate, Any]]) -> List[str]:
        """æŸ¥æ‰¾JSONå“åº”çš„å…¬å…±é”®æ¨¡å¼"""
        key_patterns = []

        # æ”¶é›†æ‰€æœ‰é”®
        all_keys = set()
        for _, json_data in json_responses:
            if isinstance(json_data, dict):
                all_keys.update(self._extract_json_keys(json_data))

        # æŸ¥æ‰¾åœ¨å¤šä¸ªå“åº”ä¸­å‡ºç°çš„é”®
        key_counts = Counter()
        for _, json_data in json_responses:
            if isinstance(json_data, dict):
                response_keys = set(self._extract_json_keys(json_data))
                for key in response_keys:
                    key_counts[key] += 1

        # é€‰æ‹©å‡ºç°é¢‘ç‡é«˜çš„é”®ä½œä¸ºæ¨¡å¼
        min_occurrences = max(2, len(json_responses) // 2)
        for key, count in key_counts.items():
            if count >= min_occurrences:
                key_patterns.append(key)

        return key_patterns

    def _extract_json_keys(self, json_data: Any, prefix: str = "") -> List[str]:
        """é€’å½’æå–JSONä¸­çš„æ‰€æœ‰é”®"""
        keys = []

        if isinstance(json_data, dict):
            for key, value in json_data.items():
                full_key = f"{prefix}.{key}" if prefix else key
                keys.append(full_key)

                # é€’å½’å¤„ç†åµŒå¥—å¯¹è±¡ï¼ˆé™åˆ¶æ·±åº¦ï¼‰
                if isinstance(value, (dict, list)) and len(prefix.split('.')) < 3:
                    keys.extend(self._extract_json_keys(value, full_key))

        elif isinstance(json_data, list) and json_data:
            # å¤„ç†æ•°ç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
            keys.extend(self._extract_json_keys(json_data[0], prefix))

        return keys

    def _learn_sequence_patterns(self, candidates: List[APICandidate]) -> List[LearnedPattern]:
        """å­¦ä¹ APIè°ƒç”¨åºåˆ—æ¨¡å¼"""
        patterns = []

        # æŒ‰åŸŸååˆ†ç»„åˆ†æåºåˆ—
        domain_groups = defaultdict(list)
        for candidate in candidates:
            if candidate.neighbor_context:
                domain_groups[candidate.domain].append(candidate)

        for domain, domain_candidates in domain_groups.items():
            if len(domain_candidates) < 2:
                continue

            # åˆ†æè®¤è¯åAPIè°ƒç”¨æ¨¡å¼
            auth_sequences = self._extract_auth_sequences(domain_candidates)
            for sequence in auth_sequences:
                learned_pattern = LearnedPattern(
                    pattern_type='sequence_pattern',
                    pattern_value=sequence,
                    confidence=0.8,
                    source_apis=[c.url for c in domain_candidates],
                    institution=self._identify_institution(domain),
                    category='authentication_flow'
                )
                patterns.append(learned_pattern)

            # åˆ†ææ‰¹é‡APIè°ƒç”¨æ¨¡å¼
            batch_patterns = self._extract_batch_patterns(domain_candidates)
            for pattern in batch_patterns:
                learned_pattern = LearnedPattern(
                    pattern_type='sequence_pattern',
                    pattern_value=pattern,
                    confidence=0.7,
                    source_apis=[c.url for c in domain_candidates],
                    institution=self._identify_institution(domain),
                    category='batch_loading'
                )
                patterns.append(learned_pattern)

        return patterns

    def _extract_auth_sequences(self, candidates: List[APICandidate]) -> List[str]:
        """æå–è®¤è¯åºåˆ—æ¨¡å¼"""
        sequences = []

        # æŸ¥æ‰¾è®¤è¯åAPIæ¨¡å¼
        post_auth_apis = [
            c for c in candidates
            if 'post_authentication_api' in c.neighbor_context.get('business_flow_indicators', [])
        ]

        if len(post_auth_apis) >= 2:
            sequences.append('auth_login -> business_api_calls')

        return sequences

    def _extract_batch_patterns(self, candidates: List[APICandidate]) -> List[str]:
        """æå–æ‰¹é‡è°ƒç”¨æ¨¡å¼"""
        patterns = []

        # æŸ¥æ‰¾dashboardåŠ è½½æ¨¡å¼
        dashboard_apis = [
            c for c in candidates
            if 'dashboard_loading_sequence' in c.neighbor_context.get('business_flow_indicators', [])
        ]

        if len(dashboard_apis) >= 2:
            patterns.append('dashboard_load -> multiple_api_calls')

        return patterns

    def _identify_institution(self, domain: str) -> str:
        """è¯†åˆ«é‡‘èæœºæ„"""
        # æ£€æŸ¥å·²çŸ¥æœºæ„
        for category in self.feature_library.values():
            if isinstance(category, dict) and 'institutions' in category:
                for inst_key, inst_data in category['institutions'].items():
                    if 'domains' in inst_data and domain in inst_data['domains']:
                        return inst_data.get('name', inst_key)

        # åŸºäºåŸŸåæ¨æ–­
        if 'hsbc' in domain:
            return 'HSBC'
        elif 'bochk' in domain or 'boc' in domain:
            return 'ä¸­é“¶é¦™æ¸¯'
        elif 'hangseng' in domain:
            return 'æ’ç”Ÿé“¶è¡Œ'
        elif 'dbs' in domain:
            return 'DBS'
        elif 'citibank' in domain or 'citi' in domain:
            return 'èŠ±æ——é“¶è¡Œ'

        return 'Unknown'

    def update_feature_library(self, learned_patterns: List[LearnedPattern]) -> bool:
        """æ›´æ–°ç‰¹å¾åº“"""
        self.logger.info("å¼€å§‹æ›´æ–°ç‰¹å¾åº“...")

        if not learned_patterns:
            self.logger.info("æ²¡æœ‰æ–°æ¨¡å¼éœ€è¦æ›´æ–°")
            return True

        try:
            # éªŒè¯æ¨¡å¼è´¨é‡
            validated_patterns = self._validate_patterns(learned_patterns)

            if not validated_patterns:
                self.logger.info("æ²¡æœ‰é€šè¿‡éªŒè¯çš„æ¨¡å¼")
                return True

            # æ›´æ–°ç‰¹å¾åº“ç»“æ„
            updates_made = 0

            for pattern in validated_patterns:
                if self._add_pattern_to_library(pattern):
                    updates_made += 1

            if updates_made > 0:
                # ä¿å­˜æ›´æ–°åçš„ç‰¹å¾åº“
                if self._save_feature_library():
                    self.learning_stats['feature_library_updates'] = updates_made
                    self.logger.info(f"æˆåŠŸæ›´æ–°ç‰¹å¾åº“ï¼Œæ·»åŠ äº† {updates_made} ä¸ªæ–°æ¨¡å¼")
                    return True
                else:
                    self.logger.error("ä¿å­˜ç‰¹å¾åº“å¤±è´¥")
                    return False
            else:
                self.logger.info("æ²¡æœ‰æ–°æ¨¡å¼è¢«æ·»åŠ åˆ°ç‰¹å¾åº“")
                return True

        except Exception as e:
            self.logger.error(f"æ›´æ–°ç‰¹å¾åº“æ—¶å‡ºé”™: {e}")
            return False

    def _validate_patterns(self, patterns: List[LearnedPattern]) -> List[LearnedPattern]:
        """éªŒè¯æ¨¡å¼è´¨é‡"""
        validated = []

        for pattern in patterns:
            # åŸºæœ¬è´¨é‡æ£€æŸ¥
            if pattern.confidence < 0.5:
                continue

            if len(pattern.source_apis) < 2:
                continue

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç±»ä¼¼æ¨¡å¼
            if self._is_duplicate_pattern(pattern):
                continue

            validated.append(pattern)

        return validated

    def _is_duplicate_pattern(self, pattern: LearnedPattern) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤æ¨¡å¼"""
        # ç®€å•çš„é‡å¤æ£€æŸ¥
        for existing_pattern in self.learned_patterns:
            if (existing_pattern.pattern_type == pattern.pattern_type and
                existing_pattern.pattern_value == pattern.pattern_value):
                return True

        return False

    def _add_pattern_to_library(self, pattern: LearnedPattern) -> bool:
        """å°†æ¨¡å¼æ·»åŠ åˆ°ç‰¹å¾åº“"""
        try:
            institution = pattern.institution
            if not institution or institution == 'Unknown':
                return False

            # æŸ¥æ‰¾æˆ–åˆ›å»ºæœºæ„æ¡ç›®
            institution_key = self._find_or_create_institution_key(institution)
            if not institution_key:
                return False

            # æ ¹æ®æ¨¡å¼ç±»å‹æ·»åŠ åˆ°ç›¸åº”ä½ç½®
            if pattern.pattern_type == 'url_pattern':
                self._add_url_pattern(institution_key, pattern)
            elif pattern.pattern_type == 'response_pattern':
                self._add_response_pattern(institution_key, pattern)
            elif pattern.pattern_type == 'sequence_pattern':
                self._add_sequence_pattern(institution_key, pattern)

            return True

        except Exception as e:
            self.logger.warning(f"æ·»åŠ æ¨¡å¼åˆ°ç‰¹å¾åº“å¤±è´¥: {e}")
            return False

    def _find_or_create_institution_key(self, institution: str) -> Optional[str]:
        """æŸ¥æ‰¾æˆ–åˆ›å»ºæœºæ„é”®"""
        # æŸ¥æ‰¾ç°æœ‰æœºæ„
        for category_key, category in self.feature_library.items():
            if isinstance(category, dict) and 'institutions' in category:
                for inst_key, inst_data in category['institutions'].items():
                    if inst_data.get('name') == institution:
                        return f"{category_key}.institutions.{inst_key}"

        # åˆ›å»ºæ–°æœºæ„æ¡ç›®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„é€»è¾‘ï¼‰
        if 'learned_institutions' not in self.feature_library:
            self.feature_library['learned_institutions'] = {
                'description': 'å­¦ä¹ åˆ°çš„æ–°é‡‘èæœºæ„',
                'institutions': {}
            }

        # ç”Ÿæˆæ–°çš„æœºæ„é”®
        inst_key = institution.lower().replace(' ', '_').replace('é“¶è¡Œ', '_bank')
        self.feature_library['learned_institutions']['institutions'][inst_key] = {
            'name': institution,
            'domains': [],
            'api_patterns': {},
            'value_indicators': {
                'high_value_keywords': [],
                'response_patterns': [],
                'bonus_weight': 10
            },
            'learned_date': datetime.now().isoformat()
        }

        return f"learned_institutions.institutions.{inst_key}"

    def _add_url_pattern(self, institution_key: str, pattern: LearnedPattern):
        """æ·»åŠ URLæ¨¡å¼"""
        keys = institution_key.split('.')
        inst_data = self.feature_library
        for key in keys:
            inst_data = inst_data[key]

        if 'api_patterns' not in inst_data:
            inst_data['api_patterns'] = {}

        if 'learned_patterns' not in inst_data['api_patterns']:
            inst_data['api_patterns']['learned_patterns'] = []

        inst_data['api_patterns']['learned_patterns'].append(pattern.pattern_value)

    def _add_response_pattern(self, institution_key: str, pattern: LearnedPattern):
        """æ·»åŠ å“åº”æ¨¡å¼"""
        keys = institution_key.split('.')
        inst_data = self.feature_library
        for key in keys:
            inst_data = inst_data[key]

        if 'value_indicators' not in inst_data:
            inst_data['value_indicators'] = {'response_patterns': []}

        if 'response_patterns' not in inst_data['value_indicators']:
            inst_data['value_indicators']['response_patterns'] = []

        inst_data['value_indicators']['response_patterns'].append(pattern.pattern_value)

    def _add_sequence_pattern(self, institution_key: str, pattern: LearnedPattern):
        """æ·»åŠ åºåˆ—æ¨¡å¼"""
        keys = institution_key.split('.')
        inst_data = self.feature_library
        for key in keys:
            inst_data = inst_data[key]

        if 'sequence_patterns' not in inst_data:
            inst_data['sequence_patterns'] = []

        inst_data['sequence_patterns'].append({
            'pattern': pattern.pattern_value,
            'confidence': pattern.confidence,
            'category': pattern.category
        })

    def learn_from_flows(self, flows: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ä¸»è¦å­¦ä¹ æµç¨‹ - ä»æµæ•°æ®ä¸­å­¦ä¹ æ–°çš„é‡‘èAPIæ¨¡å¼"""
        self.logger.info("å¼€å§‹é‡‘èAPIå¢å¼ºå­¦ä¹ æµç¨‹...")

        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šå®½æ¾å‰ç½®æ‰«æ
            self.logger.info("ç¬¬ä¸€é˜¶æ®µï¼šå®½æ¾å‰ç½®æ‰«æ")
            candidates = self.loose_scan_apis(flows)

            if not candidates:
                self.logger.info("æ²¡æœ‰å‘ç°å€™é€‰API")
                return self._generate_learning_report()

            # ç¬¬äºŒé˜¶æ®µï¼šé‚»å±…æŠ¥æ–‡åˆ†æ
            self.logger.info("ç¬¬äºŒé˜¶æ®µï¼šé‚»å±…æŠ¥æ–‡åˆ†æ")
            enriched_candidates = self.analyze_neighbor_context(candidates, flows)

            # ç¬¬ä¸‰é˜¶æ®µï¼šæ¨¡å¼å­¦ä¹ 
            self.logger.info("ç¬¬ä¸‰é˜¶æ®µï¼šæ¨¡å¼å­¦ä¹ ")
            learned_patterns = self.learn_patterns_from_candidates(enriched_candidates)

            # ç¬¬å››é˜¶æ®µï¼šç‰¹å¾åº“æ›´æ–°
            self.logger.info("ç¬¬å››é˜¶æ®µï¼šç‰¹å¾åº“æ›´æ–°")
            update_success = self.update_feature_library(learned_patterns)

            # ç”Ÿæˆå­¦ä¹ æŠ¥å‘Š
            report = self._generate_learning_report()
            report['update_success'] = update_success
            report['learned_patterns'] = [asdict(p) for p in learned_patterns]
            report['high_confidence_candidates'] = [
                asdict(c) for c in enriched_candidates
                if c.confidence_score >= self.config['min_confidence_for_learning']
            ]

            self.logger.info("é‡‘èAPIå¢å¼ºå­¦ä¹ æµç¨‹å®Œæˆ")
            return report

        except Exception as e:
            self.logger.error(f"å­¦ä¹ æµç¨‹å‡ºé”™: {e}")
            return {
                'success': False,
                'error': str(e),
                'stats': self.learning_stats
            }

    def _generate_learning_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå­¦ä¹ æŠ¥å‘Š"""
        return {
            'success': True,
            'timestamp': datetime.now().isoformat(),
            'stats': self.learning_stats.copy(),
            'config': self.config.copy(),
            'summary': {
                'total_scanned': self.learning_stats['total_scanned'],
                'candidates_found': self.learning_stats['candidates_found'],
                'patterns_learned': self.learning_stats['patterns_learned'],
                'feature_library_updates': self.learning_stats['feature_library_updates']
            }
        }

    def export_learned_knowledge(self, output_path: str) -> bool:
        """å¯¼å‡ºå­¦ä¹ åˆ°çš„çŸ¥è¯†"""
        try:
            # è½¬æ¢APIå€™é€‰ï¼Œå¤„ç†datetimeåºåˆ—åŒ–
            serializable_candidates = []
            for candidate in self.api_candidates:
                candidate_dict = asdict(candidate)
                # è½¬æ¢datetimeä¸ºå­—ç¬¦ä¸²
                if 'timestamp' in candidate_dict:
                    candidate_dict['timestamp'] = candidate_dict['timestamp'].isoformat()
                serializable_candidates.append(candidate_dict)

            knowledge = {
                'metadata': {
                    'export_time': datetime.now().isoformat(),
                    'learner_version': '1.0.0',
                    'total_patterns': len(self.learned_patterns)
                },
                'learned_patterns': [asdict(p) for p in self.learned_patterns],
                'api_candidates': serializable_candidates,
                'learning_stats': self.learning_stats,
                'config': self.config
            }

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(knowledge, f, indent=2, ensure_ascii=False)

            self.logger.info(f"å­¦ä¹ çŸ¥è¯†å·²å¯¼å‡ºåˆ°: {output_path}")
            return True

        except Exception as e:
            self.logger.error(f"å¯¼å‡ºå­¦ä¹ çŸ¥è¯†å¤±è´¥: {e}")
            return False

    def import_learned_knowledge(self, input_path: str) -> bool:
        """å¯¼å…¥å­¦ä¹ åˆ°çš„çŸ¥è¯†"""
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                knowledge = json.load(f)

            # å¯¼å…¥å­¦ä¹ åˆ°çš„æ¨¡å¼
            if 'learned_patterns' in knowledge:
                imported_patterns = []
                for pattern_data in knowledge['learned_patterns']:
                    pattern = LearnedPattern(**pattern_data)
                    imported_patterns.append(pattern)

                self.learned_patterns.extend(imported_patterns)
                self.logger.info(f"å¯¼å…¥äº† {len(imported_patterns)} ä¸ªå­¦ä¹ æ¨¡å¼")

            # å¯¼å…¥APIå€™é€‰
            if 'api_candidates' in knowledge:
                imported_candidates = []
                for candidate_data in knowledge['api_candidates']:
                    # å¤„ç†datetimeå­—æ®µ
                    if 'timestamp' in candidate_data:
                        candidate_data['timestamp'] = datetime.fromisoformat(candidate_data['timestamp'])
                    candidate = APICandidate(**candidate_data)
                    imported_candidates.append(candidate)

                self.api_candidates.extend(imported_candidates)
                self.logger.info(f"å¯¼å…¥äº† {len(imported_candidates)} ä¸ªAPIå€™é€‰")

            return True

        except Exception as e:
            self.logger.error(f"å¯¼å…¥å­¦ä¹ çŸ¥è¯†å¤±è´¥: {e}")
            return False


def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•å’Œç‹¬ç«‹è¿è¡Œ"""
    import argparse

    parser = argparse.ArgumentParser(description='é‡‘èAPIå¢å¼ºå­¦ä¹ å¼•æ“')
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥çš„mitmæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', default='learned_knowledge.json', help='è¾“å‡ºçš„å­¦ä¹ çŸ¥è¯†æ–‡ä»¶')
    parser.add_argument('--feature-lib', '-f', help='ç‰¹å¾åº“æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')

    args = parser.parse_args()

    # é…ç½®æ—¥å¿—
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # åˆå§‹åŒ–å­¦ä¹ å¼•æ“
    learner = FinancialAPILearner(args.feature_lib)

    # è¯»å–mitmæ–‡ä»¶
    sys.path.append('mitmproxy2swagger/mitmproxy2swagger')
    from mitmproxy_capture_reader import MitmproxyCaptureReader

    try:
        print(f"ğŸ” è¯»å–mitmæ–‡ä»¶: {args.input}")
        capture_reader = MitmproxyCaptureReader(args.input)

        # è½¬æ¢ä¸ºæµæ•°æ®æ ¼å¼
        flows = []
        for flow_wrapper in capture_reader.captured_requests():
            try:
                flow_data = {
                    'url': flow_wrapper.get_url(),
                    'method': flow_wrapper.get_method(),
                    'status_code': flow_wrapper.get_response_status_code(),
                    'response_body': flow_wrapper.get_response_body().decode('utf-8', errors='ignore') if flow_wrapper.get_response_body() else '',
                    'request_body': flow_wrapper.get_request_body().decode('utf-8', errors='ignore') if flow_wrapper.get_request_body() else ''
                }
                flows.append(flow_data)
            except Exception as e:
                continue

        print(f"âœ… æˆåŠŸè¯»å– {len(flows)} ä¸ªæµæ•°æ®")

        # æ‰§è¡Œå­¦ä¹ 
        print("ğŸš€ å¼€å§‹å¢å¼ºå­¦ä¹ ...")
        report = learner.learn_from_flows(flows)

        # è¾“å‡ºç»“æœ
        if report['success']:
            print("ğŸ‰ å­¦ä¹ å®Œæˆï¼")
            print(f"ğŸ“Š æ‰«ææ€»æ•°: {report['stats']['total_scanned']}")
            print(f"ğŸ¯ å€™é€‰API: {report['stats']['candidates_found']}")
            print(f"ğŸ“š å­¦ä¹ æ¨¡å¼: {report['stats']['patterns_learned']}")
            print(f"ğŸ”„ ç‰¹å¾åº“æ›´æ–°: {report['stats']['feature_library_updates']}")

            # å¯¼å‡ºå­¦ä¹ çŸ¥è¯†
            if learner.export_learned_knowledge(args.output):
                print(f"ğŸ’¾ å­¦ä¹ çŸ¥è¯†å·²ä¿å­˜åˆ°: {args.output}")
        else:
            print(f"âŒ å­¦ä¹ å¤±è´¥: {report.get('error', 'Unknown error')}")

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        return 1

    return 0


if __name__ == '__main__':
    exit(main())
