#!/usr/bin/env python3
"""
å¢å¼ºå­¦ä¹ ç®¡é“
Enhanced Learning Pipeline

é›†æˆå¢å¼ºå­¦ä¹ å¼•æ“å’ŒAPIå±æ€§æå–å™¨ï¼Œæä¾›å®Œæ•´çš„é‡‘èAPIå­¦ä¹ å’Œç‰¹å¾åº“æ›´æ–°æµç¨‹ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. é›†æˆå­¦ä¹ æµç¨‹ - ç»“åˆå®½æ¾æ‰«æã€é‚»å±…åˆ†æã€æ¨¡å¼å­¦ä¹ å’Œå±æ€§æå–
2. æ·±åº¦å±æ€§æå– - å¯¹å­¦ä¹ åˆ°çš„APIè¿›è¡Œç»†è‡´çš„å±æ€§åˆ†æ
3. æ™ºèƒ½ç‰¹å¾åº“æ›´æ–° - å°†æå–çš„å±æ€§è¡¥å……åˆ°ç‰¹å¾åº“çš„æŒ‡å®šä½ç½®
4. è´¨é‡éªŒè¯ - ç¡®ä¿å­¦ä¹ è´¨é‡å’Œç‰¹å¾åº“ä¸€è‡´æ€§
5. å­¦ä¹ æŠ¥å‘Š - ç”Ÿæˆè¯¦ç»†çš„å­¦ä¹ å’Œæ›´æ–°æŠ¥å‘Š
"""

import json
import sys
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥ç°æœ‰æ¨¡å—
sys.path.append('mitmproxy2swagger/mitmproxy2swagger')
sys.path.append('mitmproxy2swagger/feature-library/ai_analysis_features')
sys.path.append('mitmproxy2swagger/feature-library/learning_engine')

from financial_api_learner import FinancialAPILearner, APICandidate
from api_attribute_extractor import APIAttributeExtractor, ExtractedAPIAttributes


class EnhancedLearningPipeline:
    """å¢å¼ºå­¦ä¹ ç®¡é“"""

    def __init__(self, feature_library_path: str = None):
        """åˆå§‹åŒ–å¢å¼ºå­¦ä¹ ç®¡é“"""
        self.logger = logging.getLogger(__name__)

        # åˆå§‹åŒ–ç»„ä»¶
        self.learner = FinancialAPILearner(feature_library_path)
        self.extractor = APIAttributeExtractor(feature_library_path)

        # ç®¡é“é…ç½®
        self.config = {
            'min_extraction_confidence': 0.6,  # æœ€å°æå–ç½®ä¿¡åº¦
            'max_apis_per_institution': 20,    # æ¯ä¸ªæœºæ„æœ€å¤§APIæ•°é‡
            'enable_feature_library_update': True,  # æ˜¯å¦æ›´æ–°ç‰¹å¾åº“
            'backup_before_update': True,      # æ›´æ–°å‰æ˜¯å¦å¤‡ä»½
            'quality_validation': True         # æ˜¯å¦è¿›è¡Œè´¨é‡éªŒè¯
        }

        # ç®¡é“çŠ¶æ€
        self.pipeline_stats = {
            'total_flows_processed': 0,
            'candidates_discovered': 0,
            'attributes_extracted': 0,
            'feature_library_updates': 0,
            'quality_issues': 0
        }

    def run_complete_pipeline(self, flows: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„å¢å¼ºå­¦ä¹ ç®¡é“"""
        self.logger.info("ğŸš€ å¼€å§‹å¢å¼ºå­¦ä¹ ç®¡é“...")

        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€å­¦ä¹ 
            self.logger.info("ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€å­¦ä¹ ï¼ˆå®½æ¾æ‰«æ + é‚»å±…åˆ†æ + æ¨¡å¼å­¦ä¹ ï¼‰")
            learning_report = self.learner.learn_from_flows(flows)

            if not learning_report['success']:
                return {
                    'success': False,
                    'error': 'Basic learning failed',
                    'learning_report': learning_report
                }

            self.pipeline_stats['total_flows_processed'] = learning_report['stats']['total_scanned']
            self.pipeline_stats['candidates_discovered'] = learning_report['stats']['candidates_found']

            # ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦å±æ€§æå–
            self.logger.info("ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦å±æ€§æå–")
            extracted_attributes = self._extract_attributes_from_candidates(
                self.learner.api_candidates, flows
            )

            self.pipeline_stats['attributes_extracted'] = len(extracted_attributes)

            # ç¬¬ä¸‰é˜¶æ®µï¼šè´¨é‡éªŒè¯
            if self.config['quality_validation']:
                self.logger.info("ç¬¬ä¸‰é˜¶æ®µï¼šè´¨é‡éªŒè¯")
                validated_attributes = self._validate_extracted_attributes(extracted_attributes)
            else:
                validated_attributes = extracted_attributes

            # ç¬¬å››é˜¶æ®µï¼šç‰¹å¾åº“æ›´æ–°
            update_report = {}
            if self.config['enable_feature_library_update'] and validated_attributes:
                self.logger.info("ç¬¬å››é˜¶æ®µï¼šç‰¹å¾åº“æ›´æ–°")
                update_report = self.extractor.update_feature_library_with_attributes(validated_attributes)
                self.pipeline_stats['feature_library_updates'] = update_report['successful_updates']

            # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
            complete_report = self._generate_complete_report(
                learning_report, extracted_attributes, validated_attributes, update_report
            )

            self.logger.info("ğŸ‰ å¢å¼ºå­¦ä¹ ç®¡é“å®Œæˆ")
            return complete_report

        except Exception as e:
            self.logger.error(f"å¢å¼ºå­¦ä¹ ç®¡é“å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'pipeline_stats': self.pipeline_stats
            }

    def _extract_attributes_from_candidates(self, candidates: List[APICandidate],
                                          flows: List[Dict[str, Any]]) -> List[ExtractedAPIAttributes]:
        """ä»å€™é€‰APIä¸­æå–å±æ€§"""
        extracted_attributes = []

        # åˆ›å»ºURLåˆ°flowçš„æ˜ å°„
        url_to_flow = {flow['url']: flow for flow in flows}

        for candidate in candidates:
            try:
                # æŸ¥æ‰¾å¯¹åº”çš„flowæ•°æ®
                flow_data = url_to_flow.get(candidate.url)
                if not flow_data:
                    continue

                # æå–å®Œæ•´å±æ€§
                attributes = self.extractor.extract_complete_attributes(
                    url=candidate.url,
                    method=candidate.method,
                    request_headers={},  # ä»flow_dataä¸­è·å–
                    request_body=flow_data.get('request_body', ''),
                    response_content=candidate.response_content,
                    response_headers={},  # ä»flow_dataä¸­è·å–
                    institution=self._identify_institution_from_candidate(candidate)
                )

                if attributes and attributes.extraction_confidence >= self.config['min_extraction_confidence']:
                    extracted_attributes.append(attributes)

            except Exception as e:
                self.logger.warning(f"æå–APIå±æ€§å¤±è´¥ {candidate.url}: {e}")
                continue

        self.logger.info(f"æˆåŠŸæå– {len(extracted_attributes)} ä¸ªAPIçš„å±æ€§")
        return extracted_attributes

    def _identify_institution_from_candidate(self, candidate: APICandidate) -> str:
        """ä»å€™é€‰APIè¯†åˆ«æœºæ„"""
        domain = candidate.domain

        # åŸºäºåŸŸåè¯†åˆ«æœºæ„
        if 'hsbc.com' in domain:
            return 'HSBC'
        elif 'bochk.com' in domain:
            return 'ä¸­é“¶é¦™æ¸¯'
        elif 'hangseng.com' in domain:
            return 'æ’ç”Ÿé“¶è¡Œ'
        elif 'dbs.com' in domain:
            return 'DBS'
        elif 'cmbwinglungbank.com' in domain:
            return 'æ°¸éš†é“¶è¡Œ'
        else:
            return f'Unknown_{domain}'

    def _validate_extracted_attributes(self, attributes: List[ExtractedAPIAttributes]) -> List[ExtractedAPIAttributes]:
        """éªŒè¯æå–çš„å±æ€§"""
        validated = []
        quality_issues = 0

        for attr in attributes:
            try:
                # åŸºæœ¬è´¨é‡æ£€æŸ¥
                if self._validate_single_attribute(attr):
                    validated.append(attr)
                else:
                    quality_issues += 1

            except Exception as e:
                self.logger.warning(f"éªŒè¯å±æ€§æ—¶å‡ºé”™ {attr.url}: {e}")
                quality_issues += 1

        self.pipeline_stats['quality_issues'] = quality_issues
        self.logger.info(f"è´¨é‡éªŒè¯å®Œæˆ: {len(validated)} ä¸ªé€šè¿‡éªŒè¯, {quality_issues} ä¸ªå­˜åœ¨é—®é¢˜")

        return validated

    def _validate_single_attribute(self, attr: ExtractedAPIAttributes) -> bool:
        """éªŒè¯å•ä¸ªå±æ€§"""
        # æ£€æŸ¥åŸºæœ¬å­—æ®µ
        if not attr.url or not attr.institution:
            return False

        # æ£€æŸ¥æå–ç½®ä¿¡åº¦
        if attr.extraction_confidence < self.config['min_extraction_confidence']:
            return False

        # æ£€æŸ¥ä¸šåŠ¡å±æ€§
        if attr.business_attrs.api_category == 'unknown':
            return False

        # æ£€æŸ¥å“åº”å±æ€§
        if attr.response_attrs.data_structure == 'empty':
            return False

        return True

    def _generate_complete_report(self, learning_report: Dict[str, Any],
                                extracted_attributes: List[ExtractedAPIAttributes],
                                validated_attributes: List[ExtractedAPIAttributes],
                                update_report: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""

        # ç»Ÿè®¡åˆ†æ
        institution_stats = {}
        category_stats = {}

        for attr in validated_attributes:
            # æœºæ„ç»Ÿè®¡
            inst = attr.institution
            if inst not in institution_stats:
                institution_stats[inst] = {
                    'api_count': 0,
                    'avg_confidence': 0.0,
                    'categories': set()
                }
            institution_stats[inst]['api_count'] += 1
            institution_stats[inst]['categories'].add(attr.business_attrs.api_category)

            # ç±»åˆ«ç»Ÿè®¡
            category = attr.business_attrs.api_category
            category_stats[category] = category_stats.get(category, 0) + 1

        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        for inst_data in institution_stats.values():
            inst_apis = [attr for attr in validated_attributes if attr.institution == inst]
            if inst_apis:
                inst_data['avg_confidence'] = sum(attr.extraction_confidence for attr in inst_apis) / len(inst_apis)
                inst_data['categories'] = list(inst_data['categories'])

        return {
            'success': True,
            'timestamp': datetime.now().isoformat(),
            'pipeline_config': self.config,
            'pipeline_stats': self.pipeline_stats,
            'learning_report': learning_report,
            'extraction_summary': {
                'total_extracted': len(extracted_attributes),
                'validated_count': len(validated_attributes),
                'validation_rate': len(validated_attributes) / len(extracted_attributes) if extracted_attributes else 0,
                'institution_stats': institution_stats,
                'category_stats': category_stats
            },
            'feature_library_update': update_report,
            'quality_metrics': {
                'avg_extraction_confidence': sum(attr.extraction_confidence for attr in validated_attributes) / len(validated_attributes) if validated_attributes else 0,
                'high_confidence_apis': len([attr for attr in validated_attributes if attr.extraction_confidence > 0.8]),
                'critical_priority_apis': len([attr for attr in validated_attributes if attr.business_attrs.priority_level == 'critical'])
            },
            'detailed_results': [
                {
                    'url': attr.url,
                    'institution': attr.institution,
                    'api_category': attr.business_attrs.api_category,
                    'value_score': attr.business_attrs.value_score,
                    'priority_level': attr.business_attrs.priority_level,
                    'extraction_confidence': attr.extraction_confidence,
                    'data_sensitivity': attr.business_attrs.data_sensitivity
                }
                for attr in validated_attributes
            ]
        }

    def export_pipeline_results(self, report: Dict[str, Any], output_path: str) -> bool:
        """å¯¼å‡ºç®¡é“ç»“æœ"""
        try:
            # å¤„ç†datetimeåºåˆ—åŒ–é—®é¢˜
            serializable_report = self._make_json_serializable(report)

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_report, f, indent=2, ensure_ascii=False)

            self.logger.info(f"ç®¡é“ç»“æœå·²å¯¼å‡ºåˆ°: {output_path}")
            return True

        except Exception as e:
            self.logger.error(f"å¯¼å‡ºç®¡é“ç»“æœå¤±è´¥: {e}")
            return False

    def _make_json_serializable(self, obj):
        """ä½¿å¯¹è±¡å¯JSONåºåˆ—åŒ–"""
        if isinstance(obj, datetime):
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {key: self._make_json_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, set):
            return list(obj)
        else:
            return obj


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='å¢å¼ºå­¦ä¹ ç®¡é“')
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥çš„mitmæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', default='enhanced_learning_report.json', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶')
    parser.add_argument('--feature-lib', '-f', help='ç‰¹å¾åº“æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--no-update', action='store_true', help='ä¸æ›´æ–°ç‰¹å¾åº“')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')

    args = parser.parse_args()

    # é…ç½®æ—¥å¿—
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # åˆå§‹åŒ–ç®¡é“
    pipeline = EnhancedLearningPipeline(args.feature_lib)

    if args.no_update:
        pipeline.config['enable_feature_library_update'] = False

    # è¯»å–mitmæ–‡ä»¶
    sys.path.append('mitmproxy2swagger/mitmproxy2swagger')
    from mitmproxy_capture_reader import MitmproxyCaptureReader

    try:
        print(f"ğŸ” è¯»å–mitmæ–‡ä»¶: {args.input}")
        capture_reader = MitmproxyCaptureReader(args.input)

        # è½¬æ¢ä¸ºæµæ•°æ®æ ¼å¼
        flows = []
        for flow_wrapper in capture_reader.captured_requests():
            try:
                flow_data = {
                    'url': flow_wrapper.get_url(),
                    'method': flow_wrapper.get_method(),
                    'status_code': flow_wrapper.get_response_status_code(),
                    'response_body': flow_wrapper.get_response_body().decode('utf-8', errors='ignore') if flow_wrapper.get_response_body() else '',
                    'request_body': flow_wrapper.get_request_body().decode('utf-8', errors='ignore') if flow_wrapper.get_request_body() else ''
                }
                flows.append(flow_data)
            except Exception as e:
                continue

        print(f"âœ… æˆåŠŸè¯»å– {len(flows)} ä¸ªæµæ•°æ®")

        # è¿è¡Œå¢å¼ºå­¦ä¹ ç®¡é“
        print("ğŸš€ å¼€å§‹å¢å¼ºå­¦ä¹ ç®¡é“...")
        report = pipeline.run_complete_pipeline(flows)

        # è¾“å‡ºç»“æœ
        if report['success']:
            print("ğŸ‰ å¢å¼ºå­¦ä¹ ç®¡é“å®Œæˆï¼")

            stats = report['pipeline_stats']
            extraction = report['extraction_summary']
            quality = report['quality_metrics']

            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"  æµæ•°æ®: {stats['total_flows_processed']}")
            print(f"  å€™é€‰API: {stats['candidates_discovered']}")
            print(f"  å±æ€§æå–: {stats['attributes_extracted']}")
            print(f"  éªŒè¯é€šè¿‡: {extraction['validated_count']}")
            print(f"  ç‰¹å¾åº“æ›´æ–°: {stats['feature_library_updates']}")

            print(f"ğŸ¯ è´¨é‡æŒ‡æ ‡:")
            print(f"  å¹³å‡ç½®ä¿¡åº¦: {quality['avg_extraction_confidence']:.3f}")
            print(f"  é«˜ç½®ä¿¡åº¦API: {quality['high_confidence_apis']}")
            print(f"  å…³é”®ä¼˜å…ˆçº§API: {quality['critical_priority_apis']}")

            print(f"ğŸ¦ æœºæ„åˆ†å¸ƒ:")
            for inst, data in extraction['institution_stats'].items():
                print(f"  {inst}: {data['api_count']} ä¸ªAPI (ç½®ä¿¡åº¦: {data['avg_confidence']:.3f})")

            # å¯¼å‡ºç»“æœ
            if pipeline.export_pipeline_results(report, args.output):
                print(f"ğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        else:
            print(f"âŒ å¢å¼ºå­¦ä¹ ç®¡é“å¤±è´¥: {report.get('error', 'Unknown error')}")

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        return 1

    return 0


if __name__ == '__main__':
    exit(main())
