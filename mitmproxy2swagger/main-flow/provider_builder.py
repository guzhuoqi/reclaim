#!/usr/bin/env python3
"""
Reclaim Provideræ„å»ºå™¨
Reclaim Provider Builder

åŸºäºç‰¹å¾åº“åˆ†æç»“æœå’ŒæŠ“åŒ…æ•°æ®ï¼Œè‡ªåŠ¨æ„å»ºReclaimæ ‡å‡†çš„provideré…ç½®
é‡ç‚¹åˆ†æresponseMatchesã€responseRedactionsç­‰å…³é”®å­—æ®µ

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. è¯»å–integrate_with_mitmproxy2swagger.pyçš„åˆ†æç»“æœ
2. å›æº¯åŸå§‹æŠ“åŒ…æ•°æ®ï¼Œæå–å®Œæ•´çš„è¯·æ±‚/å“åº”ä¿¡æ¯
3. åˆ†æHTTP headersä¸­çš„è®¤è¯ä¿¡æ¯
4. æ„å»ºresponseMatcheså’ŒresponseRedactions
5. ç”Ÿæˆå®Œæ•´çš„provideré…ç½®
6. è´¨é‡æ£€æŸ¥ï¼Œå°†ä¿¡æ¯ä¸è¶³çš„APIè¾“å‡ºåˆ°å­˜ç–‘æ–‡ä»¶
"""

import os
import sys
import json
import re
import hashlib
import uuid
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict
from urllib.parse import urlparse, parse_qs

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
sys.path.append(str(Path(__file__).parent.parent / "mitmproxy2swagger"))

from mitmproxy2swagger.mitmproxy_capture_reader import MitmproxyCaptureReader


@dataclass
class ProviderQualityCheck:
    """Providerè´¨é‡æ£€æŸ¥ç»“æœ"""
    has_authentication: bool = False
    has_response_data: bool = False
    has_financial_patterns: bool = False
    has_sufficient_headers: bool = False
    missing_fields: List[str] = None
    confidence_score: float = 0.0

    def __post_init__(self):
        if self.missing_fields is None:
            self.missing_fields = []


class ReclaimProviderBuilder:
    """Reclaim Provideræ„å»ºå™¨"""

    def __init__(self, mitm_file_path: str, analysis_result_file: str):
        """åˆå§‹åŒ–æ„å»ºå™¨

        Args:
            mitm_file_path: åŸå§‹mitmæ–‡ä»¶è·¯å¾„
            analysis_result_file: ç‰¹å¾åº“åˆ†æç»“æœæ–‡ä»¶è·¯å¾„
        """
        self.mitm_file_path = mitm_file_path
        self.analysis_result_file = analysis_result_file

        # åŠ è½½åˆ†æç»“æœ
        self.analysis_data = self.load_analysis_result()

        # åˆ›å»ºmitmè¯»å–å™¨
        self.capture_reader = MitmproxyCaptureReader(mitm_file_path)

        # å­˜å‚¨åŸå§‹æµæ•°æ®çš„æ˜ å°„
        self.flow_data_map = {}
        self.build_flow_data_map()

        # è®¤è¯ç›¸å…³çš„headeræ¨¡å¼
        self.auth_header_patterns = [
            'authorization', 'x-auth-token', 'x-api-key', 'x-session-token',
            'x-csrf-token', 'x-nonce', 'x-requested-with', 'x-target-unit',
            'cookie', 'set-cookie', 'session-id', 'jsessionid'
        ]

        # é‡è¦çš„headeræ¨¡å¼
        self.important_header_patterns = [
            'content-type', 'accept', 'user-agent', 'referer', 'origin',
            'sec-ch-ua', 'sec-fetch-', 'x-'
        ]

    def load_analysis_result(self) -> Dict[str, Any]:
        """åŠ è½½ç‰¹å¾åº“åˆ†æç»“æœ"""
        try:
            with open(self.analysis_result_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            raise Exception(f"æ— æ³•åŠ è½½åˆ†æç»“æœæ–‡ä»¶: {e}")

    def build_flow_data_map(self):
        """æ„å»ºæµæ•°æ®æ˜ å°„ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾åŸå§‹è¯·æ±‚/å“åº”æ•°æ®"""
        print("ğŸ” æ„å»ºæµæ•°æ®æ˜ å°„...")

        for flow_wrapper in self.capture_reader.captured_requests():
            url = flow_wrapper.get_url()

            # ğŸ¯ å®‰å…¨åœ°è·å–å“åº”ä½“ï¼Œå¤„ç†ç¼–ç é—®é¢˜
            try:
                response_body = flow_wrapper.get_response_body()
            except ValueError as e:
                if "Invalid Content-Encoding" in str(e):
                    print(f"âš ï¸  è·³è¿‡ç¼–ç æœ‰é—®é¢˜çš„å“åº”: {url}")
                    continue
                else:
                    raise

            # æå–å®Œæ•´çš„è¯·æ±‚/å“åº”æ•°æ®
            flow_data = {
                'url': url,
                'method': flow_wrapper.get_method(),
                'request_headers': dict(flow_wrapper.get_request_headers()),
                'response_headers': dict(flow_wrapper.get_response_headers()),
                'request_body': flow_wrapper.get_request_body(),
                'response_body': response_body,
                'status_code': flow_wrapper.get_response_status_code()
            }

            self.flow_data_map[url] = flow_data

        print(f"âœ… æ„å»ºå®Œæˆï¼Œå…±æ˜ å°„ {len(self.flow_data_map)} ä¸ªæµ")

    def extract_authentication_info(self, headers: Dict[str, str]) -> Dict[str, Any]:
        """æå–è®¤è¯ä¿¡æ¯

        Args:
            headers: HTTP headers

        Returns:
            è®¤è¯ä¿¡æ¯å­—å…¸
        """
        auth_info = {
            'has_auth': False,
            'auth_headers': [],
            'session_info': [],
            'csrf_tokens': [],
            'api_keys': []
        }

        for header_name, header_value in headers.items():
            header_lower = header_name.lower()

            # æ£€æŸ¥è®¤è¯ç›¸å…³header
            for pattern in self.auth_header_patterns:
                if pattern in header_lower:
                    auth_info['has_auth'] = True
                    auth_info['auth_headers'].append({
                        'name': header_name,
                        'value': header_value,
                        'type': self.classify_auth_header(header_name, header_value)
                    })
                    break

        return auth_info

    def classify_auth_header(self, name: str, value: str) -> str:
        """åˆ†ç±»è®¤è¯headerç±»å‹"""
        name_lower = name.lower()

        if 'authorization' in name_lower:
            return 'bearer_token' if 'bearer' in value.lower() else 'basic_auth'
        elif 'session' in name_lower or 'jsessionid' in name_lower:
            return 'session'
        elif 'csrf' in name_lower or 'xsrf' in name_lower:
            return 'csrf_token'
        elif 'nonce' in name_lower:
            return 'nonce'
        elif 'api-key' in name_lower:
            return 'api_key'
        elif 'cookie' in name_lower:
            return 'cookie'
        else:
            return 'custom'

    def extract_response_patterns(self, response_content: str, url: str, api_data: Dict = None) -> Tuple[List[Dict], List[Dict]]:
        """ä»å“åº”å†…å®¹ä¸­æå–æ¨¡å¼ï¼Œç”¨äºæ„å»ºresponseMatcheså’ŒresponseRedactions

        Args:
            response_content: å“åº”å†…å®¹
            url: API URL
            api_data: APIåˆ†ææ•°æ®ï¼ˆåŒ…å«matched_patternsï¼‰

        Returns:
            Tuple[List[Dict], List[Dict]]: (responseMatches, responseRedactions)
        """
        response_matches = []
        response_redactions = []

        if not response_content:
            return response_matches, response_redactions

        # ğŸ¯ æ­£ç¡®æ–¹æ¡ˆï¼šåŸºäºæ¯ä¸ªAPIçš„å®é™…åŒ¹é…æ¨¡å¼ç”Ÿæˆå¯¹åº”çš„æ­£åˆ™è¡¨è¾¾å¼
        if api_data and 'matched_patterns' in api_data:
            matched_patterns = api_data['matched_patterns']
            print(f"ğŸ” åŸºäºç‰¹å¾åº“åŒ¹é…ç»“æœç”Ÿæˆå“åº”æ¨¡å¼: {len(matched_patterns)} ä¸ªæ¨¡å¼")
            print(f"ğŸ” åŒ¹é…æ¨¡å¼: {matched_patterns}")

            order_counter = 1

            # ğŸ¯ æ ¹æ®å®é™…åŒ¹é…çš„æ¨¡å¼ç”Ÿæˆå¯¹åº”çš„æ­£åˆ™è¡¨è¾¾å¼
            for pattern in matched_patterns:
                if pattern.startswith("field:"):
                    # å­—æ®µåŒ¹é… - ç”Ÿæˆå­—æ®µéªŒè¯å’Œæå–è§„åˆ™
                    field_name = pattern.replace("field:", "")

                    response_matches.append({
                        "value": f'"{field_name}"',
                        "type": "contains",
                        "invert": False,
                        "description": f"éªŒè¯{field_name}å­—æ®µå­˜åœ¨",
                        "order": order_counter,
                        "isOptional": False
                    })

                    response_redactions.append({
                        "xPath": "",
                        "jsonPath": f"$.{field_name}",
                        "regex": f'"{field_name}":\\s*"?([^",\\}}]+)"?',
                        "hash": "sha256" if self._is_sensitive_field(field_name) else "",
                        "order": order_counter
                    })
                    order_counter += 1

                elif "content:balance" in pattern:
                    # ä½™é¢ç›¸å…³API - ç”Ÿæˆä½™é¢éªŒè¯å’Œæå–è§„åˆ™
                    response_matches.append({
                        "value": "\"balance\":\\s*[0-9]+",
                        "type": "regex",
                        "invert": False,
                        "description": "éªŒè¯ä½™é¢æ•°æ®æ ¼å¼",
                        "order": order_counter,
                        "isOptional": False
                    })

                    response_redactions.append({
                        "xPath": "",
                        "jsonPath": "$.balance",
                        "regex": "\"balance\":\\s*([0-9]+)",
                        "hash": "",
                        "order": order_counter
                    })
                    order_counter += 1

                elif "content:account" in pattern or "content:acc" in pattern:
                    # è´¦æˆ·ç›¸å…³API - ç”Ÿæˆè´¦æˆ·éªŒè¯è§„åˆ™
                    response_matches.append({
                        "value": "account",
                        "type": "contains",
                        "invert": False,
                        "description": "éªŒè¯å“åº”åŒ…å«è´¦æˆ·ä¿¡æ¯",
                        "order": order_counter,
                        "isOptional": True
                    })
                    order_counter += 1

                elif "content:login" in pattern or "content:logon" in pattern:
                    # ç™»å½•ç›¸å…³API - ç”Ÿæˆç™»å½•éªŒè¯è§„åˆ™
                    response_matches.append({
                        "value": "session|token|login|success",
                        "type": "regex",
                        "invert": False,
                        "description": "éªŒè¯ç™»å½•å“åº”åŒ…å«ä¼šè¯ä¿¡æ¯",
                        "order": order_counter,
                        "isOptional": True
                    })

                    response_redactions.append({
                        "xPath": "",
                        "jsonPath": "",
                        "regex": "(session[^\\s]*|token[^\\s]*)",
                        "hash": "sha256",
                        "order": order_counter
                    })
                    order_counter += 1

                elif pattern.startswith("core_banking:"):
                    # æ ¸å¿ƒé“¶è¡Œä¸šåŠ¡ - ç”Ÿæˆé‡‘èæ•°æ®éªŒè¯è§„åˆ™
                    response_matches.append({
                        "value": "\"amount\":\\s*[0-9]+|\"balance\":\\s*[0-9]+|\"value\":\\s*[0-9]+",
                        "type": "regex",
                        "invert": False,
                        "description": "éªŒè¯æ ¸å¿ƒé“¶è¡Œä¸šåŠ¡æ•°æ®",
                        "order": order_counter,
                        "isOptional": False
                    })

                    response_redactions.append({
                        "xPath": "",
                        "jsonPath": "",
                        "regex": "\"(?:amount|balance|value)\":\\s*([0-9]+)",
                        "hash": "",
                        "order": order_counter
                    })
                    order_counter += 1

            if response_matches or response_redactions:
                print(f"âœ… æˆåŠŸç”Ÿæˆ: {len(response_matches)} ä¸ªéªŒè¯è§„åˆ™, {len(response_redactions)} ä¸ªæå–è§„åˆ™")
                return response_matches, response_redactions
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°å¯è½¬æ¢çš„æ¨¡å¼ï¼Œä½¿ç”¨é€šç”¨è§„åˆ™")
                # ç”Ÿæˆé€šç”¨çš„éªŒè¯è§„åˆ™
                response_matches.append({
                    "value": "200",
                    "type": "contains",
                    "invert": False,
                    "description": "éªŒè¯HTTPå“åº”æˆåŠŸ",
                    "order": 1,
                    "isOptional": True
                })
                return response_matches, response_redactions

        # ğŸ”„ å›é€€ï¼šä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
        print(f"âš ï¸  å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•ç”Ÿæˆå“åº”æ¨¡å¼")
        try:
            # å°è¯•è§£æJSONå“åº”
            response_json = json.loads(response_content)

            # åˆ†æJSONç»“æ„ï¼Œæå–å…³é”®å­—æ®µ
            financial_patterns = self.analyze_json_financial_patterns(response_json)

            for pattern in financial_patterns:
                # æ„å»ºresponseMatches
                if pattern['type'] == 'amount':
                    response_matches.append({
                        "value": f'"{pattern["field"]}":{pattern["pattern"]}',
                        "type": "contains",
                        "invert": False,
                        "description": f"åŒ¹é…{pattern['description']}",
                        "order": None,
                        "isOptional": False
                    })

                    # æ„å»ºresponseRedactions
                    response_redactions.append({
                        "xPath": "",
                        "jsonPath": pattern['json_path'],
                        "regex": f'"{pattern["field"]}":(.*)',
                        "hash": "",
                        "order": None
                    })

                elif pattern['type'] == 'account':
                    response_matches.append({
                        "value": f'"{pattern["field"]}":"{pattern["pattern"]}"',
                        "type": "contains",
                        "invert": False,
                        "description": f"åŒ¹é…{pattern['description']}",
                        "order": None,
                        "isOptional": False
                    })

                    response_redactions.append({
                        "xPath": "",
                        "jsonPath": pattern['json_path'],
                        "regex": f'"{pattern["field"]}":"(.*)"',
                        "hash": "",
                        "order": None
                    })

        except json.JSONDecodeError:
            # éJSONå“åº”ï¼Œä½¿ç”¨æ–‡æœ¬æ¨¡å¼åˆ†æ
            text_patterns = self.analyze_text_financial_patterns(response_content)

            for pattern in text_patterns:
                response_matches.append({
                    "value": pattern['regex'],
                    "type": "regex",
                    "invert": False,
                    "description": pattern['description'],
                    "order": None,
                    "isOptional": False
                })

        return response_matches, response_redactions

    def analyze_json_financial_patterns(self, json_data: Any, path: str = "$") -> List[Dict]:
        """åˆ†æJSONæ•°æ®ä¸­çš„é‡‘èæ¨¡å¼"""
        patterns = []

        if isinstance(json_data, dict):
            for key, value in json_data.items():
                current_path = f"{path}.{key}"

                # æ£€æŸ¥é‡‘é¢å­—æ®µ
                if self.is_amount_field(key, value):
                    patterns.append({
                        'field': key,
                        'type': 'amount',
                        'pattern': '{{credit_amount}}' if 'credit' in key.lower() else '{{amount}}',
                        'json_path': current_path,
                        'description': 'é‡‘é¢å­—æ®µ'
                    })

                # æ£€æŸ¥è´¦æˆ·å­—æ®µ
                elif self.is_account_field(key, value):
                    patterns.append({
                        'field': key,
                        'type': 'account',
                        'pattern': '{{account_number}}',
                        'json_path': current_path,
                        'description': 'è´¦æˆ·å­—æ®µ'
                    })

                # æ£€æŸ¥äº¤æ˜“IDå­—æ®µ
                elif self.is_transaction_field(key, value):
                    patterns.append({
                        'field': key,
                        'type': 'transaction',
                        'pattern': '{{trans_id}}',
                        'json_path': current_path,
                        'description': 'äº¤æ˜“IDå­—æ®µ'
                    })

                # é€’å½’å¤„ç†åµŒå¥—å¯¹è±¡
                if isinstance(value, (dict, list)):
                    patterns.extend(self.analyze_json_financial_patterns(value, current_path))

        elif isinstance(json_data, list):
            for i, item in enumerate(json_data):
                current_path = f"{path}[{i}]"
                patterns.extend(self.analyze_json_financial_patterns(item, current_path))

        return patterns

    def is_amount_field(self, key: str, value: Any) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé‡‘é¢å­—æ®µ"""
        amount_keywords = ['amount', 'balance', 'value', 'total', 'sum', 'é‡‘é¢', 'ä½™é¢', 'æ€»é¢']
        key_lower = key.lower()

        # æ£€æŸ¥å­—æ®µå
        if any(keyword in key_lower for keyword in amount_keywords):
            # æ£€æŸ¥å€¼æ˜¯å¦ä¸ºæ•°å­—
            if isinstance(value, (int, float)) or (isinstance(value, str) and value.replace('.', '').replace(',', '').isdigit()):
                return True

        return False

    def is_account_field(self, key: str, value: Any) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè´¦æˆ·å­—æ®µ"""
        account_keywords = ['account', 'acct', 'number', 'id', 'è´¦æˆ·', 'è´¦å·']
        key_lower = key.lower()

        if any(keyword in key_lower for keyword in account_keywords):
            if isinstance(value, str) and len(value) > 5:  # è´¦æˆ·å·é€šå¸¸è¾ƒé•¿
                return True

        return False

    def is_transaction_field(self, key: str, value: Any) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“å­—æ®µ"""
        transaction_keywords = ['transaction', 'trans', 'txn', 'reference', 'cheque', 'äº¤æ˜“', 'æµæ°´']
        key_lower = key.lower()

        if any(keyword in key_lower for keyword in transaction_keywords):
            if isinstance(value, str):
                return True

        return False

    def analyze_text_financial_patterns(self, text: str) -> List[Dict]:
        """åˆ†ææ–‡æœ¬ä¸­çš„é‡‘èæ¨¡å¼"""
        patterns = []

        # é‡‘é¢æ¨¡å¼
        amount_patterns = [
            (r'ä½™é¢[ï¼š:]\s*([0-9,]+\.?\d*)', 'ä½™é¢åŒ¹é…'),
            (r'é‡‘é¢[ï¼š:]\s*([0-9,]+\.?\d*)', 'é‡‘é¢åŒ¹é…'),
            (r'è´¦æˆ·ä½™é¢[ï¼š:]\s*([0-9,]+\.?\d*)', 'è´¦æˆ·ä½™é¢åŒ¹é…')
        ]

        for pattern, description in amount_patterns:
            if re.search(pattern, text):
                patterns.append({
                    'regex': pattern,
                    'description': description,
                    'type': 'amount'
                })

        return patterns

    def calculate_request_hash(self, url: str, method: str, headers: Dict[str, str]) -> str:
        """è®¡ç®—è¯·æ±‚å“ˆå¸Œ"""
        # æ„å»ºç”¨äºå“ˆå¸Œçš„å­—ç¬¦ä¸²
        hash_string = f"{method}:{url}:{json.dumps(sorted(headers.items()))}"

        # è®¡ç®—SHA256å“ˆå¸Œ
        hash_object = hashlib.sha256(hash_string.encode())
        return f"0x{hash_object.hexdigest()}"

    def filter_important_headers(self, headers: Dict[str, str]) -> Dict[str, str]:
        """è¿‡æ»¤å‡ºé‡è¦çš„headers"""
        important_headers = {}

        for name, value in headers.items():
            name_lower = name.lower()

            # æ£€æŸ¥æ˜¯å¦ä¸ºé‡è¦header
            is_important = False

            # è®¤è¯ç›¸å…³header
            for pattern in self.auth_header_patterns:
                if pattern in name_lower:
                    is_important = True
                    break

            # å…¶ä»–é‡è¦header
            if not is_important:
                for pattern in self.important_header_patterns:
                    if pattern in name_lower:
                        is_important = True
                        break

            if is_important:
                important_headers[name] = value

        return important_headers

    def perform_quality_check(self, api_data: Dict[str, Any], flow_data: Dict[str, Any]) -> ProviderQualityCheck:
        """æ‰§è¡Œè´¨é‡æ£€æŸ¥"""
        check = ProviderQualityCheck()

        # æ£€æŸ¥è®¤è¯ä¿¡æ¯
        auth_info = self.extract_authentication_info(flow_data['request_headers'])
        check.has_authentication = auth_info['has_auth']
        if not check.has_authentication:
            check.missing_fields.append('authentication_headers')

        # æ£€æŸ¥å“åº”æ•°æ®
        response_body = flow_data.get('response_body')
        if response_body:
            try:
                response_content = response_body.decode('utf-8', errors='ignore')
                check.has_response_data = len(response_content) > 100  # è‡³å°‘100å­—ç¬¦

                # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡‘èæ¨¡å¼
                financial_keywords = ['balance', 'amount', 'account', 'transaction', 'ä½™é¢', 'é‡‘é¢', 'è´¦æˆ·']
                check.has_financial_patterns = any(keyword in response_content.lower() for keyword in financial_keywords)
            except:
                check.has_response_data = False

        if not check.has_response_data:
            check.missing_fields.append('response_data')
        if not check.has_financial_patterns:
            check.missing_fields.append('financial_patterns')

        # æ£€æŸ¥headeræ•°é‡
        important_headers = self.filter_important_headers(flow_data['request_headers'])
        check.has_sufficient_headers = len(important_headers) >= 3
        if not check.has_sufficient_headers:
            check.missing_fields.append('sufficient_headers')

        # è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°
        score = 0
        if check.has_authentication:
            score += 30
        if check.has_response_data:
            score += 25
        if check.has_financial_patterns:
            score += 25
        if check.has_sufficient_headers:
            score += 20

        check.confidence_score = score / 100.0

        return check

    def build_provider_for_api(self, api_data: Dict[str, Any]) -> Tuple[Optional[Dict], Optional[ProviderQualityCheck]]:
        """ä¸ºå•ä¸ªAPIæ„å»ºprovideré…ç½®

        Args:
            api_data: APIåˆ†ææ•°æ®

        Returns:
            Tuple[Optional[Dict], Optional[ProviderQualityCheck]]: (provideré…ç½®, è´¨é‡æ£€æŸ¥ç»“æœ)
        """
        url = api_data['url']

        # è·å–åŸå§‹æµæ•°æ®
        flow_data = self.flow_data_map.get(url)
        if not flow_data:
            print(f"âš ï¸  æœªæ‰¾åˆ°URLçš„æµæ•°æ®: {url}")
            return None, None

        # æ‰§è¡Œè´¨é‡æ£€æŸ¥
        quality_check = self.perform_quality_check(api_data, flow_data)

        # å¦‚æœè´¨é‡æ£€æŸ¥ä¸é€šè¿‡ï¼Œè¿”å›æ£€æŸ¥ç»“æœç”¨äºå­˜ç–‘æ–‡ä»¶
        if quality_check.confidence_score < 0.6:  # 60%ç½®ä¿¡åº¦é˜ˆå€¼
            return None, quality_check

        # è§£æå“åº”å†…å®¹
        response_content = ""
        if flow_data['response_body']:
            try:
                response_content = flow_data['response_body'].decode('utf-8', errors='ignore')
            except:
                response_content = ""

        # ğŸ¯ æå–å“åº”æ¨¡å¼ - ä¼ å…¥APIæ•°æ®ä»¥åˆ©ç”¨ç‰¹å¾åº“åŒ¹é…ç»“æœ
        response_matches, response_redactions = self.extract_response_patterns(response_content, url, api_data)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å“åº”æ¨¡å¼ï¼Œé™çº§å¤„ç†
        if not response_matches and not response_redactions:
            print(f"âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆçš„å“åº”æ¨¡å¼: {url}")
            quality_check.missing_fields.append('response_patterns')
            quality_check.confidence_score *= 0.7  # é™ä½ç½®ä¿¡åº¦

            if quality_check.confidence_score < 0.6:
                return None, quality_check

        # æå–é‡è¦çš„headers
        important_headers = self.filter_important_headers(flow_data['request_headers'])

        # è®¡ç®—è¯·æ±‚å“ˆå¸Œ
        request_hash = self.calculate_request_hash(url, flow_data['method'], important_headers)

        # æ„å»ºprovideré…ç½®
        provider_config = {
            "providerConfig": {
                "id": str(uuid.uuid4()).replace('-', '')[:24],  # 24å­—ç¬¦ID
                "createdAt": None,
                "providerId": str(uuid.uuid4()),
                "version": {
                    "major": 1,
                    "minor": 0,
                    "patch": 0,
                    "prereleaseTag": None,
                    "prereleaseNumber": None
                },
                "providerConfig": {
                    "loginUrl": self.extract_login_url(url),
                    "customInjection": self.generate_custom_injection(api_data, flow_data),
                    "userAgent": {
                        "ios": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1",
                        "android": None
                    },
                    "geoLocation": self.extract_geo_location(flow_data),
                    "injectionType": "NONE",
                    "disableRequestReplay": True,
                    "verificationType": "WITNESS",
                    "requestData": [
                        {
                            "url": url,
                            "expectedPageUrl": "",
                            "urlType": "CONSTANT",
                            "method": flow_data['method'],
                            "responseMatches": response_matches,
                            "responseRedactions": response_redactions,
                            "bodySniff": {
                                "enabled": False,
                                "template": ""
                            },
                            "requestHash": request_hash,
                            "responseVariables": self.extract_response_variables(response_matches, response_redactions)
                        }
                    ],
                    "pageTitle": None,
                    "metadata": {
                        "institution": api_data.get('institution', ''),
                        "api_type": self.classify_api_type(url, response_content),
                        "value_score": api_data.get('value_score', 0),
                        "priority_level": api_data.get('priority_level', 'medium'),
                        "generated_at": datetime.now().isoformat(),
                        "confidence_score": quality_check.confidence_score
                    },
                    "stepsToFollow": None,
                    "useIncognitoWebview": None
                },
                "createdBy": "auto_generated_provider_builder"
            }
        }

        return provider_config, quality_check

    def extract_login_url(self, api_url: str) -> str:
        """é€šè¿‡ä¸Šä¸‹æ–‡åˆ†ææå–çœŸå®çš„ç™»å½•URL

        Args:
            api_url: å½“å‰APIçš„URL

        Returns:
            str: ç™»å½•URLæˆ–æç¤ºä¿¡æ¯
        """
        parsed = urlparse(api_url)
        domain = parsed.netloc

        # ğŸ¯ ä¸Šä¸‹æ–‡åˆ†æï¼šåœ¨åŒåŸŸåçš„è®¤è¯ç±»APIä¸­æŸ¥æ‰¾çœŸå®ç™»å½•URL
        real_login_url = self._find_real_login_url_from_context(domain)

        if real_login_url:
            return real_login_url

        # ğŸ¯ å¦‚æœæ²¡æœ‰æ‰¾åˆ°çœŸå®ç™»å½•URLï¼Œè¿”å›æç¤ºä¿¡æ¯
        return f"[éœ€è¦ä¸Šä¸‹æ–‡åˆ†æ] æœªåœ¨æŠ“åŒ…æ•°æ®ä¸­æ‰¾åˆ° {domain} çš„ç™»å½•URLï¼Œè¯·æ‰‹åŠ¨ç¡®è®¤"

    def _find_real_login_url_from_context(self, domain: str) -> Optional[str]:
        """ä»ä¸Šä¸‹æ–‡ä¸­æŸ¥æ‰¾çœŸå®çš„ç™»å½•URL

        Args:
            domain: ç›®æ ‡åŸŸå

        Returns:
            Optional[str]: æ‰¾åˆ°çš„ç™»å½•URLï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°è¿”å›None
        """
        # ä»åˆ†ææ•°æ®ä¸­æŸ¥æ‰¾åŒåŸŸåçš„è®¤è¯ç±»API
        if not hasattr(self, 'analysis_data') or not self.analysis_data:
            return None

        extracted_data = self.analysis_data.get('extracted_data', [])

        login_candidates = []

        for api_data in extracted_data:
            api_url = api_data.get('url', '')
            api_category = api_data.get('api_category', 'unknown')

            # ğŸ¯ ç¬¬ä¸€æ­¥ï¼šè¯†åˆ«ç™»å½•æäº¤é¡µï¼ˆä¼˜å…ˆçº§é«˜ï¼‰
            if domain in api_url and api_category == 'auth':
                flow_data = self.flow_data_map.get(api_url)
                if flow_data:
                    # è¯„åˆ†ç™»å½•æäº¤é¡µ
                    submit_score = self._score_login_submit_api(api_url, flow_data)
                    if submit_score > 20:  # åªæœ‰é«˜åˆ†çš„æ‰è®¤ä¸ºæ˜¯ç™»å½•æäº¤é¡µ
                        login_candidates.append({
                            'url': api_url,
                            'score': submit_score,
                            'type': 'submit',
                            'flow_data': flow_data
                        })
                        print(f"ğŸ” å‘ç°ç™»å½•æäº¤é¡µå€™é€‰: {api_url} (è¯„åˆ†: {submit_score})")

        # ğŸ¯ ç¬¬äºŒæ­¥ï¼šåŸºäºç™»å½•æäº¤é¡µæ‰¾å¯¹åº”çš„ç™»å½•é¡µ
        if login_candidates:
            # é€‰æ‹©è¯„åˆ†æœ€é«˜çš„ç™»å½•æäº¤é¡µ
            best_submit = max(login_candidates, key=lambda x: x['score'])
            print(f"ğŸ¯ æœ€ä½³ç™»å½•æäº¤é¡µ: {best_submit['url']} (è¯„åˆ†: {best_submit['score']})")

            # å°è¯•æ‰¾åˆ°å¯¹åº”çš„ç™»å½•é¡µé¢
            login_page = self._find_corresponding_login_page(domain, best_submit)
            if login_page:
                print(f"ğŸ” æ‰¾åˆ°å¯¹åº”çš„ç™»å½•é¡µé¢: {login_page}")
                return login_page
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ç™»å½•é¡µé¢ï¼Œè¿”å›ç™»å½•æäº¤é¡µçš„URLï¼ˆå»æ‰å‚æ•°ï¼‰
                submit_url = best_submit['url'].split('?')[0]
                print(f"âš ï¸  æœªæ‰¾åˆ°ç™»å½•é¡µé¢ï¼Œä½¿ç”¨ç™»å½•æäº¤é¡µ: {submit_url}")
                return submit_url

        if login_candidates:
            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ç™»å½•URL
            best_candidate = max(login_candidates, key=lambda x: x['score'])
            print(f"ğŸ” é€šè¿‡ä¸Šä¸‹æ–‡åˆ†ææ‰¾åˆ°ç™»å½•URL: {best_candidate['url']}")
            return best_candidate['url']

        # ğŸ¯ å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç™»å½•æäº¤é¡µï¼Œä½¿ç”¨å…¨é‡æµæ•°æ®åˆ†æ
        print(f"âš ï¸  ç‰¹å¾åº“æœªè¯†åˆ«åˆ°ç™»å½•æäº¤é¡µï¼Œå¯ç”¨å…¨é‡æµæ•°æ®åˆ†æ...")
        discovered_submit = self._discover_login_submit_by_behavior(domain)
        if discovered_submit:
            print(f"ğŸ¯ é€šè¿‡è¡Œä¸ºåˆ†æå‘ç°ç™»å½•æäº¤é¡µ: {discovered_submit['url']} (è¯„åˆ†: {discovered_submit['score']})")

            # å°è¯•æ‰¾åˆ°å¯¹åº”çš„ç™»å½•é¡µé¢
            login_page = self._find_corresponding_login_page(domain, discovered_submit)
            if login_page:
                print(f"ğŸ” æ‰¾åˆ°å¯¹åº”çš„ç™»å½•é¡µé¢: {login_page}")
                return login_page
            else:
                # è¿”å›ç™»å½•æäº¤é¡µçš„URLï¼ˆå»æ‰å‚æ•°ï¼‰
                submit_url = discovered_submit['url'].split('?')[0]
                print(f"âš ï¸  æœªæ‰¾åˆ°ç™»å½•é¡µé¢ï¼Œä½¿ç”¨ç™»å½•æäº¤é¡µ: {submit_url}")
                return submit_url

        return None

    def _discover_login_submit_by_behavior(self, domain: str) -> Optional[Dict]:
        """é€šè¿‡è¡Œä¸ºç‰¹å¾å‘ç°ç™»å½•æäº¤é¡µï¼ˆç»•è¿‡ç‰¹å¾åº“é™åˆ¶ï¼‰

        Args:
            domain: ç›®æ ‡åŸŸå

        Returns:
            Optional[Dict]: å‘ç°çš„ç™»å½•æäº¤é¡µä¿¡æ¯
        """
        if not hasattr(self, 'flow_data_map'):
            return None

        candidates = []

        # ğŸ¯ éå†æ‰€æœ‰æµæ•°æ®ï¼Œå¯»æ‰¾ç™»å½•æäº¤çš„è¡Œä¸ºç‰¹å¾
        for url, flow_data in self.flow_data_map.items():
            # å¿…é¡»æ˜¯åŒåŸŸå
            if domain not in url:
                continue

            # ğŸ¯ æ ¸å¿ƒç®—æ³•ï¼šPOST + è®¤è¯å­—æ®µ = ç™»å½•æäº¤
            method = flow_data.get('method', '').upper()
            if method != 'POST':
                continue

            # æ£€æŸ¥è¯·æ±‚ä½“æ˜¯å¦åŒ…å«è®¤è¯å­—æ®µ
            request_body = flow_data.get('request_body', '')
            if not request_body:
                continue

            # å¤„ç†bytesç±»å‹
            if isinstance(request_body, bytes):
                try:
                    request_body = request_body.decode('utf-8', errors='ignore')
                except:
                    request_body = str(request_body)

            request_body_lower = request_body.lower()

            # ğŸ¯ æ£€æµ‹è®¤è¯å­—æ®µï¼ˆæ›´å…¨é¢çš„å…³é”®å­—ï¼‰
            auth_indicators = [
                'loginid', 'userid', 'username', 'user', 'login',
                'password', 'passwd', 'pwd', 'pass',
                'vercode', 'captcha', 'verify'
            ]

            auth_field_count = 0
            for indicator in auth_indicators:
                if indicator in request_body_lower:
                    auth_field_count += 1

            # è‡³å°‘åŒ…å«2ä¸ªè®¤è¯ç›¸å…³å­—æ®µæ‰è®¤ä¸ºæ˜¯ç™»å½•æäº¤
            if auth_field_count >= 2:
                score = self._score_login_submit_api(url, flow_data)
                candidates.append({
                    'url': url,
                    'score': score,
                    'auth_field_count': auth_field_count,
                    'flow_data': flow_data
                })
                print(f"ğŸ” å‘ç°ç™»å½•æäº¤å€™é€‰: {url} (è®¤è¯å­—æ®µ: {auth_field_count}, è¯„åˆ†: {score})")

        if candidates:
            # é€‰æ‹©è¯„åˆ†æœ€é«˜çš„å€™é€‰
            best_candidate = max(candidates, key=lambda x: x['score'])
            return best_candidate

        return None

    def _score_login_url(self, url: str) -> int:
        """ä¸ºç™»å½•URLæ‰“åˆ†ï¼Œç”¨äºé€‰æ‹©æœ€ä½³å€™é€‰

        Args:
            url: ç™»å½•URL

        Returns:
            int: å¾—åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        """
        score = 0
        url_lower = url.lower()

        # æ˜ç¡®çš„ç™»å½•å…³é”®å­—å¾—åˆ†æ›´é«˜
        if 'logon' in url_lower:
            score += 10
        elif 'login' in url_lower:
            score += 8
        elif 'lgn' in url_lower:  # ä¸­é“¶é¦™æ¸¯çš„ç¼©å†™
            score += 9
        elif 'signin' in url_lower:
            score += 6
        elif 'auth' in url_lower:
            score += 4
        elif 'default' in url_lower and any(x in url_lower for x in ['lgn', 'login']):
            score += 7  # lgn.default.do è¿™ç§æ¨¡å¼

        # ç‰¹å®šçš„ç™»å½•æ–‡ä»¶æ‰©å±•å
        if url_lower.endswith('.do'):  # ä¸­é“¶é¦™æ¸¯ä½¿ç”¨çš„Strutsæ¡†æ¶
            score += 5
        elif 'servlet' in url_lower:  # æ°¸éš†é“¶è¡Œä½¿ç”¨çš„Servlet
            score += 5
        elif url_lower.endswith('.jsp'):  # JSPé¡µé¢
            score += 3

        # è·¯å¾„ç‰¹å¾
        if '/lgn/' in url_lower or '/login/' in url_lower:
            score += 4

        # è·¯å¾„è¶ŠçŸ­é€šå¸¸è¶Šæ˜¯ä¸»è¦ç™»å½•å…¥å£
        path_segments = url.split('/')
        if len(path_segments) <= 5:
            score += 3
        elif len(path_segments) <= 3:
            score += 5  # éå¸¸çŸ­çš„è·¯å¾„ï¼Œå¯èƒ½æ˜¯ä¸»å…¥å£

        # é¿å…æ˜æ˜¾çš„éç™»å½•URL
        if any(exclude in url_lower for exclude in ['overview', 'balance', 'account', 'transaction']):
            score -= 5

        return score

    def _score_login_submit_api(self, url: str, flow_data: Dict[str, Any]) -> int:
        """è¯„åˆ†ç™»å½•æäº¤é¡µAPIï¼ˆç¬¬ä¸€ä¼˜å…ˆçº§ï¼‰

        Args:
            url: API URL
            flow_data: æµæ•°æ®

        Returns:
            int: ç™»å½•æäº¤é¡µè¯„åˆ†
        """
        score = 0
        url_lower = url.lower()

        # ğŸ¯ URLå…³é”®å­—è¯„åˆ†
        submit_keywords = ['login', 'logon', 'authenticate', 'signin', 'submit', 'dologin']
        for keyword in submit_keywords:
            if keyword in url_lower:
                score += 10
                break

        # ğŸ¯ HTTPæ–¹æ³•è¯„åˆ†ï¼ˆPOSTé€šå¸¸æ˜¯æäº¤ï¼‰
        method = flow_data.get('method', '').upper()
        if method == 'POST':
            score += 15

        # ğŸ¯ è¯·æ±‚ä½“åˆ†æï¼ˆåŒ…å«è®¤è¯ä¿¡æ¯ï¼‰
        request_body = flow_data.get('request_body', '')
        if request_body:
            if isinstance(request_body, bytes):
                try:
                    request_body = request_body.decode('utf-8', errors='ignore')
                except:
                    request_body = str(request_body)

            auth_fields = ['username', 'password', 'userid', 'pwd', 'user', 'pass']
            for field in auth_fields:
                if field in request_body.lower():
                    score += 20  # åŒ…å«è®¤è¯å­—æ®µï¼Œå¾ˆå¯èƒ½æ˜¯ç™»å½•æäº¤
                    break

        # ğŸ¯ å“åº”å¤´åˆ†æï¼ˆè®¾ç½®è®¤è¯ä¿¡æ¯ï¼‰
        response_headers = flow_data.get('response_headers', {})
        set_cookie = response_headers.get('Set-Cookie', '')
        if isinstance(set_cookie, list):
            set_cookie = '; '.join(set_cookie) if set_cookie else ''

        if set_cookie:
            auth_cookie_keywords = ['session', 'jsessionid', 'token', 'auth']
            for keyword in auth_cookie_keywords:
                if keyword.lower() in set_cookie.lower():
                    score += 15
                    break

        # ğŸ¯ å“åº”å†…å®¹åˆ†æï¼ˆç®€çŸ­å…³é”®å­—ï¼‰
        response_body = flow_data.get('response_body', '')
        if response_body:
            if isinstance(response_body, bytes):
                try:
                    response_body = response_body.decode('utf-8', errors='ignore')
                except:
                    response_body = str(response_body)

            response_lower = response_body.lower()
            auth_response_keywords = ['token', 'authority', 'code', 'session', 'redirect', 'success']
            for keyword in auth_response_keywords:
                if keyword in response_lower:
                    score += 8

        # ğŸ¯ çŠ¶æ€ç åˆ†æ
        status_code = flow_data.get('status_code', 0)
        if status_code in [302, 301]:  # é‡å®šå‘ï¼Œå¯èƒ½æ˜¯ç™»å½•æˆåŠŸ
            score += 10
        elif status_code == 200:
            score += 5

        return score

    def _score_login_api_by_flow_data(self, url: str, flow_data: Dict[str, Any]) -> int:
        """åŸºäºå®é™…çš„è¯·æ±‚/åº”ç­”æµæ•°æ®æ¥è¯„åˆ†ç™»å½•API

        Args:
            url: API URL
            flow_data: æµæ•°æ®ï¼ˆåŒ…å«è¯·æ±‚å’Œåº”ç­”ä¿¡æ¯ï¼‰

        Returns:
            int: å¾—åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        """
        score = 0

        # åŸºç¡€URLè¯„åˆ†
        url_lower = url.lower()
        if 'lgn' in url_lower:
            score += 5
        elif 'login' in url_lower or 'logon' in url_lower:
            score += 3

        # ğŸ¯ è¯·æ±‚ç‰¹å¾åˆ†æ
        method = flow_data.get('method', '').upper()
        request_headers = flow_data.get('request_headers', {})
        request_body = flow_data.get('request_body', '')

        # POSTæ–¹æ³•é€šå¸¸æ˜¯ç™»å½•æäº¤
        if method == 'POST':
            score += 10
        elif method == 'GET':
            score += 2  # å¯èƒ½æ˜¯ç™»å½•é¡µé¢

        # è¯·æ±‚ä½“ç‰¹å¾
        if request_body:
            # å¤„ç†bytesç±»å‹çš„è¯·æ±‚ä½“
            if isinstance(request_body, bytes):
                try:
                    request_body = request_body.decode('utf-8', errors='ignore')
                except:
                    request_body = str(request_body)
            request_body_lower = str(request_body).lower()

            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç™»å½•ç›¸å…³å­—æ®µ
            login_fields = ['username', 'password', 'userid', 'pwd', 'user', 'pass', 'account']
            for field in login_fields:
                if field in request_body_lower:
                    score += 8
                    break

        # Content-Typeæ£€æŸ¥
        content_type = request_headers.get('Content-Type', '')
        if isinstance(content_type, list):
            content_type = content_type[0] if content_type else ''
        content_type = str(content_type).lower()

        if 'application/x-www-form-urlencoded' in content_type:
            score += 5  # è¡¨å•æäº¤
        elif 'application/json' in content_type:
            score += 3  # JSONæäº¤

        # ğŸ¯ åº”ç­”ç‰¹å¾åˆ†æ
        response_headers = flow_data.get('response_headers', {})
        response_body = flow_data.get('response_body', '')
        status_code = flow_data.get('status_code', 0)

        # çŠ¶æ€ç åˆ†æ
        if status_code == 302 or status_code == 301:
            score += 8  # é‡å®šå‘ï¼Œå¯èƒ½æ˜¯ç™»å½•æˆåŠŸåè·³è½¬
        elif status_code == 200:
            score += 5  # æ­£å¸¸å“åº”
        elif status_code >= 400:
            score -= 5  # é”™è¯¯å“åº”ï¼Œå¯èƒ½ä¸æ˜¯çœŸæ­£çš„ç™»å½•API

        # æ£€æŸ¥Set-Cookieå¤´ï¼ˆç™»å½•é€šå¸¸ä¼šè®¾ç½®session cookieï¼‰
        set_cookie = response_headers.get('Set-Cookie', '')
        if isinstance(set_cookie, list):
            set_cookie = '; '.join(set_cookie) if set_cookie else ''
        set_cookie = str(set_cookie)

        if set_cookie:
            cookie_lower = set_cookie.lower()
            if any(keyword in cookie_lower for keyword in ['session', 'jsessionid', 'token', 'auth']):
                score += 10
            else:
                score += 3  # ä»»ä½•cookieéƒ½å¯èƒ½æ˜¯ç™»å½•ç›¸å…³

        # æ£€æŸ¥Locationå¤´ï¼ˆé‡å®šå‘ç›®æ ‡ï¼‰
        location = response_headers.get('Location', '')
        if isinstance(location, list):
            location = location[0] if location else ''
        location = str(location)

        if location:
            location_lower = location.lower()
            if any(keyword in location_lower for keyword in ['main', 'home', 'index', 'welcome', 'dashboard']):
                score += 8  # é‡å®šå‘åˆ°ä¸»é¡µï¼Œå¾ˆå¯èƒ½æ˜¯ç™»å½•æˆåŠŸ

        # ğŸ¯ åº”ç­”å†…å®¹åˆ†æ
        if response_body:
            # å¤„ç†bytesç±»å‹çš„å“åº”ä½“
            if isinstance(response_body, bytes):
                try:
                    response_body = response_body.decode('utf-8', errors='ignore')
                except:
                    response_body = str(response_body)
            response_body_lower = str(response_body).lower()

            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç™»å½•æˆåŠŸçš„æ ‡è¯†
            success_indicators = ['welcome', 'dashboard', 'logout', 'account', 'balance']
            for indicator in success_indicators:
                if indicator in response_body_lower:
                    score += 5
                    break

            # æ£€æŸ¥æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯ï¼ˆå¯èƒ½æ˜¯ç™»å½•å¤±è´¥ï¼‰
            error_indicators = ['error', 'invalid', 'incorrect', 'failed', 'wrong']
            for indicator in error_indicators:
                if indicator in response_body_lower:
                    score += 3  # æœ‰é”™è¯¯ä¿¡æ¯ä¹Ÿè¯´æ˜æ˜¯ç™»å½•API
                    break

        print(f"ğŸ” ç™»å½•APIè¯„åˆ† {url}: {score}åˆ†")
        return score

    def _find_corresponding_login_page(self, domain: str, submit_api: Dict) -> Optional[str]:
        """åŸºäºç™»å½•æäº¤é¡µæ‰¾åˆ°å¯¹åº”çš„ç™»å½•é¡µé¢

        Args:
            domain: ç›®æ ‡åŸŸå
            submit_api: ç™»å½•æäº¤é¡µä¿¡æ¯

        Returns:
            Optional[str]: æ‰¾åˆ°çš„ç™»å½•é¡µé¢URL
        """
        if not hasattr(self, 'analysis_data') or not self.analysis_data:
            return None

        extracted_data = self.analysis_data.get('extracted_data', [])
        submit_url = submit_api['url']

        # ğŸ¯ æŸ¥æ‰¾å€™é€‰çš„ç™»å½•é¡µé¢
        page_candidates = []

        for api_data in extracted_data:
            api_url = api_data.get('url', '')

            # å¿…é¡»æ˜¯åŒåŸŸå
            if domain not in api_url:
                continue

            # ğŸ¯ ç®€å•çš„ç™»å½•é¡µé¢å…³é”®å­—åŒ¹é…ï¼ˆå°½é‡çŸ­ï¼Œæé«˜æˆåŠŸç‡ï¼‰
            url_lower = api_url.lower()
            page_keywords = ['login', 'logon', 'signin']

            has_page_keyword = any(keyword in url_lower for keyword in page_keywords)
            if not has_page_keyword:
                continue

            # ğŸ¯ æ’é™¤æ˜æ˜¾çš„æäº¤é¡µé¢
            if any(exclude in url_lower for exclude in ['servlet', 'submit', 'authenticate']):
                continue

            # ğŸ¯ ä¼˜å…ˆé€‰æ‹©é¡µé¢æ–‡ä»¶
            page_score = 0
            if any(ext in url_lower for ext in ['.jsp', '.html', '.htm', '.php']):
                page_score += 10
            elif url_lower.endswith('/login') or url_lower.endswith('/logon'):
                page_score += 8

            # ğŸ¯ URLç›¸ä¼¼åº¦è¯„åˆ†
            similarity_score = self._calculate_url_similarity(submit_url, api_url)
            page_score += similarity_score

            if page_score > 5:  # åŸºæœ¬é—¨æ§›
                page_candidates.append({
                    'url': api_url,
                    'score': page_score
                })

        if page_candidates:
            # é€‰æ‹©è¯„åˆ†æœ€é«˜çš„ç™»å½•é¡µé¢
            best_page = max(page_candidates, key=lambda x: x['score'])
            return best_page['url']

        return None

    def _calculate_url_similarity(self, url1: str, url2: str) -> int:
        """è®¡ç®—ä¸¤ä¸ªURLçš„ç›¸ä¼¼åº¦è¯„åˆ†

        Args:
            url1: URL1
            url2: URL2

        Returns:
            int: ç›¸ä¼¼åº¦è¯„åˆ†
        """
        from urllib.parse import urlparse

        parsed1 = urlparse(url1)
        parsed2 = urlparse(url2)

        score = 0

        # åŸŸåå¿…é¡»ç›¸åŒï¼ˆå·²åœ¨ä¸Šå±‚æ£€æŸ¥ï¼‰
        if parsed1.netloc == parsed2.netloc:
            score += 5

        # è·¯å¾„ç›¸ä¼¼åº¦
        path1_parts = parsed1.path.strip('/').split('/')
        path2_parts = parsed2.path.strip('/').split('/')

        # å…±åŒçš„è·¯å¾„æ®µ
        common_parts = set(path1_parts) & set(path2_parts)
        if common_parts:
            score += len(common_parts) * 2

        # è·¯å¾„é•¿åº¦ç›¸ä¼¼
        if abs(len(path1_parts) - len(path2_parts)) <= 1:
            score += 3

        return score

    def extract_geo_location(self, flow_data: Dict[str, Any]) -> str:
        """æå–åœ°ç†ä½ç½®"""
        # æ ¹æ®åŸŸåæ¨æ–­åœ°ç†ä½ç½®
        url = flow_data['url']

        if '.hk' in url or 'hong' in url.lower():
            return "HK"
        elif '.cn' in url or 'china' in url.lower():
            return "CN"
        elif '.in' in url or 'india' in url.lower():
            return "IN"
        else:
            return "US"  # é»˜è®¤

    def classify_api_type(self, url: str, response_content: str) -> str:
        """åˆ†ç±»APIç±»å‹"""
        url_lower = url.lower()
        content_lower = response_content.lower()

        if 'account' in url_lower or 'acc' in url_lower:
            return "account_management"
        elif 'transaction' in url_lower or 'txn' in url_lower:
            return "transaction_history"
        elif 'balance' in url_lower or 'balance' in content_lower:
            return "balance_inquiry"
        elif 'login' in url_lower or 'logon' in url_lower:
            return "authentication"
        else:
            return "general_banking"

    def extract_response_variables(self, response_matches: List[Dict], response_redactions: List[Dict]) -> List[str]:
        """æå–å“åº”å˜é‡"""
        variables = set()

        # ä»responseMatchesä¸­æå–å˜é‡
        for match in response_matches:
            value = match.get('value', '')
            # æŸ¥æ‰¾{{variable}}æ¨¡å¼
            var_matches = re.findall(r'\{\{(\w+)\}\}', value)
            variables.update(var_matches)

        # ä»responseRedactionsä¸­æå–å˜é‡
        for redaction in response_redactions:
            json_path = redaction.get('jsonPath', '')
            regex = redaction.get('regex', '')

            # æŸ¥æ‰¾å˜é‡æ¨¡å¼
            var_matches = re.findall(r'\{\{(\w+)\}\}', json_path + regex)
            variables.update(var_matches)

        return list(variables)

    def _is_sensitive_field(self, field_name: str) -> bool:
        """åˆ¤æ–­å­—æ®µæ˜¯å¦ä¸ºæ•æ„Ÿå­—æ®µï¼Œéœ€è¦å“ˆå¸Œå¤„ç†

        Args:
            field_name: å­—æ®µå

        Returns:
            bool: æ˜¯å¦ä¸ºæ•æ„Ÿå­—æ®µ
        """
        sensitive_keywords = [
            'account', 'password', 'token', 'id', 'number', 'card',
            'phone', 'email', 'name', 'address', 'è´¦å·', 'å¯†ç ', 'å§“å'
        ]
        field_lower = field_name.lower()
        return any(keyword in field_lower for keyword in sensitive_keywords)

    def generate_custom_injection(self, api_data: Dict[str, Any], flow_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆè‡ªå®šä¹‰æ³¨å…¥ä»£ç """
        # é¢„å¤„ç†å˜é‡ä»¥é¿å…f-stringè¯­æ³•é—®é¢˜
        institution = api_data.get('institution', 'Unknown')
        url = flow_data['url']
        method = flow_data['method']
        timestamp = datetime.now().isoformat()

        important_headers = self.filter_important_headers(flow_data['request_headers'])
        headers_json = json.dumps(important_headers, indent=16)
        headers_json_compact = json.dumps(important_headers, indent=12)
        geo_location = self.extract_geo_location(flow_data)

        # åŸºç¡€çš„æ³¨å…¥æ¨¡æ¿
        injection_template = f"""
// Auto-generated injection for {institution} API
// API: {url}
// Generated at: {timestamp}

const extractData = async () => {{
    try {{
        const response = await fetch("{url}", {{
            method: "{method}",
            headers: {headers_json},
            credentials: "include",
            mode: "cors"
        }});

        if (!response.ok) {{
            throw new Error(`HTTP error! status: ${{response.status}}`);
        }}

        const data = await response.json();

        // Extract relevant data based on response patterns
        const extractedData = {{
            url: "{url}",
            method: "{method}",
            headers: {headers_json_compact},
            responseBody: data,
            extractedParams: {{}},
            geoLocation: "{geo_location}",
            responseRedactions: [],
            responseMatches: [],
            witnessParameters: {{}}
        }};

        // Send extracted data
        if (window.flutter_inappwebview) {{
            window.flutter_inappwebview.callHandler("extractedData", JSON.stringify(extractedData));
        }}

    }} catch (error) {{
        console.error('Data extraction failed:', error);
    }}
}};

// Auto-trigger extraction when page is ready
if (document.readyState === 'loading') {{
    document.addEventListener('DOMContentLoaded', extractData);
}} else {{
    extractData();
}}
"""

        return injection_template.strip()

    def build_all_providers(self) -> Tuple[List[Dict], List[Dict]]:
        """æ„å»ºæ‰€æœ‰APIçš„provideré…ç½®

        Returns:
            Tuple[List[Dict], List[Dict]]: (æˆåŠŸçš„providers, å­˜ç–‘çš„APIs)
        """
        print("ğŸš€ å¼€å§‹æ„å»ºReclaim Providers...")

        successful_providers = []
        questionable_apis = []

        # è·å–åˆ†æç»“æœä¸­çš„APIæ•°æ®
        extracted_data = self.analysis_data.get('extracted_data', [])

        print(f"ğŸ“Š å…±å‘ç° {len(extracted_data)} ä¸ªæœ‰ä»·å€¼çš„API")

        # ğŸ¯ æ–°å¢ï¼šç”¨äºå»é‡çš„å­—å…¸ï¼Œkeyä¸ºURLï¼Œvalueä¸ºæœ€ä½³çš„APIæ•°æ®
        best_apis_by_url = {}

        # ğŸ¯ ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰å€¼å¾—æ„å»ºproviderçš„APIï¼Œå¹¶é€‰æ‹©æœ€ä½³ç‰ˆæœ¬
        print("ğŸ” ç¬¬ä¸€æ­¥ï¼šAPIå»é‡å’Œæœ€ä½³ç‰ˆæœ¬é€‰æ‹©...")

        for i, api_data in enumerate(extracted_data, 1):
            api_category = api_data.get('api_category', 'unknown')
            provider_worthy = api_data.get('provider_worthy', False)

            if not provider_worthy:
                questionable_api = {
                    'api_data': api_data,
                    'reason': f'APIåˆ†ç±»ä¸º{api_category}ï¼Œä¸é€‚åˆç”Ÿæˆprovider',
                    'api_category': api_category,
                    'confidence_score': 0.0
                }
                questionable_apis.append(questionable_api)
                continue

            url = api_data['url']

            # ğŸ¯ å»é‡é€»è¾‘ï¼šé€‰æ‹©æœ€ä½³ç‰ˆæœ¬
            if url in best_apis_by_url:
                current_best = best_apis_by_url[url]
                if self._is_better_api_version(api_data, current_best):
                    print(f"ğŸ”„ å‘ç°æ›´ä½³ç‰ˆæœ¬: {url[:60]}...")
                    print(f"   æ›¿æ¢ç‰ˆæœ¬: {len(current_best.get('matched_patterns', []))}æ¨¡å¼ â†’ {len(api_data.get('matched_patterns', []))}æ¨¡å¼")
                    best_apis_by_url[url] = api_data
                else:
                    print(f"âš ï¸  è·³è¿‡é‡å¤API (å·²æœ‰æ›´ä½³ç‰ˆæœ¬): {url[:60]}...")
            else:
                best_apis_by_url[url] = api_data

        print(f"ğŸ“Š å»é‡åå‰©ä½™ {len(best_apis_by_url)} ä¸ªå”¯ä¸€API")

        # ğŸ¯ ç¬¬äºŒæ­¥ï¼šä¸ºé€‰ä¸­çš„æœ€ä½³APIæ„å»ºprovider
        print("ğŸ” ç¬¬äºŒæ­¥ï¼šæ„å»ºproviders...")

        for i, (url, api_data) in enumerate(best_apis_by_url.items(), 1):
            print(f"\nğŸ” å¤„ç†API {i}/{len(best_apis_by_url)}: {api_data['url']}")

            try:
                provider_config, quality_check = self.build_provider_for_api(api_data)

                if provider_config:
                    successful_providers.append(provider_config)
                    print(f"âœ… æˆåŠŸæ„å»ºprovider (ç½®ä¿¡åº¦: {quality_check.confidence_score:.2f})")
                else:
                    # æ·»åŠ åˆ°å­˜ç–‘åˆ—è¡¨
                    questionable_api = {
                        'api_data': api_data,
                        'quality_check': asdict(quality_check) if quality_check else None,
                        'reason': 'è´¨é‡æ£€æŸ¥æœªé€šè¿‡',
                        'missing_fields': quality_check.missing_fields if quality_check else ['unknown'],
                        'confidence_score': quality_check.confidence_score if quality_check else 0.0
                    }
                    questionable_apis.append(questionable_api)
                    print(f"âš ï¸  æ·»åŠ åˆ°å­˜ç–‘åˆ—è¡¨ (ç½®ä¿¡åº¦: {quality_check.confidence_score if quality_check else 0:.2f})")

            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥: {e}")
                questionable_api = {
                    'api_data': api_data,
                    'quality_check': None,
                    'reason': f'å¤„ç†å¼‚å¸¸: {str(e)}',
                    'missing_fields': ['processing_error'],
                    'confidence_score': 0.0
                }
                questionable_apis.append(questionable_api)

        print(f"\nğŸ“ˆ æ„å»ºå®Œæˆ:")
        print(f"   âœ… æˆåŠŸæ„å»º: {len(successful_providers)} ä¸ªproviders")
        print(f"   âš ï¸  å­˜ç–‘API: {len(questionable_apis)} ä¸ª")

        return successful_providers, questionable_apis

    def _is_better_api_version(self, new_api: Dict, current_best: Dict) -> bool:
        """åˆ¤æ–­æ–°çš„APIç‰ˆæœ¬æ˜¯å¦æ¯”å½“å‰æœ€ä½³ç‰ˆæœ¬æ›´å¥½

        Args:
            new_api: æ–°çš„APIæ•°æ®
            current_best: å½“å‰æœ€ä½³APIæ•°æ®

        Returns:
            bool: æ–°ç‰ˆæœ¬æ˜¯å¦æ›´å¥½
        """
        # ğŸ¯ è¯„åˆ¤æ ‡å‡†ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

        # 1. åŒ¹é…æ¨¡å¼æ•°é‡ï¼ˆæ›´å¤šæ¨¡å¼ = æ›´ä¸°å¯Œçš„ç‰¹å¾ï¼‰
        new_patterns = len(new_api.get('matched_patterns', []))
        current_patterns = len(current_best.get('matched_patterns', []))

        if new_patterns != current_patterns:
            return new_patterns > current_patterns

        # 2. ä»·å€¼è¯„åˆ†ï¼ˆæ›´é«˜è¯„åˆ† = æ›´æœ‰ä»·å€¼ï¼‰
        new_score = new_api.get('value_score', 0)
        current_score = current_best.get('value_score', 0)

        if new_score != current_score:
            return new_score > current_score

        # 3. æ•°æ®ç±»å‹æ•°é‡ï¼ˆæ›´å¤šæ•°æ®ç±»å‹ = æ›´ä¸°å¯Œçš„å†…å®¹ï¼‰
        new_data_types = len(new_api.get('data_types', []))
        current_data_types = len(current_best.get('data_types', []))

        if new_data_types != current_data_types:
            return new_data_types > current_data_types

        # 4. ä¼˜å…ˆçº§çº§åˆ«ï¼ˆcritical > high > medium > lowï¼‰
        priority_order = {'critical': 4, 'high': 3, 'medium': 2, 'low': 1, 'unknown': 0}
        new_priority = priority_order.get(new_api.get('priority_level', 'unknown'), 0)
        current_priority = priority_order.get(current_best.get('priority_level', 'unknown'), 0)

        if new_priority != current_priority:
            return new_priority > current_priority

        # 5. å¦‚æœæ‰€æœ‰æŒ‡æ ‡éƒ½ç›¸åŒï¼Œä¿æŒå½“å‰ç‰ˆæœ¬ï¼ˆå…ˆåˆ°å…ˆå¾—ï¼‰
        return False

    def save_results(self, successful_providers: List[Dict], questionable_apis: List[Dict],
                    output_dir: str = "data") -> Tuple[str, str]:
        """ä¿å­˜æ„å»ºç»“æœ

        Args:
            successful_providers: æˆåŠŸçš„providers
            questionable_apis: å­˜ç–‘çš„APIs
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            Tuple[str, str]: (providersæ–‡ä»¶è·¯å¾„, å­˜ç–‘æ–‡ä»¶è·¯å¾„)
        """
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)

        # ä½¿ç”¨æ—¥æœŸä½œä¸ºæ–‡ä»¶ååç¼€ï¼ˆè¦†ç›–å†™ï¼‰
        date_str = datetime.now().strftime("%Y%m%d")

        # ä¿å­˜æˆåŠŸçš„providers
        providers_file = os.path.join(output_dir, f"reclaim_providers_{date_str}.json")
        providers_output = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_providers": len(successful_providers),
                "source_mitm_file": self.mitm_file_path,
                "source_analysis_file": self.analysis_result_file,
                "generator_version": "1.0.0"
            },
            "providers": successful_providers
        }

        with open(providers_file, 'w', encoding='utf-8') as f:
            json.dump(providers_output, f, indent=2, ensure_ascii=False)

        # ä¿å­˜å­˜ç–‘çš„APIs
        questionable_file = os.path.join(output_dir, f"questionable_apis_{date_str}.json")
        questionable_output = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_questionable": len(questionable_apis),
                "reasons_summary": self.analyze_questionable_reasons(questionable_apis),
                "source_mitm_file": self.mitm_file_path,
                "source_analysis_file": self.analysis_result_file
            },
            "questionable_apis": questionable_apis
        }

        with open(questionable_file, 'w', encoding='utf-8') as f:
            json.dump(questionable_output, f, indent=2, ensure_ascii=False)

        return providers_file, questionable_file

    def analyze_questionable_reasons(self, questionable_apis: List[Dict]) -> Dict[str, int]:
        """åˆ†æå­˜ç–‘APIçš„åŸå› ç»Ÿè®¡"""
        reasons = {}

        for api in questionable_apis:
            missing_fields = api.get('missing_fields', [])
            for field in missing_fields:
                reasons[field] = reasons.get(field, 0) + 1

        return reasons


def run_integration_and_build_providers(mitm_file: str, output_dir: str = "data") -> Tuple[str, str, str]:
    """è¿è¡Œå®Œæ•´çš„é›†æˆæµç¨‹ï¼šåˆ†æ + æ„å»ºproviders

    Args:
        mitm_file: mitmæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        Tuple[str, str, str]: (åˆ†æç»“æœæ–‡ä»¶, providersæ–‡ä»¶, å­˜ç–‘æ–‡ä»¶)
    """
    print("ğŸš€ å¼€å§‹å®Œæ•´çš„Provideræ„å»ºæµç¨‹...")

    # ç¬¬ä¸€æ­¥ï¼šè¿è¡Œintegrate_with_mitmproxy2swagger.pyè¿›è¡Œåˆ†æ
    print("\nğŸ“Š ç¬¬ä¸€æ­¥ï¼šè¿è¡Œç‰¹å¾åº“åˆ†æ...")

    import subprocess
    import sys

    # æ„å»ºå›ºå®šçš„åˆ†æç»“æœæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    analysis_result_file = os.path.join(output_dir, f"provider_analysis_result_{timestamp}.json")

    # è¿è¡Œåˆ†æè„šæœ¬
    script_dir = Path(__file__).parent.parent / "feature-library" / "plugins"
    cmd = [
        sys.executable,
        str(script_dir / "integrate_with_mitmproxy2swagger.py"),
        "--mode", "direct",
        "--input", mitm_file,
        "--output", analysis_result_file
    ]

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        print("âœ… ç‰¹å¾åº“åˆ†æå®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"âŒ ç‰¹å¾åº“åˆ†æå¤±è´¥: {e}")
        print(f"é”™è¯¯è¾“å‡º: {e.stderr}")
        raise

    # ç¬¬äºŒæ­¥ï¼šæ„å»ºproviders
    print("\nğŸ—ï¸  ç¬¬äºŒæ­¥ï¼šæ„å»ºReclaim Providers...")

    builder = ReclaimProviderBuilder(mitm_file, analysis_result_file)
    successful_providers, questionable_apis = builder.build_all_providers()

    # ç¬¬ä¸‰æ­¥ï¼šä¿å­˜ç»“æœ
    print("\nğŸ’¾ ç¬¬ä¸‰æ­¥ï¼šä¿å­˜æ„å»ºç»“æœ...")

    providers_file, questionable_file = builder.save_results(
        successful_providers, questionable_apis, output_dir
    )

    print(f"\nğŸ‰ å®Œæ•´æµç¨‹å®Œæˆ!")
    print(f"ğŸ“ åˆ†æç»“æœ: {analysis_result_file}")
    print(f"ğŸ“ Providers: {providers_file}")
    print(f"ğŸ“ å­˜ç–‘APIs: {questionable_file}")

    return analysis_result_file, providers_file, questionable_file


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description='Reclaim Provideræ„å»ºå™¨')
    parser.add_argument('--mitm-file', '-i', required=True, help='è¾“å…¥çš„mitmæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--analysis-file', '-a', help='ç‰¹å¾åº“åˆ†æç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›ä¼šè‡ªåŠ¨è¿è¡Œåˆ†æï¼‰')
    parser.add_argument('--output-dir', '-o', default='data', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--run-full-pipeline', '-f', action='store_true',
                       help='è¿è¡Œå®Œæ•´æµç¨‹ï¼ˆåˆ†æ+æ„å»ºï¼‰')

    args = parser.parse_args()

    try:
        if args.run_full_pipeline or not args.analysis_file:
            # è¿è¡Œå®Œæ•´æµç¨‹
            analysis_file, providers_file, questionable_file = run_integration_and_build_providers(
                args.mitm_file, args.output_dir
            )
        else:
            # åªæ„å»ºproviders
            print("ğŸ—ï¸  æ„å»ºReclaim Providers...")

            builder = ReclaimProviderBuilder(args.mitm_file, args.analysis_file)
            successful_providers, questionable_apis = builder.build_all_providers()

            providers_file, questionable_file = builder.save_results(
                successful_providers, questionable_apis, args.output_dir
            )

            print(f"\nğŸ‰ æ„å»ºå®Œæˆ!")
            print(f"ğŸ“ Providers: {providers_file}")
            print(f"ğŸ“ å­˜ç–‘APIs: {questionable_file}")

    except Exception as e:
        print(f"âŒ æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
