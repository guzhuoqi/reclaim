#!/usr/bin/env python3
"""
Reclaim Provideræ„å»ºå™¨
Reclaim Provider Builder

åŸºäºç‰¹å¾åº“åˆ†æç»“æœå’ŒæŠ“åŒ…æ•°æ®ï¼Œè‡ªåŠ¨æ„å»ºReclaimæ ‡å‡†çš„provideré…ç½®
é‡ç‚¹åˆ†æresponseMatchesã€responseRedactionsç­‰å…³é”®å­—æ®µ

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. è¯»å–integrate_with_mitmproxy2swagger.pyçš„åˆ†æç»“æœ
2. å›æº¯åŸå§‹æŠ“åŒ…æ•°æ®ï¼Œæå–å®Œæ•´çš„è¯·æ±‚/å“åº”ä¿¡æ¯
3. åˆ†æHTTP headersä¸­çš„è®¤è¯ä¿¡æ¯
4. æ„å»ºresponseMatcheså’ŒresponseRedactions
5. ç”Ÿæˆå®Œæ•´çš„provideré…ç½®
6. è´¨é‡æ£€æŸ¥ï¼Œå°†ä¿¡æ¯ä¸è¶³çš„APIè¾“å‡ºåˆ°å­˜ç–‘æ–‡ä»¶
"""

import os
import shutil
import sys
import json
import re
import hashlib
import uuid
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict
from urllib.parse import urlparse, parse_qs

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
sys.path.append(str(Path(__file__).parent.parent / "mitmproxy2swagger"))

from mitmproxy2swagger.mitmproxy_capture_reader import MitmproxyCaptureReader


@dataclass
class ProviderQualityCheck:
    """Providerè´¨é‡æ£€æŸ¥ç»“æœ"""
    has_authentication: bool = False
    has_response_data: bool = False
    has_financial_patterns: bool = False
    has_sufficient_headers: bool = False
    missing_fields: List[str] = None
    confidence_score: float = 0.0

    def __post_init__(self):
        if self.missing_fields is None:
            self.missing_fields = []


class ReclaimProviderBuilder:
    """Reclaim Provideræ„å»ºå™¨"""

    def __init__(self, mitm_file_path: str, analysis_result_file: str):
        """åˆå§‹åŒ–æ„å»ºå™¨

        Args:
            mitm_file_path: åŸå§‹mitmæ–‡ä»¶è·¯å¾„
            analysis_result_file: ç‰¹å¾åº“åˆ†æç»“æœæ–‡ä»¶è·¯å¾„
        """
        self.mitm_file_path = mitm_file_path
        self.analysis_result_file = analysis_result_file

        # åŠ è½½åˆ†æç»“æœ
        self.analysis_data = self.load_analysis_result()

        # åˆ›å»ºmitmè¯»å–å™¨
        self.capture_reader = MitmproxyCaptureReader(mitm_file_path)

        # å­˜å‚¨åŸå§‹æµæ•°æ®çš„æ˜ å°„
        self.flow_data_map = {}
        self.build_flow_data_map()

        # è®¤è¯ç›¸å…³çš„headeræ¨¡å¼
        self.auth_header_patterns = [
            'authorization', 'x-auth-token', 'x-api-key', 'x-session-token',
            'x-csrf-token', 'x-nonce', 'x-requested-with', 'x-target-unit',
            'cookie', 'set-cookie', 'session-id', 'jsessionid'
        ]

        # é‡è¦çš„headeræ¨¡å¼
        self.important_header_patterns = [
            'content-type', 'accept', 'user-agent', 'referer', 'origin',
            'sec-ch-ua', 'sec-fetch-', 'x-'
        ]

    def load_analysis_result(self) -> Dict[str, Any]:
        """åŠ è½½ç‰¹å¾åº“åˆ†æç»“æœ"""
        try:
            with open(self.analysis_result_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            raise Exception(f"æ— æ³•åŠ è½½åˆ†æç»“æœæ–‡ä»¶: {e}")

    def build_flow_data_map(self):
        """æ„å»ºæµæ•°æ®æ˜ å°„ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾åŸå§‹è¯·æ±‚/å“åº”æ•°æ®"""
        print("ğŸ” æ„å»ºæµæ•°æ®æ˜ å°„...")

        for flow_wrapper in self.capture_reader.captured_requests():
            url = flow_wrapper.get_url()

            # ğŸ¯ å®‰å…¨åœ°è·å–å“åº”ä½“ï¼Œå¤„ç†ç¼–ç é—®é¢˜
            try:
                response_body = flow_wrapper.get_response_body()
            except ValueError as e:
                if "Invalid Content-Encoding" in str(e):
                    print(f"âš ï¸  è·³è¿‡ç¼–ç æœ‰é—®é¢˜çš„å“åº”: {url}")
                    continue
                else:
                    raise

            # æå–å®Œæ•´çš„è¯·æ±‚/å“åº”æ•°æ®
            flow_data = {
                'url': url,
                'method': flow_wrapper.get_method(),
                'request_headers': dict(flow_wrapper.get_request_headers()),
                'response_headers': dict(flow_wrapper.get_response_headers()),
                'request_body': flow_wrapper.get_request_body(),
                'response_body': response_body,
                'status_code': flow_wrapper.get_response_status_code()
            }

            self.flow_data_map[url] = flow_data

        print(f"âœ… æ„å»ºå®Œæˆï¼Œå…±æ˜ å°„ {len(self.flow_data_map)} ä¸ªæµ")

    def extract_authentication_info(self, headers: Dict[str, str]) -> Dict[str, Any]:
        """æå–è®¤è¯ä¿¡æ¯

        Args:
            headers: HTTP headers

        Returns:
            è®¤è¯ä¿¡æ¯å­—å…¸
        """
        auth_info = {
            'has_auth': False,
            'auth_headers': [],
            'session_info': [],
            'csrf_tokens': [],
            'api_keys': []
        }

        for header_name, header_value in headers.items():
            header_lower = header_name.lower()

            # æ£€æŸ¥è®¤è¯ç›¸å…³header
            for pattern in self.auth_header_patterns:
                if pattern in header_lower:
                    auth_info['has_auth'] = True
                    auth_info['auth_headers'].append({
                        'name': header_name,
                        'value': header_value,
                        'type': self.classify_auth_header(header_name, header_value)
                    })
                    break

        return auth_info

    def classify_auth_header(self, name: str, value: str) -> str:
        """åˆ†ç±»è®¤è¯headerç±»å‹"""
        name_lower = name.lower()

        if 'authorization' in name_lower:
            return 'bearer_token' if 'bearer' in value.lower() else 'basic_auth'
        elif 'session' in name_lower or 'jsessionid' in name_lower:
            return 'session'
        elif 'csrf' in name_lower or 'xsrf' in name_lower:
            return 'csrf_token'
        elif 'nonce' in name_lower:
            return 'nonce'
        elif 'api-key' in name_lower:
            return 'api_key'
        elif 'cookie' in name_lower:
            return 'cookie'
        else:
            return 'custom'

    def extract_response_patterns(self, response_content: str, url: str, api_data: Dict = None) -> Tuple[List[Dict], List[Dict]]:
        """ä»å“åº”å†…å®¹ä¸­æå–æ¨¡å¼ï¼Œç”¨äºæ„å»ºresponseMatcheså’ŒresponseRedactions

        å…³é”®åŸåˆ™ï¼š
        1. responseMatchesæ•°ç»„ï¼šä¸€å®šæ˜¯èƒ½åŒ¹é…æˆåŠŸï¼Œæ‰èƒ½çº³å…¥
        2. responseRedactionsæ•°ç»„ï¼šæ˜¯è¦èƒ½æå–å‡ºç”¨æˆ·çš„é‡‘èä¿¡æ¯

        Args:
            response_content: å“åº”å†…å®¹
            url: API URL
            api_data: APIåˆ†ææ•°æ®ï¼ˆåŒ…å«matched_patternsï¼‰

        Returns:
            Tuple[List[Dict], List[Dict]]: (responseMatches, responseRedactions)
        """
        response_matches = []
        response_redactions = []

        if not response_content:
            return response_matches, response_redactions

        # ğŸ¯ é¦–å…ˆåˆ¤æ–­å“åº”å†…å®¹çš„æ ¼å¼ç±»å‹
        content_type = self._detect_content_type(response_content)
        print(f"ğŸ” å“åº”å†…å®¹ç±»å‹: {content_type}, é•¿åº¦: {len(response_content)}")

        # ğŸ¯ æ ¹æ®å®é™…å†…å®¹å’Œç‰¹å¾åˆ†æç»“æœç”ŸæˆåŒ¹é…è§„åˆ™
        if api_data and 'matched_patterns' in api_data:
            matched_patterns = api_data['matched_patterns']
            print(f"ğŸ” ç‰¹å¾åˆ†æè¯†åˆ«çš„æ¨¡å¼: {matched_patterns}")

            order_counter = 1
            processed_patterns = set()  # é˜²æ­¢é‡å¤å¤„ç†ç›¸åŒæ¨¡å¼

            # ğŸ¯ æ ¹æ®å®é™…åŒ¹é…çš„æ¨¡å¼ç”Ÿæˆå¯¹åº”çš„æ­£åˆ™è¡¨è¾¾å¼
            for pattern in matched_patterns:
                # è·³è¿‡å·²å¤„ç†çš„æ¨¡å¼
                if pattern in processed_patterns:
                    print(f"ğŸ”„ è·³è¿‡é‡å¤æ¨¡å¼: {pattern}")
                    continue
                processed_patterns.add(pattern)
                print(f"ğŸ” å¤„ç†æ¨¡å¼: {pattern}")
                if pattern.startswith("field:"):
                    # å­—æ®µåŒ¹é… - ç”Ÿæˆå­—æ®µéªŒè¯å’Œæå–è§„åˆ™
                    field_name = pattern.replace("field:", "")

                    # å…ˆåšå‘½ä¸­é¢„æ ¡éªŒï¼šä»…å½“å“åº”æ­£æ–‡åŒ…å«è¯¥å­—æ®µåæ‰åŠ å…¥ containsï¼ˆä¸¥æ ¼ AND ä¿éšœï¼‰
                    if f'"{field_name}"' in response_content:
                        response_matches.append({
                            "value": f'"{field_name}"',
                            "type": "contains",
                            "invert": False,
                            "description": f"éªŒè¯{field_name}å­—æ®µå­˜åœ¨",
                            "order": order_counter,
                            "isOptional": False
                        })

                    # ğŸ¯ æ ¹æ®å“åº”ç±»å‹å†³å®šæ˜¯å¦ä½¿ç”¨jsonPath
                    json_path = "" if self._is_html_response(matched_patterns) else f"$.{field_name}"

                    response_redactions.append({
                        "xPath": "",
                        "jsonPath": json_path,
                        "regex": f'"{field_name}":\\s*"?(?P<field_value>[^",\\}}]+)"?',
                        "hash": "sha256" if self._is_sensitive_field(field_name) else "",
                        "order": order_counter
                    })
                    order_counter += 1

                elif ("html_content:balance" in pattern or ("content:balance" in pattern and content_type == "html") or
                      ("html_currency:" in pattern and any("html_currency:" in p for p in matched_patterns))):
                    # ğŸ¯ HTMLä½™é¢ç›¸å…³API - åº”ç”¨ä¼˜å…ˆçº§åŒ¹é…è§„åˆ™ï¼šä»ä¸¥æ ¼åˆ°å®½æ¾
                    print(f"ğŸ¯ DEBUG: è§¦å‘HTMLä½™é¢ä¼˜å…ˆçº§åŒ¹é…è§„åˆ™! pattern={pattern}, matched_patterns={matched_patterns}")

                    # ç«™ç‚¹å®šåˆ¶ä¸¥æ ¼è§„åˆ™ï¼ˆå‚è€ƒæä¾›çš„æ¨¡æ¿æ–‡ä»¶ï¼‰
                    try:
                        host = urlparse(url).netloc.lower()
                    except Exception:
                        host = ""

                    if 'its.bochk.com' in host:
                        # ä»…é™è´¦æˆ·æ€»è§ˆé¡µå‚ä¸ä½™é¢ä¸¥æ ¼æ ¡éªŒï¼Œç™»å½•/ç™»å½•æäº¤é¡µä¸å‚ä¸
                        try:
                            _path_lower = urlparse(url).path.lower()
                        except Exception:
                            _path_lower = ''
                        if 'acc.overview.do' not in _path_lower:
                            print(f"â­ï¸ è·³è¿‡BOCä¸¥æ ¼ä½™é¢è§„åˆ™ï¼ˆéæ¦‚è§ˆé¡µï¼‰ï¼š{url}")
                            # ç»§ç»­åç»­é€šç”¨æµç¨‹å¤„ç†
                            pass
                        else:
                            # ä¸­å›½é“¶è¡Œé¦™æ¸¯ï¼šåŸºäº table cell class çš„ä¸¥æ ¼è§„åˆ™ï¼ˆåªåŠ å…¥ responseMatchesï¼‰
                            strict_class_rules = [
                                (
                                    r'data_table_swap1_txt data_table_lastcell"[^>]*>(?P<hkd_balance>[\d,]+\.\d{2})</td>',
                                    'ä¸¥æ ¼è§„åˆ™ï¼šBOC HKD ä½™é¢ï¼ˆclassé”šç‚¹ï¼‰'
                                ),
                                (
                                    r'data_table_swap2_txt data_table_lastcell"[^>]*>(?P<usd_balance>[\d,]+\.\d{2})</td>',
                                    'ä¸¥æ ¼è§„åˆ™ï¼šBOC USD ä½™é¢ï¼ˆclassé”šç‚¹ï¼‰'
                                ),
                                (
                                    r'data_table_subtotal data_table_lastcell"[^>]*>(?P<total_balance>[\d,]+\.\d{2})</td>',
                                    'ä¸¥æ ¼è§„åˆ™ï¼šBOC æ€»ä½™é¢ï¼ˆclassé”šç‚¹ï¼‰'
                                ),
                            ]
                            for regex, desc in strict_class_rules:
                                response_matches.append({
                                    "value": regex,
                                    "type": "regex",
                                    "invert": False,
                                    "description": desc,
                                    "order": order_counter,
                                    "isOptional": False
                                })
                                order_counter += 1
                            # å·²æŒ‰ç«™ç‚¹å®šåˆ¶ç”Ÿæˆï¼Œè·³è¿‡é€šç”¨æµç¨‹
                            continue

                    if 'cmbwinglungbank.com' in host:
                        # æ‹›å•†æ°¸éš†ï¼šè´§å¸ç´§é‚»é‡‘é¢çš„ä¸¥æ ¼è§„åˆ™
                        strict_currency_rules = [
                            (r'HKD[^\d]*(?P<hkd_balance>\d[\d,]*\.\d{2})', 'ä¸¥æ ¼è§„åˆ™ï¼šCMB WL HKD çº¯å‡€é‡‘é¢'),
                            (r'USD[^\d]*(?P<usd_balance>\d[\d,]*\.\d{2})', 'ä¸¥æ ¼è§„åˆ™ï¼šCMB WL USD çº¯å‡€é‡‘é¢'),
                            (r'CNY[^\d]*(?P<cny_balance>\d[\d,]*\.\d{2})', 'ä¸¥æ ¼è§„åˆ™ï¼šCMB WL CNY çº¯å‡€é‡‘é¢'),
                        ]

                        for regex, desc in strict_currency_rules:
                            response_matches.append({
                                "value": regex,
                                "type": "regex",
                                "invert": False,
                                "description": desc,
                                "order": order_counter,
                                "isOptional": True
                            })
                            order_counter += 1

                        # ç«™ç‚¹å®šåˆ¶å·²ç”Ÿæˆï¼Œè·³è¿‡é€šç”¨æµç¨‹
                        continue

                    balance_rules = self._generate_priority_balance_rules(matched_patterns, response_content)
                    print(f"ğŸ¯ DEBUG: ç”Ÿæˆçš„ä¼˜å…ˆçº§è§„åˆ™æ•°é‡: {len(balance_rules)}")

                    if balance_rules:
                        # ä¸¥æ ¼â†’å®½æ¾ä¼˜å…ˆåŒ¹é…ï¼šä»…å°†å‘½ä¸­çš„ç¬¬ä¸€æ¡ä½œä¸ºæ ¡éªŒè§„åˆ™åŠ å…¥ responseMatchesï¼ŒåŒæ—¶åŠ å…¥ redactions ä¾¿äºæå–
                        for rule in balance_rules:
                            response_matches.append({
                                "value": rule["regex"],
                                "type": "regex",
                                "invert": False,
                                "description": rule["description"],
                                "order": order_counter,
                                "isOptional": rule.get("isOptional", True)
                            })
                            response_redactions.append({
                                "xPath": "",
                                "jsonPath": "",
                                "regex": rule["regex"],
                                "hash": "",
                                "order": order_counter
                            })
                            order_counter += 1
                    else:
                        # ä¸å†æ·»åŠ é€šç”¨containså…œåº•è§„åˆ™ï¼Œé¿å…æ— æ•ˆæ ¡éªŒ
                        print(f"âš ï¸ DEBUG: ä¼˜å…ˆçº§è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡é€šç”¨ä½™é¢containså…œåº•")



                elif "html_content:account" in pattern or ("content:account" in pattern and content_type == "html"):
                    # HTMLè´¦æˆ·ç›¸å…³API - ğŸ¯ åªç”Ÿæˆå®é™…èƒ½åŒ¹é…çš„è§„åˆ™
                    # ç™»å½•/è®¤è¯é¡µä¸å±•ç¤ºè´¦å·ä¿¡æ¯ï¼Œç›´æ¥è·³è¿‡
                    try:
                        url_lower = (url or "").lower()
                    except Exception:
                        url_lower = ""
                    if any(k in url_lower for k in ["login", "logon", "auth"]):
                        print(f"â­ï¸ è·³è¿‡ç™»å½•/è®¤è¯é¡µçš„è´¦æˆ·è§„åˆ™: {url}")
                        continue

                    actual_accounts = self._extract_actual_accounts(response_content)

                    if actual_accounts and self._validate_account_context(response_content):
                        # ğŸ¯ éªŒè¯è´¦æˆ·å·ç æ­£åˆ™è¡¨è¾¾å¼çš„æœ‰æ•ˆæ€§ï¼ˆé¿å…ä½¿ç”¨ä¸å…¼å®¹çš„å‰ç»ï¼‰
                        account_regex = "(?P<account_number>[A-Z]{2,4}\\d{8,16}|\\d{8,20}[A-Z])"
                        if self._validate_regex_effectiveness(response_content, account_regex, "è´¦æˆ·å·ç "):
                            # ä¸ºå®é™…å­˜åœ¨çš„è´¦æˆ·å·ç ç”ŸæˆåŒ¹é…è§„åˆ™
                            response_matches.append({
                                "value": "[A-Z]{2,4}\\d{8,16}|\\d{8,20}[A-Z]",
                                "type": "regex",
                                "invert": False,
                                "description": f"éªŒè¯HTMLä¸­çš„å®é™…è´¦æˆ·å·ç ",
                                "order": order_counter,
                                "isOptional": False
                            })

                            response_redactions.append({
                                "xPath": "",
                                "jsonPath": "",
                                "regex": account_regex,
                                "hash": "sha256",
                                "order": order_counter
                            })
                            order_counter += 1
                            print(f"âœ… ç”Ÿæˆè´¦æˆ·åŒ¹é…è§„åˆ™: {len(actual_accounts)}ä¸ªå®é™…è´¦æˆ·")
                        else:
                            print(f"âš ï¸ è·³è¿‡ç”Ÿæˆè´¦æˆ·å·ç åŒ¹é…è§„åˆ™ - è´¨é‡/ä¸Šä¸‹æ–‡è¯„ä¼°æœªé€šè¿‡")
                    else:
                        print(f"âš ï¸ è·³è¿‡è´¦æˆ·æ¨¡å¼ - æœªé€šè¿‡ä¸Šä¸‹æ–‡æˆ–æœªå‘ç°å®é™…è´¦æˆ·å·ç ")

                    # ğŸ¯ äºŒæ¬¡åˆ¤æ–­ï¼šæ£€æŸ¥è´¦æˆ·å…³é”®å­—çš„ä¸Šä¸‹æ–‡æ˜¯å¦ç¬¦åˆç”¨æˆ·ä¿¡æ¯æ ¼å¼
                    if self._validate_account_context(response_content):
                        response_matches.append({
                            "value": "account|Account|è´¦æˆ·|è´¦å·",
                            "type": "contains",
                            "invert": False,
                            "description": "éªŒè¯HTMLä¸­åŒ…å«è´¦æˆ·ç›¸å…³æ–‡æœ¬",
                            "order": order_counter,
                            "isOptional": True  # ğŸ¯ è®¾ä¸ºå¯é€‰ï¼Œé¿å…è¿è¡Œæ—¶éªŒè¯å¤±è´¥
                        })

                        response_redactions.append({
                            "xPath": "",
                            "jsonPath": "",
                            "regex": "(?P<account_keyword>account|Account|è´¦æˆ·|è´¦å·)",  # ğŸ¯ æ·»åŠ å‘½åæ•è·ç»„
                            "hash": "",
                            "order": order_counter
                        })
                        order_counter += 1
                        print(f"âœ… ç”Ÿæˆè´¦æˆ·å…³é”®å­—åŒ¹é…è§„åˆ™ï¼ˆé€šè¿‡ä¸Šä¸‹æ–‡éªŒè¯ï¼‰")
                    else:
                        print(f"âš ï¸ è·³è¿‡è´¦æˆ·å…³é”®å­—åŒ¹é… - ä¸Šä¸‹æ–‡ä¸ç¬¦åˆç”¨æˆ·ä¿¡æ¯æ ¼å¼")

                elif "json_content:account" in pattern or (("content:account" in pattern or "content:acc" in pattern or "account" in pattern or "acc" in pattern) and content_type == "json"):
                    # è´¦æˆ·ç›¸å…³API - ç”Ÿæˆå¤šç§è´¦æˆ·ä¿¡æ¯éªŒè¯è§„åˆ™
                    account_patterns = [
                        {
                            "value": self._get_account_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "type": "regex",
                            "description": "éªŒè¯è´¦æˆ·å·ç å­—æ®µ",
                            "jsonPath": "" if self._is_html_response(matched_patterns) else "$.account*",
                            "regex": self._get_account_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "hash": "sha256"
                        },
                        {
                            "value": self._get_account_type_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "type": "regex",
                            "description": "éªŒè¯è´¦æˆ·ç±»å‹å’ŒçŠ¶æ€",
                            "jsonPath": "" if self._is_html_response(matched_patterns) else "$.accountType,$.accountStatus",
                            "regex": self._get_account_type_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "hash": ""
                        }
                    ]

                    for acc_pattern in account_patterns:
                        response_matches.append({
                            "value": acc_pattern["value"],
                            "type": acc_pattern["type"],
                            "invert": False,
                            "description": acc_pattern["description"],
                            "order": order_counter,
                            "isOptional": False
                        })

                        response_redactions.append({
                            "xPath": "",
                            "jsonPath": acc_pattern["jsonPath"],
                            "regex": acc_pattern["regex"],
                            "hash": acc_pattern["hash"],
                            "order": order_counter
                        })
                        order_counter += 1

                elif "content:login" in pattern or "content:logon" in pattern:
                    # ğŸš« è·³è¿‡ç™»å½•æ€ç›¸å…³çš„åŒ¹é… - ä¸æ˜¯ä¸ºäº†æ„å»ºproviderçš„
                    print(f"ğŸš« è·³è¿‡ç™»å½•æ€æ¨¡å¼: {pattern} - ç™»å½•æ€ä¸æ˜¯ç”¨æˆ·é‡‘èæ•°æ®")
                    continue

                elif "html_content:currency" in pattern or "html_currency:" in pattern or ("content:currency" in pattern and content_type == "html"):
                    # HTMLè´§å¸ç›¸å…³API - ğŸ¯ åªç”Ÿæˆå®é™…èƒ½åŒ¹é…çš„è§„åˆ™
                    # å…ˆéªŒè¯å“åº”ä¸­å®é™…åŒ…å«çš„è´§å¸ä»£ç 
                    actual_currencies = self._extract_actual_currencies(response_content)

                    if actual_currencies:
                        # åªä¸ºå®é™…å­˜åœ¨çš„è´§å¸ä»£ç ç”ŸæˆåŒ¹é…è§„åˆ™
                        currency_regex = "|".join(actual_currencies)
                        response_matches.append({
                            "value": f"(?P<currency>{currency_regex})",  # ğŸ¯ æ·»åŠ å‘½åæ•è·ç»„
                            "type": "regex",
                            "invert": False,
                            "description": f"éªŒè¯HTMLä¸­çš„è´§å¸ä»£ç : {', '.join(actual_currencies)}",
                            "order": order_counter,
                            "isOptional": False
                        })

                        response_redactions.append({
                            "xPath": "",
                            "jsonPath": "",
                            "regex": f"(?P<currency>{currency_regex})",  # ğŸ¯ æ·»åŠ å‘½åæ•è·ç»„
                            "hash": "",
                            "order": order_counter
                        })
                        order_counter += 1
                        print(f"âœ… ç”Ÿæˆè´§å¸åŒ¹é…è§„åˆ™: {actual_currencies}")
                    else:
                        print(f"âš ï¸ è·³è¿‡è´§å¸æ¨¡å¼ - å“åº”ä¸­æœªæ‰¾åˆ°å®é™…è´§å¸ä»£ç ")

                elif "json_content:currency" in pattern or "json_currency:" in pattern:
                    # JSONè´§å¸ç›¸å…³API - ç”ŸæˆJSONè´§å¸éªŒè¯å’Œæå–è§„åˆ™
                    json_currency_patterns = [
                        {
                            "value": self._get_currency_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "type": "regex",
                            "description": "éªŒè¯è´§å¸ä»£ç å­—æ®µ",
                            "jsonPath": "" if self._is_html_response(matched_patterns) else "$.currency,$.currencyCode",
                            "regex": self._get_currency_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "hash": ""
                        },
                        {
                            "value": self._get_major_currency_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "type": "regex",
                            "description": "éªŒè¯ä¸»è¦è´§å¸ç±»å‹",
                            "jsonPath": "" if self._is_html_response(matched_patterns) else "$..*",
                            "regex": self._get_major_currency_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "hash": ""
                        }
                    ]

                    for currency_pattern in json_currency_patterns:
                        response_matches.append({
                            "value": currency_pattern["value"],
                            "type": currency_pattern["type"],
                            "invert": False,
                            "description": currency_pattern["description"],
                            "order": order_counter,
                            "isOptional": False
                        })

                        response_redactions.append({
                            "xPath": "",
                            "jsonPath": currency_pattern["jsonPath"],
                            "regex": currency_pattern["regex"],
                            "hash": currency_pattern["hash"],
                            "order": order_counter
                        })
                        order_counter += 1

                elif "html_content:amount" in pattern or ("content:amount" in pattern and content_type == "html"):
                    # HTMLé‡‘é¢ç›¸å…³API - ğŸ¯ åªç”Ÿæˆå®é™…èƒ½åŒ¹é…çš„è§„åˆ™
                    actual_amounts = self._extract_actual_amounts(response_content)

                    if actual_amounts:
                        # ä¸ºå®é™…å­˜åœ¨çš„é‡‘é¢æ ¼å¼ç”ŸæˆåŒ¹é…è§„åˆ™
                        response_matches.append({
                            "value": self._get_formatted_amount_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "type": "regex",
                            "invert": False,
                            "description": f"éªŒè¯HTMLä¸­çš„å®é™…é‡‘é¢æ ¼å¼",
                            "order": order_counter,
                            "isOptional": False
                        })

                        response_redactions.append({
                            "xPath": "",
                            "jsonPath": "",
                            "regex": self._get_formatted_amount_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "hash": "",
                            "order": order_counter
                        })
                        order_counter += 1
                        print(f"âœ… ç”Ÿæˆé‡‘é¢åŒ¹é…è§„åˆ™: {len(actual_amounts)}ä¸ªå®é™…é‡‘é¢")
                    else:
                        print(f"âš ï¸ è·³è¿‡é‡‘é¢æ¨¡å¼ - å“åº”ä¸­æœªæ‰¾åˆ°å®é™…é‡‘é¢æ ¼å¼")

                elif "json_content:amount" in pattern or "amount" in pattern or "é‡‘é¢" in pattern:
                    # é‡‘é¢ç›¸å…³API - ç”Ÿæˆé‡‘é¢éªŒè¯å’Œæå–è§„åˆ™
                    amount_patterns = [
                        {
                            "value": self._get_amount_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "type": "regex",
                            "description": "éªŒè¯é‡‘é¢æ•°å€¼å­—æ®µ",
                            "jsonPath": "" if self._is_html_response(matched_patterns) else "$.amount,$.value",
                            "regex": self._get_amount_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "hash": ""
                        },
                        {
                            "value": self._get_formatted_amount_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "type": "regex",
                            "description": "éªŒè¯æ ¼å¼åŒ–é‡‘é¢",
                            "jsonPath": "" if self._is_html_response(matched_patterns) else "$..*",
                            "regex": self._get_formatted_amount_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "hash": ""
                        }
                    ]

                    for amount_pattern in amount_patterns:
                        response_matches.append({
                            "value": amount_pattern["value"],
                            "type": amount_pattern["type"],
                            "invert": False,
                            "description": amount_pattern["description"],
                            "order": order_counter,
                            "isOptional": False
                        })

                        response_redactions.append({
                            "xPath": "",
                            "jsonPath": amount_pattern["jsonPath"],
                            "regex": amount_pattern["regex"],
                            "hash": amount_pattern["hash"],
                            "order": order_counter
                        })
                        order_counter += 1

                elif "content:user_info" in pattern or "content:customer" in pattern or "content:name" in pattern:
                    # ç”¨æˆ·ä¿¡æ¯ç›¸å…³API - ç”Ÿæˆç”¨æˆ·ä¿¡æ¯éªŒè¯å’Œæå–è§„åˆ™
                    # ğŸ¯ ç”Ÿæˆç”¨æˆ·å§“åæ¨¡å¼å‰å…ˆéªŒè¯æœ‰æ•ˆæ€§
                    potential_user_patterns = [
                        {
                            "value": self._get_user_name_regex(matched_patterns),
                            "type": "regex",
                            "description": "éªŒè¯ç”¨æˆ·å§“åå­—æ®µ",
                            "jsonPath": "" if self._is_html_response(matched_patterns) else "$.user_name,$.customer_name,$.holder_name,$.full_name",
                            "regex": self._get_user_name_regex(matched_patterns),
                            "hash": "sha256",
                            "field_name": "ç”¨æˆ·å§“å"
                        },
                        {
                            "value": self._get_name_component_regex(matched_patterns),
                            "type": "regex",
                            "description": "éªŒè¯å§“åç»„ä»¶å­—æ®µ",
                            "jsonPath": "" if self._is_html_response(matched_patterns) else "$.first_name,$.last_name,$.display_name",
                            "regex": self._get_name_component_regex(matched_patterns),
                            "hash": "sha256",
                            "field_name": "å§“åç»„ä»¶"
                        }
                    ]

                    # ğŸ¯ éªŒè¯æ¯ä¸ªç”¨æˆ·å§“åæ¨¡å¼çš„æœ‰æ•ˆæ€§
                    user_patterns = []
                    for pattern in potential_user_patterns:
                        if self._validate_regex_effectiveness(response_content, pattern["regex"], pattern["field_name"]):
                            user_patterns.append(pattern)
                        else:
                            print(f"âš ï¸ è·³è¿‡ç”Ÿæˆ {pattern['field_name']} çš„åŒ¹é…è§„åˆ™")

                    for user_pattern in user_patterns:
                        response_matches.append({
                            "value": user_pattern["value"],
                            "type": user_pattern["type"],
                            "invert": False,
                            "description": user_pattern["description"],
                            "order": order_counter,
                            "isOptional": False
                        })

                        response_redactions.append({
                            "xPath": "",
                            "jsonPath": user_pattern["jsonPath"],
                            "regex": user_pattern["regex"],
                            "hash": user_pattern["hash"],
                            "order": order_counter
                        })
                        order_counter += 1

                elif "content:asset" in pattern or "content:wealth" in pattern:
                    # èµ„äº§ç›¸å…³API - ç”Ÿæˆèµ„äº§ä¿¡æ¯éªŒè¯å’Œæå–è§„åˆ™
                    asset_patterns = [
                        {
                            "value": self._get_total_asset_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "type": "regex",
                            "description": "éªŒè¯æ€»èµ„äº§å­—æ®µ",
                            "jsonPath": "" if self._is_html_response(matched_patterns) else "$.total_asset,$.net_worth,$.portfolio_value",
                            "regex": self._get_total_asset_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "hash": ""
                        },
                        {
                            "value": self._get_market_value_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "type": "regex",
                            "description": "éªŒè¯å¸‚å€¼å­—æ®µ",
                            "jsonPath": "" if self._is_html_response(matched_patterns) else "$.market_value,$.book_value,$.investment_value",
                            "regex": self._get_market_value_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                            "hash": ""
                        }
                    ]

                    for asset_pattern in asset_patterns:
                        response_matches.append({
                            "value": asset_pattern["value"],
                            "type": asset_pattern["type"],
                            "invert": False,
                            "description": asset_pattern["description"],
                            "order": order_counter,
                            "isOptional": False
                        })

                        response_redactions.append({
                            "xPath": "",
                            "jsonPath": asset_pattern["jsonPath"],
                            "regex": asset_pattern["regex"],
                            "hash": asset_pattern["hash"],
                            "order": order_counter
                        })
                        order_counter += 1

                elif pattern.startswith("core_banking:"):
                    # æ ¸å¿ƒé“¶è¡Œä¸šåŠ¡ - ç”Ÿæˆé‡‘èæ•°æ®éªŒè¯è§„åˆ™
                    response_matches.append({
                        "value": self._get_core_banking_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                        "type": "regex",
                        "invert": False,
                        "description": "éªŒè¯æ ¸å¿ƒé“¶è¡Œä¸šåŠ¡æ•°æ®",
                        "order": order_counter,
                        "isOptional": False
                    })

                    response_redactions.append({
                        "xPath": "",
                        "jsonPath": "",
                        "regex": self._get_core_banking_regex(matched_patterns),  # ğŸ¯ æ ¹æ®å“åº”ç±»å‹åŠ¨æ€ç”Ÿæˆ
                        "hash": "",
                        "order": order_counter
                    })
                    order_counter += 1

            if response_matches or response_redactions:
                print(f"âœ… æˆåŠŸç”Ÿæˆ: {len(response_matches)} ä¸ªéªŒè¯è§„åˆ™, {len(response_redactions)} ä¸ªæå–è§„åˆ™")

                # ğŸ¯ å»é‡å¤„ç†ï¼šç§»é™¤é‡å¤çš„responseMatcheså’ŒresponseRedactions
                response_matches = self._deduplicate_response_matches(response_matches)
                response_redactions = self._deduplicate_response_redactions(response_redactions)

                print(f"ğŸ”§ å»é‡å: responseMatches {len(response_matches)}ä¸ª, responseRedactions {len(response_redactions)}ä¸ª")

            # ğŸ¯ è´¨é‡è¿‡æ»¤ï¼šä»…ä¿ç•™ä¸­ç­‰åä¸Šè´¨é‡çš„åŒ¹é…è§„åˆ™
            try:
                quality_threshold = 6.5  # ä¸­ç­‰åä¸Š
                filtered_matches = self._filter_response_matches_by_quality(
                    response_matches,
                    response_content,
                    threshold=quality_threshold
                )
                print(f"ğŸ§ª è´¨é‡è¿‡æ»¤: é˜ˆå€¼={quality_threshold}ï¼Œä¿ç•™ {len(filtered_matches)}/{len(response_matches)} ä¸ª")
                response_matches = filtered_matches
            except Exception as _e:
                print(f"âš ï¸ è´¨é‡è¿‡æ»¤å¼‚å¸¸ï¼ˆè·³è¿‡ï¼‰ï¼š{_e}")
            
            # HSBC å®šåˆ¶åŒ–ï¼šå¯¹ hsbc.com.hk + /api/mmf- ç«¯ç‚¹ï¼Œç¼©å‡ä¸ºâ€œæœ€å°ç¨³å®šé›†â€ï¼Œå…¶ä½™æœ‰åˆ™åŠ ï¼Œæ— åˆ™ä¸åŠ 
            try:
                response_matches = self._refine_response_matches_for_hsbc(url, response_content, response_matches)
            except Exception as _e:
                print(f"âš ï¸ HSBC ç²¾ç®€è§„åˆ™å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{_e}")

            # è´¦æˆ·å·è§„åˆ™å¢å¼ºï¼ˆå‘½ä¸­æ‰åŠ å…¥ï¼Œé¿å… AND é£é™©ï¼‰
            try:
                response_matches = self._augment_account_number_rules(url, response_content, response_matches)
            except Exception as _e:
                print(f"âš ï¸ è´¦æˆ·å·è§„åˆ™å¢å¼ºå¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{_e}")

            # æœ€ç»ˆæ ¡éªŒï¼šä»…ä¿ç•™å½“å‰å“åº”ç¡®å®å‘½ä¸­çš„è§„åˆ™ï¼Œæ»¡è¶³ AND è¯­ä¹‰
            try:
                verified_matches = self._verify_response_matches_attestor_and_logic(response_matches, response_content)
                if len(verified_matches) != len(response_matches):
                    print(f"âœ… ANDæ ¡éªŒåä¿ç•™ {len(verified_matches)}/{len(response_matches)} æ¡åŒ¹é…è§„åˆ™")
                response_matches = verified_matches
            except Exception as _e:
                print(f"âš ï¸ ANDæ ¡éªŒå¼‚å¸¸ï¼ˆè·³è¿‡ï¼‰ï¼š{_e}")

            # è‹¥è¿‡æ»¤åæ— æœ‰æ•ˆè§„åˆ™ï¼Œå¯é€‰æ‹©ä¸å¼ºå¡é€šç”¨ containsï¼Œé¿å…è¯¯å¯¼
            return response_matches, response_redactions

        # ğŸ”„ å›é€€ï¼šä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
        print(f"âš ï¸  å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•ç”Ÿæˆå“åº”æ¨¡å¼")
        try:
            # å°è¯•è§£æJSONå“åº”
            response_json = json.loads(response_content)

            # åˆ†æJSONç»“æ„ï¼Œæå–å…³é”®å­—æ®µ
            financial_patterns = self.analyze_json_financial_patterns(response_json)

            for pattern in financial_patterns:
                # ä¸å†å‘ responseMatches æ³¨å…¥é€šç”¨/å¯å‘å¼è§„åˆ™ï¼Œä»…åœ¨ç¡®è®¤æå–éœ€è¦æ—¶æ„å»º redactions
                if pattern['type'] == 'amount':
                    response_redactions.append({
                        "xPath": "",
                        "jsonPath": pattern['json_path'],
                        "regex": f'"{pattern["field"]}":(?P<field_value>.*)',
                        "hash": "",
                        "order": None
                    })

                elif pattern['type'] == 'account':
                    response_redactions.append({
                        "xPath": "",
                        "jsonPath": pattern['json_path'],
                        "regex": f'"{pattern["field"]}":"(?P<field_value>.*)"',
                        "hash": "",
                        "order": None
                    })

        except json.JSONDecodeError:
            # éJSONå“åº”ï¼Œä½¿ç”¨æ–‡æœ¬æ¨¡å¼åˆ†æ
            text_patterns = self.analyze_text_financial_patterns(response_content)

            # æ–‡æœ¬åœºæ™¯ä¸‹ï¼Œä¸å†æ³¨å…¥é€šç”¨ regex åˆ° responseMatchesï¼Œé¿å…ç¡¬ç¼–ç è¯¯æ€

        # HSBC å®šåˆ¶åŒ–ï¼ˆfallback åˆ†æ”¯ï¼‰
        try:
            response_matches = self._refine_response_matches_for_hsbc(url, response_content, response_matches)
        except Exception as _e:
            print(f"âš ï¸ HSBC ç²¾ç®€è§„åˆ™å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{_e}")

        # è´¦æˆ·å·è§„åˆ™å¢å¼ºï¼ˆfallback åˆ†æ”¯ï¼‰
        try:
            response_matches = self._augment_account_number_rules(url, response_content, response_matches)
        except Exception as _e:
            print(f"âš ï¸ è´¦æˆ·å·è§„åˆ™å¢å¼ºå¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{_e}")

        # æœ€ç»ˆ AND å¤æ ¸ï¼ˆfallback åˆ†æ”¯ï¼‰ï¼šä»…ä¿ç•™å½“å‰åº”ç­”ä¸ŠçœŸå®å‘½ä¸­çš„è§„åˆ™
        try:
            verified_matches = self._verify_response_matches_attestor_and_logic(response_matches, response_content)
            if len(verified_matches) != len(response_matches):
                print(f"âœ… ANDæ ¡éªŒ(å›é€€)åä¿ç•™ {len(verified_matches)}/{len(response_matches)} æ¡åŒ¹é…è§„åˆ™")
            response_matches = verified_matches
        except Exception as _e:
            print(f"âš ï¸ ANDæ ¡éªŒ(å›é€€)å¼‚å¸¸ï¼ˆè·³è¿‡ï¼‰ï¼š{_e}")

        return response_matches, response_redactions

    def _refine_response_matches_for_hsbc(self, url: str, body: str, response_matches: List[Dict]) -> List[Dict]:
        """å¯¹ hsbc.com.hk + /api/mmf- ç«¯ç‚¹è¿›è¡Œâ€œæœ€å°ç¨³å®šé›†â€ç²¾ç®€ï¼š
        - ä»…ä¿ç•™ç¨³å®šå­—æ®µç”¨äº AND æ ¡éªŒï¼ˆå‘½ä¸­æ‰åŠ å…¥ï¼‰
        - è´¦æˆ·ç±»ç«¯ç‚¹ï¼ˆaccounts/domesticï¼‰ï¼šè‹¥ç¡®å®å­˜åœ¨ï¼Œå†è¿½åŠ  accountNumberã€accountType|accountStatus
        å…¶ä»–é“¶è¡Œ/ç«¯ç‚¹ä¸å˜ã€‚
        """
        try:
            from urllib.parse import urlparse
            import re
            pr = urlparse(url)
            host = (pr.netloc or '').lower()
            path = (pr.path or '')
        except Exception:
            return response_matches

        if 'hsbc.com.hk' not in host or '/api/mmf-' not in path:
            return response_matches

        body = body or ''
        refined: List[Dict] = []

        def add_contains(val: str):
            refined.append({ 'type': 'contains', 'value': val, 'invert': False })

        def add_regex(pattern: str):
            refined.append({ 'type': 'regex', 'value': pattern, 'invert': False })

        # æœ€å°ç¨³å®šé›†ï¼šcurrencyï¼ˆcontainsï¼‰ + currencyCode æ­£åˆ™ + amount/value æ­£åˆ™ï¼ˆå‘½ä¸­æ‰åŠ ï¼‰
        try:
            if 'currency' in body:
                add_contains('"currency"')
            if re.search(r'"(?:currency|currencyCode)"\s*:\s*"[A-Z]{3}"', body, re.S):
                add_regex(r'"(?:currency|currencyCode)"\s*:\s*"(?P<currency>[A-Z]{3})"')
            if re.search(r'"(?:amount|value|availableBalance)"\s*:\s*[0-9.]+', body, re.S):
                add_regex(r'"(?:amount|value|availableBalance)"\s*:\s*(?P<amount>[0-9.]+)')
        except Exception:
            pass

        # accounts/domestic ä¸‹å†æ‹©æœºè¿½åŠ ï¼ˆå‘½ä¸­æ‰åŠ ï¼‰ï¼Œé¿å… AND å¤±è´¥
        if 'accounts/domestic' in path:
            try:
                if '"accountNumber"' in body:
                    add_contains('"accountNumber"')
                if re.search(r'"(?:accountType|accountStatus)"\s*:\s*"[^"]+"', body, re.S):
                    add_regex(r'"(?:accountType|accountStatus)"\s*:\s*"(?P<account_type>[^"]+)"')
                # ä¸»æµè´§å¸é›†å‘½ä¸­å†è¿½åŠ 
                if re.search(r'"(?:HKD|USD|CNY|EUR|GBP|JPY|AUD|CAD|SGD)"', body, re.S):
                    add_regex(r'"(?P<major_currency>HKD|USD|CNY|EUR|GBP|JPY|AUD|CAD|SGD)"')
            except Exception:
                pass

        # è‹¥æœ€å°é›†ä¸ºç©ºï¼Œåˆ™å›é€€åŸæœ‰ï¼ˆé¿å…æ¸…ç©ºå¯¼è‡´ schema ä¸æ»¡è¶³ï¼‰
        return refined if len(refined) > 0 else response_matches

    def _augment_account_number_rules(self, url: str, body: str, response_matches: List[Dict]) -> List[Dict]:
        """å¢å¼ºè´¦æˆ·å·è¯†åˆ«è§„åˆ™ï¼ˆä½é£é™©ï¼‰ï¼š
        - ä»…å½“æ­£æ–‡ä¸­æ£€æµ‹åˆ°â€œæœªæ©ç è´¦å·â€å€™é€‰æ—¶ï¼Œæ‰åŠ å…¥ AND è§„åˆ™
        - JSONï¼šé”®ååŒä¹‰è¯ä¸¥æ ¼åŒ¹é… -> ä»…æ•°å­—ä¸åˆ†éš”ç¬¦
        - HTML/TEXTï¼šé‚»è¿‘å…³é”®è¯ + æ•°å­—åºåˆ—ï¼ˆå…è®¸ç©ºæ ¼/çŸ­æ¨ªï¼Œä½†ä¸å…è®¸ * X æ©ç ï¼‰
        - é€‚ç”¨ï¼šBOC HK / CMB WL ä¼˜å…ˆï¼›å…¶ä»–åŸŸè‹¥å‘½ä¸­ä¹Ÿå¯å—ç›Š
        """
        import re
        from urllib.parse import urlparse

        body = body or ''
        pr = urlparse(url)
        host = (pr.netloc or '').lower()
        path = (pr.path or '')

        # å€™é€‰åŸŸï¼ˆä¼˜å…ˆå¯ç”¨ï¼‰
        preferred_hosts = (
            'its.bochk.com', 'bochk.com',
            'www.cmbwinglungbank.com', 'cmbwinglungbank.com'
        )

        # JSON é”®ååŒä¹‰è¯
        json_keys = r'(?:accountNumber|accNo|acctNo|accountId|displayAccountNumber)'
        json_regex = rf'"{json_keys}"\s*:\s*"(?P<account_number>\d(?:[ -]?\d){{7,19}})"'

        # HTML/TEXTï¼šå…³é”®è¯é‚»åŸŸ + è´¦å·
        # å…³é”®è¯ï¼ˆä¸­è‹±ï¼‰
        kw = r'(?:è´¦æˆ·|å¸³è™Ÿ|è³¬è™Ÿ|Account(?:\s*No)?|Acct(?:\s*No)?)'
        # è´¦å·ä¸»ä½“ï¼šä»…æ•°å­—åŠå¯é€‰åˆ†éš”ç¬¦ï¼ˆç©ºæ ¼/çŸ­æ¨ªï¼‰ï¼›ä¸å…è®¸ * x X â€¢ ç­‰æ©ç å­—ç¬¦
        acct_core = r'(?P<account_number>\d(?:[ -]?\d){7,19})'
        html_regex = rf'{kw}[^\n\r\d]{{0,32}}{acct_core}'

        def already_has_account_rule(rms: List[Dict]) -> bool:
            for m in rms or []:
                val = (m.get('value') or '')
                if 'account_number' in val or 'accountNumber' in val or 'accNo' in val:
                    return True
            return False

        # å¿«é€Ÿæ£€æµ‹ï¼šæ˜¯å¦åŒ…å«æœªæ©ç è´¦å·å€™é€‰
        has_candidate = False
        # 1) JSON é£æ ¼
        if re.search(json_regex, body, re.S):
            has_candidate = True
        # 2) HTML/æ–‡æœ¬é£æ ¼ï¼ˆå…³é”®è¯é‚»åŸŸï¼‰
        if re.search(html_regex, body, re.S | re.I):
            has_candidate = True

        if not has_candidate or already_has_account_rule(response_matches):
            return response_matches

        # ä»…å½“æ¥è‡ªä¼˜å…ˆåŸŸæˆ–æ£€æµ‹å‘½ä¸­æ—¶åŠ å…¥ï¼Œé¿å…å¯¹æ•´ä¸ªç³»ç»Ÿå¸¦æ¥è¯¯æŠ¥
        if any(h in host for h in preferred_hosts) or has_candidate:
            # JSON è§„åˆ™ï¼ˆå‘½ååˆ†ç»„ï¼Œç”¨äºæå–ï¼‰
            if re.search(json_regex, body, re.S):
                response_matches.append({
                    'type': 'regex',
                    'value': json_regex,
                    'invert': False,
                    'description': 'æå–è´¦æˆ·å·ï¼ˆJSONé”®ååŒä¹‰è¯ï¼‰'
                })
            # HTML/æ–‡æœ¬è§„åˆ™ï¼ˆå¸¦å…³é”®è¯çš„ç¨³å¥ç‰ˆæœ¬ï¼‰
            if re.search(html_regex, body, re.S | re.I):
                response_matches.append({
                    'type': 'regex',
                    'value': html_regex,
                    'invert': False,
                    'description': 'æå–è´¦æˆ·å·ï¼ˆå…³é”®è¯é‚»åŸŸ+æœªæ©ç ï¼‰'
                })

        return response_matches

    def _verify_response_matches_attestor_and_logic(self, response_matches: List[Dict], response_content: str) -> List[Dict]:
        """æ ¡éªŒ responseMatches çš„ AND è¯­ä¹‰ï¼šä»…è¿”å›åœ¨å½“å‰å“åº”ä¸Šå…¨éƒ¨èƒ½å‘½ä¸­çš„è§„åˆ™ã€‚

        - contains: ä½œä¸ºå­ä¸²æ£€æŸ¥ï¼Œæ”¯æŒå¤§å°å†™æ•æ„Ÿçš„ç²¾ç¡®åŒ…å«ï¼ˆä¸ attestor è¡Œä¸ºä¸€è‡´ï¼‰
        - regex: ä½¿ç”¨ Python çš„ re æ¨¡å—è¿›è¡ŒåŒ¹é…ï¼ˆDOTALLï¼‰ï¼Œè‹¥è¡¨è¾¾å¼æ— æ•ˆåˆ™ä¸¢å¼ƒè¯¥æ¡
        - invert: åè½¬åŒ¹é…ç»“æœ

        Args:
            response_matches: å€™é€‰åŒ¹é…è§„åˆ™
            response_content: æœ¬æ¬¡æ ·æœ¬å“åº”å†…å®¹

        Returns:
            é€šè¿‡éªŒè¯çš„åŒ¹é…è§„åˆ™åˆ—è¡¨ï¼ˆä¿è¯æ¯ä¸€æ¡éƒ½å‘½ä¸­å½“å‰å“åº”ï¼Œä»è€Œ AND å¯é€šè¿‡ï¼‰
        """
        import re

        verified: List[Dict] = []
        body = response_content or ""

        for m in response_matches or []:
            t = (m.get('type') or 'regex').strip()
            v = m.get('value') or ''
            inv = bool(m.get('invert', False))

            if not v:
                # ç©ºè§„åˆ™ç›´æ¥è·³è¿‡
                continue

            matched = False
            try:
                if t == 'contains':
                    matched = (v in body)
                elif t == 'regex':
                    # ä½¿ç”¨ DOTALL ä»¥é€‚é…è·¨è¡ŒåŒ¹é…ï¼Œå°½é‡è´´è¿‘ attestor çš„å­—ç¬¦ä¸²è§†å›¾
                    matched = re.search(v, body, re.DOTALL) is not None
                else:
                    # æœªçŸ¥ç±»å‹ï¼Œè·³è¿‡
                    continue
            except re.error:
                # éæ³•æ­£åˆ™ï¼Œè·³è¿‡
                matched = False

            # å¤„ç† invert è¯­ä¹‰
            matched = (not matched) if inv else matched

            if matched:
                verified.append(m)

        return verified

    def analyze_json_financial_patterns(self, json_data: Any, path: str = "$") -> List[Dict]:
        """åˆ†æJSONæ•°æ®ä¸­çš„é‡‘èæ¨¡å¼"""
        patterns = []

        if isinstance(json_data, dict):
            for key, value in json_data.items():
                current_path = f"{path}.{key}"

                # æ£€æŸ¥é‡‘é¢å­—æ®µ
                if self.is_amount_field(key, value):
                    patterns.append({
                        'field': key,
                        'type': 'amount',
                        'pattern': '{{credit_amount}}' if 'credit' in key.lower() else '{{amount}}',
                        'json_path': current_path,
                        'description': 'é‡‘é¢å­—æ®µ'
                    })

                # æ£€æŸ¥è´¦æˆ·å­—æ®µ
                elif self.is_account_field(key, value):
                    patterns.append({
                        'field': key,
                        'type': 'account',
                        'pattern': '{{account_number}}',
                        'json_path': current_path,
                        'description': 'è´¦æˆ·å­—æ®µ'
                    })

                # æ£€æŸ¥äº¤æ˜“IDå­—æ®µ
                elif self.is_transaction_field(key, value):
                    patterns.append({
                        'field': key,
                        'type': 'transaction',
                        'pattern': '{{trans_id}}',
                        'json_path': current_path,
                        'description': 'äº¤æ˜“IDå­—æ®µ'
                    })

                # é€’å½’å¤„ç†åµŒå¥—å¯¹è±¡
                if isinstance(value, (dict, list)):
                    patterns.extend(self.analyze_json_financial_patterns(value, current_path))

        elif isinstance(json_data, list):
            for i, item in enumerate(json_data):
                current_path = f"{path}[{i}]"
                patterns.extend(self.analyze_json_financial_patterns(item, current_path))

        return patterns

    def is_amount_field(self, key: str, value: Any) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé‡‘é¢å­—æ®µ"""
        amount_keywords = ['amount', 'balance', 'value', 'total', 'sum', 'é‡‘é¢', 'ä½™é¢', 'æ€»é¢']
        key_lower = key.lower()

        # æ£€æŸ¥å­—æ®µå
        if any(keyword in key_lower for keyword in amount_keywords):
            # æ£€æŸ¥å€¼æ˜¯å¦ä¸ºæ•°å­—
            if isinstance(value, (int, float)) or (isinstance(value, str) and value.replace('.', '').replace(',', '').isdigit()):
                return True

        return False

    def is_account_field(self, key: str, value: Any) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè´¦æˆ·å­—æ®µ"""
        account_keywords = ['account', 'acct', 'number', 'id', 'è´¦æˆ·', 'è´¦å·']
        key_lower = key.lower()

        if any(keyword in key_lower for keyword in account_keywords):
            if isinstance(value, str) and len(value) > 5:  # è´¦æˆ·å·é€šå¸¸è¾ƒé•¿
                return True

        return False

    def is_transaction_field(self, key: str, value: Any) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“å­—æ®µ"""
        transaction_keywords = ['transaction', 'trans', 'txn', 'reference', 'cheque', 'äº¤æ˜“', 'æµæ°´']
        key_lower = key.lower()

        if any(keyword in key_lower for keyword in transaction_keywords):
            if isinstance(value, str):
                return True

        return False

    def analyze_text_financial_patterns(self, text: str) -> List[Dict]:
        """åˆ†ææ–‡æœ¬ä¸­çš„é‡‘èæ¨¡å¼"""
        patterns = []

        # é‡‘é¢æ¨¡å¼
        amount_patterns = [
            (r'ä½™é¢[ï¼š:]\s*([0-9,]+\.?\d*)', 'ä½™é¢åŒ¹é…'),
            (r'é‡‘é¢[ï¼š:]\s*([0-9,]+\.?\d*)', 'é‡‘é¢åŒ¹é…'),
            (r'è´¦æˆ·ä½™é¢[ï¼š:]\s*([0-9,]+\.?\d*)', 'è´¦æˆ·ä½™é¢åŒ¹é…')
        ]

        for pattern, description in amount_patterns:
            if re.search(pattern, text):
                patterns.append({
                    'regex': pattern,
                    'description': description,
                    'type': 'amount'
                })

        return patterns

    def calculate_request_hash(self, url: str, method: str, headers: Dict[str, str]) -> str:
        """è®¡ç®—è¯·æ±‚å“ˆå¸Œ"""
        # æ„å»ºç”¨äºå“ˆå¸Œçš„å­—ç¬¦ä¸²
        hash_string = f"{method}:{url}:{json.dumps(sorted(headers.items()))}"

        # è®¡ç®—SHA256å“ˆå¸Œ
        hash_object = hashlib.sha256(hash_string.encode())
        return f"0x{hash_object.hexdigest()}"

    def filter_important_headers(self, headers: Dict[str, str]) -> Dict[str, str]:
        """è¿‡æ»¤å‡ºé‡è¦çš„headers"""
        important_headers = {}

        for name, value in headers.items():
            name_lower = name.lower()

            # æ£€æŸ¥æ˜¯å¦ä¸ºé‡è¦header
            is_important = False

            # è®¤è¯ç›¸å…³header
            for pattern in self.auth_header_patterns:
                if pattern in name_lower:
                    is_important = True
                    break

            # å…¶ä»–é‡è¦header
            if not is_important:
                for pattern in self.important_header_patterns:
                    if pattern in name_lower:
                        is_important = True
                        break

            if is_important:
                important_headers[name] = value

        return important_headers

    def perform_quality_check(self, api_data: Dict[str, Any], flow_data: Dict[str, Any]) -> ProviderQualityCheck:
        """æ‰§è¡Œè´¨é‡æ£€æŸ¥"""
        check = ProviderQualityCheck()

        # æ£€æŸ¥è®¤è¯ä¿¡æ¯
        auth_info = self.extract_authentication_info(flow_data['request_headers'])
        check.has_authentication = auth_info['has_auth']
        if not check.has_authentication:
            check.missing_fields.append('authentication_headers')

        # æ£€æŸ¥å“åº”æ•°æ®
        response_body = flow_data.get('response_body')
        if response_body:
            try:
                response_content = response_body.decode('utf-8', errors='ignore')
                check.has_response_data = len(response_content) > 100  # è‡³å°‘100å­—ç¬¦

                # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡‘èæ¨¡å¼
                financial_keywords = ['balance', 'amount', 'account', 'transaction', 'ä½™é¢', 'é‡‘é¢', 'è´¦æˆ·']
                check.has_financial_patterns = any(keyword in response_content.lower() for keyword in financial_keywords)
            except:
                check.has_response_data = False

        if not check.has_response_data:
            check.missing_fields.append('response_data')
        if not check.has_financial_patterns:
            check.missing_fields.append('financial_patterns')

        # æ£€æŸ¥headeræ•°é‡
        important_headers = self.filter_important_headers(flow_data['request_headers'])
        check.has_sufficient_headers = len(important_headers) >= 3
        if not check.has_sufficient_headers:
            check.missing_fields.append('sufficient_headers')

        # è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°
        score = 0
        if check.has_authentication:
            score += 30
        if check.has_response_data:
            score += 25
        if check.has_financial_patterns:
            score += 25
        if check.has_sufficient_headers:
            score += 20

        check.confidence_score = score / 100.0

        return check

    def build_provider_for_api(self, api_data: Dict[str, Any]) -> Tuple[Optional[Dict], Optional[ProviderQualityCheck]]:
        """ä¸ºå•ä¸ªAPIæ„å»ºprovideré…ç½®

        Args:
            api_data: APIåˆ†ææ•°æ®

        Returns:
            Tuple[Optional[Dict], Optional[ProviderQualityCheck]]: (provideré…ç½®, è´¨é‡æ£€æŸ¥ç»“æœ)
        """
        url = api_data['url']

        # è·å–åŸå§‹æµæ•°æ®
        flow_data = self.flow_data_map.get(url)
        if not flow_data:
            print(f"âš ï¸  æœªæ‰¾åˆ°URLçš„æµæ•°æ®: {url}")
            return None, None

        # æ‰§è¡Œè´¨é‡æ£€æŸ¥
        quality_check = self.perform_quality_check(api_data, flow_data)

        # å¦‚æœè´¨é‡æ£€æŸ¥ä¸é€šè¿‡ï¼Œè¿”å›æ£€æŸ¥ç»“æœç”¨äºå­˜ç–‘æ–‡ä»¶
        if quality_check.confidence_score < 0.6:  # 60%ç½®ä¿¡åº¦é˜ˆå€¼
            return None, quality_check

        # è§£æå“åº”å†…å®¹
        response_content = ""
        if flow_data['response_body']:
            try:
                response_content = flow_data['response_body'].decode('utf-8', errors='ignore')
            except:
                response_content = ""

        # ğŸ¯ æå–å“åº”æ¨¡å¼ - ä¼ å…¥APIæ•°æ®ä»¥åˆ©ç”¨ç‰¹å¾åº“åŒ¹é…ç»“æœ
        response_matches, response_redactions = self.extract_response_patterns(response_content, url, api_data)

        # ç¡¬è§„åˆ™ï¼šresponseMatches ä¸ºç©ºåˆ™ä¸çº³å…¥ provider
        if not response_matches:
            print(f"âš ï¸  responseMatches ä¸ºç©ºï¼Œä¸çº³å…¥provider: {url}")
            quality_check.missing_fields.append('response_matches')
            return None, quality_check

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å“åº”æ¨¡å¼ï¼ˆåŒç©ºï¼‰ï¼Œé™çº§å¤„ç†ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
        if not response_matches and not response_redactions:
            print(f"âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆçš„å“åº”æ¨¡å¼: {url}")
            quality_check.missing_fields.append('response_patterns')
            quality_check.confidence_score *= 0.7  # é™ä½ç½®ä¿¡åº¦

            if quality_check.confidence_score < 0.6:
                return None, quality_check

        # æå–é‡è¦çš„headers
        important_headers = self.filter_important_headers(flow_data['request_headers'])

        # è®¡ç®—è¯·æ±‚å“ˆå¸Œ
        request_hash = self.calculate_request_hash(url, flow_data['method'], important_headers)

        # æ„å»ºprovideré…ç½®
        provider_config = {
            "providerConfig": {
                "id": str(uuid.uuid4()).replace('-', '')[:24],  # 24å­—ç¬¦ID
                "createdAt": None,
                "providerId": str(uuid.uuid4()),
                "version": {
                    "major": 1,
                    "minor": 0,
                    "patch": 0,
                    "prereleaseTag": None,
                    "prereleaseNumber": None
                },
                "providerConfig": {
                    "loginUrl": self.extract_login_url(url),
                    "customInjection": None,
                    "userAgent": {
                        "ios": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1",
                        "android": None
                    },
                    "geoLocation": self.extract_geo_location(flow_data),
                    "injectionType": "NONE",
                    "disableRequestReplay": True,
                    "verificationType": "WITNESS",
                    "requestData": [
                        {
                            "url": url,
                            "expectedPageUrl": "",
                            "urlType": "CONSTANT",
                            "method": flow_data['method'],
                            "responseMatches": response_matches,
                            "responseRedactions": [],
                            "bodySniff": {
                                "enabled": False,
                                "template": ""
                            },
                            "requestHash": request_hash,
                            "responseVariables": self.extract_response_variables(response_matches, response_redactions)
                        }
                    ],
                    "pageTitle": None,
                    "metadata": {
                        "institution": api_data.get('institution', ''),
                        "api_type": self.classify_api_type(url, response_content),
                        "value_score": api_data.get('value_score', 0),
                        "priority_level": api_data.get('priority_level', 'medium'),
                        "generated_at": datetime.now().isoformat(),
                        "confidence_score": quality_check.confidence_score
                    },
                    "stepsToFollow": None,
                    "useIncognitoWebview": None
                },
                "createdBy": "auto_generated_provider_builder"
            }
        }

        return provider_config, quality_check

    def extract_login_url(self, api_url: str) -> str:
        """é€šè¿‡ä¸Šä¸‹æ–‡åˆ†ææå–çœŸå®çš„ç™»å½•URL

        Args:
            api_url: å½“å‰APIçš„URL

        Returns:
            str: ç™»å½•URLæˆ–æç¤ºä¿¡æ¯
        """
        parsed = urlparse(api_url)
        domain = parsed.netloc

        # ğŸ¯ ä¸Šä¸‹æ–‡åˆ†æï¼šåœ¨åŒåŸŸåçš„è®¤è¯ç±»APIä¸­æŸ¥æ‰¾çœŸå®ç™»å½•URL
        real_login_url = self._find_real_login_url_from_context(domain)

        if real_login_url:
            return real_login_url

        # ğŸ¯ å¦‚æœæ²¡æœ‰æ‰¾åˆ°çœŸå®ç™»å½•URLï¼Œè¿”å›æç¤ºä¿¡æ¯
        return f"[éœ€è¦ä¸Šä¸‹æ–‡åˆ†æ] æœªåœ¨æŠ“åŒ…æ•°æ®ä¸­æ‰¾åˆ° {domain} çš„ç™»å½•URLï¼Œè¯·æ‰‹åŠ¨ç¡®è®¤"

    def _find_real_login_url_from_context(self, domain: str) -> Optional[str]:
        """ä»ä¸Šä¸‹æ–‡ä¸­æŸ¥æ‰¾çœŸå®çš„ç™»å½•URL

        Args:
            domain: ç›®æ ‡åŸŸå

        Returns:
            Optional[str]: æ‰¾åˆ°çš„ç™»å½•URLï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°è¿”å›None
        """
        # ä»åˆ†ææ•°æ®ä¸­æŸ¥æ‰¾åŒåŸŸåçš„è®¤è¯ç±»API
        if not hasattr(self, 'analysis_data') or not self.analysis_data:
            return None

        extracted_data = self.analysis_data.get('extracted_data', [])

        login_candidates = []

        for api_data in extracted_data:
            api_url = api_data.get('url', '')
            api_category = api_data.get('api_category', 'unknown')

            # ğŸ¯ ç¬¬ä¸€æ­¥ï¼šè¯†åˆ«ç™»å½•æäº¤é¡µï¼ˆä¼˜å…ˆçº§é«˜ï¼‰
            if domain in api_url and api_category == 'auth':
                flow_data = self.flow_data_map.get(api_url)
                if flow_data:
                    # è¯„åˆ†ç™»å½•æäº¤é¡µ
                    submit_score = self._score_login_submit_api(api_url, flow_data)
                    if submit_score > 20:  # åªæœ‰é«˜åˆ†çš„æ‰è®¤ä¸ºæ˜¯ç™»å½•æäº¤é¡µ
                        login_candidates.append({
                            'url': api_url,
                            'score': submit_score,
                            'type': 'submit',
                            'flow_data': flow_data
                        })
                        print(f"ğŸ” å‘ç°ç™»å½•æäº¤é¡µå€™é€‰: {api_url} (è¯„åˆ†: {submit_score})")

        # ğŸ¯ ç¬¬äºŒæ­¥ï¼šåŸºäºç™»å½•æäº¤é¡µæ‰¾å¯¹åº”çš„ç™»å½•é¡µ
        if login_candidates:
            # é€‰æ‹©è¯„åˆ†æœ€é«˜çš„ç™»å½•æäº¤é¡µ
            best_submit = max(login_candidates, key=lambda x: x['score'])
            print(f"ğŸ¯ æœ€ä½³ç™»å½•æäº¤é¡µ: {best_submit['url']} (è¯„åˆ†: {best_submit['score']})")

            # å°è¯•æ‰¾åˆ°å¯¹åº”çš„ç™»å½•é¡µé¢
            login_page = self._find_corresponding_login_page(domain, best_submit)
            if login_page:
                print(f"ğŸ” æ‰¾åˆ°å¯¹åº”çš„ç™»å½•é¡µé¢: {login_page}")
                return login_page
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ç™»å½•é¡µé¢ï¼Œè¿”å›ç™»å½•æäº¤é¡µçš„URLï¼ˆå»æ‰å‚æ•°ï¼‰
                submit_url = best_submit['url'].split('?')[0]
                print(f"âš ï¸  æœªæ‰¾åˆ°ç™»å½•é¡µé¢ï¼Œä½¿ç”¨ç™»å½•æäº¤é¡µ: {submit_url}")
                return submit_url

        if login_candidates:
            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ç™»å½•URL
            best_candidate = max(login_candidates, key=lambda x: x['score'])
            print(f"ğŸ” é€šè¿‡ä¸Šä¸‹æ–‡åˆ†ææ‰¾åˆ°ç™»å½•URL: {best_candidate['url']}")
            return best_candidate['url']

        # ğŸ¯ å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç™»å½•æäº¤é¡µï¼Œä½¿ç”¨å…¨é‡æµæ•°æ®åˆ†æ
        print(f"âš ï¸  ç‰¹å¾åº“æœªè¯†åˆ«åˆ°ç™»å½•æäº¤é¡µï¼Œå¯ç”¨å…¨é‡æµæ•°æ®åˆ†æ...")
        discovered_submit = self._discover_login_submit_by_behavior(domain)
        if discovered_submit:
            print(f"ğŸ¯ é€šè¿‡è¡Œä¸ºåˆ†æå‘ç°ç™»å½•æäº¤é¡µ: {discovered_submit['url']} (è¯„åˆ†: {discovered_submit['score']})")

            # å°è¯•æ‰¾åˆ°å¯¹åº”çš„ç™»å½•é¡µé¢
            login_page = self._find_corresponding_login_page(domain, discovered_submit)
            if login_page:
                print(f"ğŸ” æ‰¾åˆ°å¯¹åº”çš„ç™»å½•é¡µé¢: {login_page}")
                return login_page
            else:
                # è¿”å›ç™»å½•æäº¤é¡µçš„URLï¼ˆå»æ‰å‚æ•°ï¼‰
                submit_url = discovered_submit['url'].split('?')[0]
                print(f"âš ï¸  æœªæ‰¾åˆ°ç™»å½•é¡µé¢ï¼Œä½¿ç”¨ç™»å½•æäº¤é¡µ: {submit_url}")
                return submit_url

        return None

    def _discover_login_submit_by_behavior(self, domain: str) -> Optional[Dict]:
        """é€šè¿‡è¡Œä¸ºç‰¹å¾å‘ç°ç™»å½•æäº¤é¡µï¼ˆç»•è¿‡ç‰¹å¾åº“é™åˆ¶ï¼‰

        Args:
            domain: ç›®æ ‡åŸŸå

        Returns:
            Optional[Dict]: å‘ç°çš„ç™»å½•æäº¤é¡µä¿¡æ¯
        """
        if not hasattr(self, 'flow_data_map'):
            return None

        candidates = []

        # ğŸ¯ éå†æ‰€æœ‰æµæ•°æ®ï¼Œå¯»æ‰¾ç™»å½•æäº¤çš„è¡Œä¸ºç‰¹å¾
        for url, flow_data in self.flow_data_map.items():
            # å¿…é¡»æ˜¯åŒåŸŸå
            if domain not in url:
                continue

            # ğŸ¯ æ ¸å¿ƒç®—æ³•ï¼šPOST + è®¤è¯å­—æ®µ = ç™»å½•æäº¤
            method = flow_data.get('method', '').upper()
            if method != 'POST':
                continue

            # æ£€æŸ¥è¯·æ±‚ä½“æ˜¯å¦åŒ…å«è®¤è¯å­—æ®µ
            request_body = flow_data.get('request_body', '')
            if not request_body:
                continue

            # å¤„ç†bytesç±»å‹
            if isinstance(request_body, bytes):
                try:
                    request_body = request_body.decode('utf-8', errors='ignore')
                except:
                    request_body = str(request_body)

            request_body_lower = request_body.lower()

            # ğŸ¯ æ£€æµ‹è®¤è¯å­—æ®µï¼ˆæ›´å…¨é¢çš„å…³é”®å­—ï¼‰
            auth_indicators = [
                'loginid', 'userid', 'username', 'user', 'login',
                'password', 'passwd', 'pwd', 'pass',
                'vercode', 'captcha', 'verify'
            ]

            auth_field_count = 0
            for indicator in auth_indicators:
                if indicator in request_body_lower:
                    auth_field_count += 1

            # è‡³å°‘åŒ…å«2ä¸ªè®¤è¯ç›¸å…³å­—æ®µæ‰è®¤ä¸ºæ˜¯ç™»å½•æäº¤
            if auth_field_count >= 2:
                score = self._score_login_submit_api(url, flow_data)
                candidates.append({
                    'url': url,
                    'score': score,
                    'auth_field_count': auth_field_count,
                    'flow_data': flow_data
                })
                print(f"ğŸ” å‘ç°ç™»å½•æäº¤å€™é€‰: {url} (è®¤è¯å­—æ®µ: {auth_field_count}, è¯„åˆ†: {score})")

        if candidates:
            # é€‰æ‹©è¯„åˆ†æœ€é«˜çš„å€™é€‰
            best_candidate = max(candidates, key=lambda x: x['score'])
            return best_candidate

        return None

    def _score_login_url(self, url: str) -> int:
        """ä¸ºç™»å½•URLæ‰“åˆ†ï¼Œç”¨äºé€‰æ‹©æœ€ä½³å€™é€‰

        Args:
            url: ç™»å½•URL

        Returns:
            int: å¾—åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        """
        score = 0
        url_lower = url.lower()

        # æ˜ç¡®çš„ç™»å½•å…³é”®å­—å¾—åˆ†æ›´é«˜
        if 'logon' in url_lower:
            score += 10
        elif 'login' in url_lower:
            score += 8
        elif 'lgn' in url_lower:  # ä¸­é“¶é¦™æ¸¯çš„ç¼©å†™
            score += 9
        elif 'signin' in url_lower:
            score += 6
        elif 'auth' in url_lower:
            score += 4
        elif 'default' in url_lower and any(x in url_lower for x in ['lgn', 'login']):
            score += 7  # lgn.default.do è¿™ç§æ¨¡å¼

        # ç‰¹å®šçš„ç™»å½•æ–‡ä»¶æ‰©å±•å
        if url_lower.endswith('.do'):  # ä¸­é“¶é¦™æ¸¯ä½¿ç”¨çš„Strutsæ¡†æ¶
            score += 5
        elif 'servlet' in url_lower:  # æ°¸éš†é“¶è¡Œä½¿ç”¨çš„Servlet
            score += 5
        elif url_lower.endswith('.jsp'):  # JSPé¡µé¢
            score += 3

        # è·¯å¾„ç‰¹å¾
        if '/lgn/' in url_lower or '/login/' in url_lower:
            score += 4

        # è·¯å¾„è¶ŠçŸ­é€šå¸¸è¶Šæ˜¯ä¸»è¦ç™»å½•å…¥å£
        path_segments = url.split('/')
        if len(path_segments) <= 5:
            score += 3
        elif len(path_segments) <= 3:
            score += 5  # éå¸¸çŸ­çš„è·¯å¾„ï¼Œå¯èƒ½æ˜¯ä¸»å…¥å£

        # é¿å…æ˜æ˜¾çš„éç™»å½•URL
        if any(exclude in url_lower for exclude in ['overview', 'balance', 'account', 'transaction']):
            score -= 5

        return score

    def _score_login_submit_api(self, url: str, flow_data: Dict[str, Any]) -> int:
        """è¯„åˆ†ç™»å½•æäº¤é¡µAPIï¼ˆç¬¬ä¸€ä¼˜å…ˆçº§ï¼‰

        Args:
            url: API URL
            flow_data: æµæ•°æ®

        Returns:
            int: ç™»å½•æäº¤é¡µè¯„åˆ†
        """
        score = 0
        url_lower = url.lower()

        # ğŸ¯ URLå…³é”®å­—è¯„åˆ†
        submit_keywords = ['login', 'logon', 'authenticate', 'signin', 'submit', 'dologin']
        for keyword in submit_keywords:
            if keyword in url_lower:
                score += 10
                break

        # ğŸ¯ HTTPæ–¹æ³•è¯„åˆ†ï¼ˆPOSTé€šå¸¸æ˜¯æäº¤ï¼‰
        method = flow_data.get('method', '').upper()
        if method == 'POST':
            score += 15

        # ğŸ¯ è¯·æ±‚ä½“åˆ†æï¼ˆåŒ…å«è®¤è¯ä¿¡æ¯ï¼‰
        request_body = flow_data.get('request_body', '')
        if request_body:
            if isinstance(request_body, bytes):
                try:
                    request_body = request_body.decode('utf-8', errors='ignore')
                except:
                    request_body = str(request_body)

            auth_fields = ['username', 'password', 'userid', 'pwd', 'user', 'pass']
            for field in auth_fields:
                if field in request_body.lower():
                    score += 20  # åŒ…å«è®¤è¯å­—æ®µï¼Œå¾ˆå¯èƒ½æ˜¯ç™»å½•æäº¤
                    break

        # ğŸ¯ å“åº”å¤´åˆ†æï¼ˆè®¾ç½®è®¤è¯ä¿¡æ¯ï¼‰
        response_headers = flow_data.get('response_headers', {})
        set_cookie = response_headers.get('Set-Cookie', '')
        if isinstance(set_cookie, list):
            set_cookie = '; '.join(set_cookie) if set_cookie else ''

        if set_cookie:
            auth_cookie_keywords = ['session', 'jsessionid', 'token', 'auth']
            for keyword in auth_cookie_keywords:
                if keyword.lower() in set_cookie.lower():
                    score += 15
                    break

        # ğŸ¯ å“åº”å†…å®¹åˆ†æï¼ˆç®€çŸ­å…³é”®å­—ï¼‰
        response_body = flow_data.get('response_body', '')
        if response_body:
            if isinstance(response_body, bytes):
                try:
                    response_body = response_body.decode('utf-8', errors='ignore')
                except:
                    response_body = str(response_body)

            response_lower = response_body.lower()
            auth_response_keywords = ['token', 'authority', 'code', 'session', 'redirect', 'success']
            for keyword in auth_response_keywords:
                if keyword in response_lower:
                    score += 8

        # ğŸ¯ çŠ¶æ€ç åˆ†æ
        status_code = flow_data.get('status_code', 0)
        if status_code in [302, 301]:  # é‡å®šå‘ï¼Œå¯èƒ½æ˜¯ç™»å½•æˆåŠŸ
            score += 10
        elif status_code == 200:
            score += 5

        return score

    def _score_login_api_by_flow_data(self, url: str, flow_data: Dict[str, Any]) -> int:
        """åŸºäºå®é™…çš„è¯·æ±‚/åº”ç­”æµæ•°æ®æ¥è¯„åˆ†ç™»å½•API

        Args:
            url: API URL
            flow_data: æµæ•°æ®ï¼ˆåŒ…å«è¯·æ±‚å’Œåº”ç­”ä¿¡æ¯ï¼‰

        Returns:
            int: å¾—åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        """
        score = 0

        # åŸºç¡€URLè¯„åˆ†
        url_lower = url.lower()
        if 'lgn' in url_lower:
            score += 5
        elif 'login' in url_lower or 'logon' in url_lower:
            score += 3

        # ğŸ¯ è¯·æ±‚ç‰¹å¾åˆ†æ
        method = flow_data.get('method', '').upper()
        request_headers = flow_data.get('request_headers', {})
        request_body = flow_data.get('request_body', '')

        # POSTæ–¹æ³•é€šå¸¸æ˜¯ç™»å½•æäº¤
        if method == 'POST':
            score += 10
        elif method == 'GET':
            score += 2  # å¯èƒ½æ˜¯ç™»å½•é¡µé¢

        # è¯·æ±‚ä½“ç‰¹å¾
        if request_body:
            # å¤„ç†bytesç±»å‹çš„è¯·æ±‚ä½“
            if isinstance(request_body, bytes):
                try:
                    request_body = request_body.decode('utf-8', errors='ignore')
                except:
                    request_body = str(request_body)
            request_body_lower = str(request_body).lower()

            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç™»å½•ç›¸å…³å­—æ®µ
            login_fields = ['username', 'password', 'userid', 'pwd', 'user', 'pass', 'account']
            for field in login_fields:
                if field in request_body_lower:
                    score += 8
                    break

        # Content-Typeæ£€æŸ¥
        content_type = request_headers.get('Content-Type', '')
        if isinstance(content_type, list):
            content_type = content_type[0] if content_type else ''
        content_type = str(content_type).lower()

        if 'application/x-www-form-urlencoded' in content_type:
            score += 5  # è¡¨å•æäº¤
        elif 'application/json' in content_type:
            score += 3  # JSONæäº¤

        # ğŸ¯ åº”ç­”ç‰¹å¾åˆ†æ
        response_headers = flow_data.get('response_headers', {})
        response_body = flow_data.get('response_body', '')
        status_code = flow_data.get('status_code', 0)

        # çŠ¶æ€ç åˆ†æ
        if status_code == 302 or status_code == 301:
            score += 8  # é‡å®šå‘ï¼Œå¯èƒ½æ˜¯ç™»å½•æˆåŠŸåè·³è½¬
        elif status_code == 200:
            score += 5  # æ­£å¸¸å“åº”
        elif status_code >= 400:
            score -= 5  # é”™è¯¯å“åº”ï¼Œå¯èƒ½ä¸æ˜¯çœŸæ­£çš„ç™»å½•API

        # æ£€æŸ¥Set-Cookieå¤´ï¼ˆç™»å½•é€šå¸¸ä¼šè®¾ç½®session cookieï¼‰
        set_cookie = response_headers.get('Set-Cookie', '')
        if isinstance(set_cookie, list):
            set_cookie = '; '.join(set_cookie) if set_cookie else ''
        set_cookie = str(set_cookie)

        if set_cookie:
            cookie_lower = set_cookie.lower()
            if any(keyword in cookie_lower for keyword in ['session', 'jsessionid', 'token', 'auth']):
                score += 10
            else:
                score += 3  # ä»»ä½•cookieéƒ½å¯èƒ½æ˜¯ç™»å½•ç›¸å…³

        # æ£€æŸ¥Locationå¤´ï¼ˆé‡å®šå‘ç›®æ ‡ï¼‰
        location = response_headers.get('Location', '')
        if isinstance(location, list):
            location = location[0] if location else ''
        location = str(location)

        if location:
            location_lower = location.lower()
            if any(keyword in location_lower for keyword in ['main', 'home', 'index', 'welcome', 'dashboard']):
                score += 8  # é‡å®šå‘åˆ°ä¸»é¡µï¼Œå¾ˆå¯èƒ½æ˜¯ç™»å½•æˆåŠŸ

        # ğŸ¯ åº”ç­”å†…å®¹åˆ†æ
        if response_body:
            # å¤„ç†bytesç±»å‹çš„å“åº”ä½“
            if isinstance(response_body, bytes):
                try:
                    response_body = response_body.decode('utf-8', errors='ignore')
                except:
                    response_body = str(response_body)
            response_body_lower = str(response_body).lower()

            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç™»å½•æˆåŠŸçš„æ ‡è¯†
            success_indicators = ['welcome', 'dashboard', 'logout', 'account', 'balance']
            for indicator in success_indicators:
                if indicator in response_body_lower:
                    score += 5
                    break

            # æ£€æŸ¥æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯ï¼ˆå¯èƒ½æ˜¯ç™»å½•å¤±è´¥ï¼‰
            error_indicators = ['error', 'invalid', 'incorrect', 'failed', 'wrong']
            for indicator in error_indicators:
                if indicator in response_body_lower:
                    score += 3  # æœ‰é”™è¯¯ä¿¡æ¯ä¹Ÿè¯´æ˜æ˜¯ç™»å½•API
                    break

        print(f"ğŸ” ç™»å½•APIè¯„åˆ† {url}: {score}åˆ†")
        return score

    def _find_corresponding_login_page(self, domain: str, submit_api: Dict) -> Optional[str]:
        """åŸºäºç™»å½•æäº¤é¡µæ‰¾åˆ°å¯¹åº”çš„ç™»å½•é¡µé¢

        Args:
            domain: ç›®æ ‡åŸŸå
            submit_api: ç™»å½•æäº¤é¡µä¿¡æ¯

        Returns:
            Optional[str]: æ‰¾åˆ°çš„ç™»å½•é¡µé¢URL
        """
        if not hasattr(self, 'analysis_data') or not self.analysis_data:
            return None

        extracted_data = self.analysis_data.get('extracted_data', [])
        submit_url = submit_api['url']

        # ğŸ¯ æŸ¥æ‰¾å€™é€‰çš„ç™»å½•é¡µé¢
        page_candidates = []

        for api_data in extracted_data:
            api_url = api_data.get('url', '')

            # å¿…é¡»æ˜¯åŒåŸŸå
            if domain not in api_url:
                continue

            # ğŸ¯ ç®€å•çš„ç™»å½•é¡µé¢å…³é”®å­—åŒ¹é…ï¼ˆå°½é‡çŸ­ï¼Œæé«˜æˆåŠŸç‡ï¼‰
            url_lower = api_url.lower()
            page_keywords = ['login', 'logon', 'signin']

            has_page_keyword = any(keyword in url_lower for keyword in page_keywords)
            if not has_page_keyword:
                continue

            # ğŸ¯ æ’é™¤æ˜æ˜¾çš„æäº¤é¡µé¢
            if any(exclude in url_lower for exclude in ['servlet', 'submit', 'authenticate']):
                continue

            # ğŸ¯ ä¼˜å…ˆé€‰æ‹©é¡µé¢æ–‡ä»¶
            page_score = 0
            if any(ext in url_lower for ext in ['.jsp', '.html', '.htm', '.php']):
                page_score += 10
            elif url_lower.endswith('/login') or url_lower.endswith('/logon'):
                page_score += 8

            # ğŸ¯ URLç›¸ä¼¼åº¦è¯„åˆ†
            similarity_score = self._calculate_url_similarity(submit_url, api_url)
            page_score += similarity_score

            if page_score > 5:  # åŸºæœ¬é—¨æ§›
                page_candidates.append({
                    'url': api_url,
                    'score': page_score
                })

        if page_candidates:
            # é€‰æ‹©è¯„åˆ†æœ€é«˜çš„ç™»å½•é¡µé¢
            best_page = max(page_candidates, key=lambda x: x['score'])
            return best_page['url']

        return None

    def _calculate_url_similarity(self, url1: str, url2: str) -> int:
        """è®¡ç®—ä¸¤ä¸ªURLçš„ç›¸ä¼¼åº¦è¯„åˆ†

        Args:
            url1: URL1
            url2: URL2

        Returns:
            int: ç›¸ä¼¼åº¦è¯„åˆ†
        """
        from urllib.parse import urlparse

        parsed1 = urlparse(url1)
        parsed2 = urlparse(url2)

        score = 0

        # åŸŸåå¿…é¡»ç›¸åŒï¼ˆå·²åœ¨ä¸Šå±‚æ£€æŸ¥ï¼‰
        if parsed1.netloc == parsed2.netloc:
            score += 5

        # è·¯å¾„ç›¸ä¼¼åº¦
        path1_parts = parsed1.path.strip('/').split('/')
        path2_parts = parsed2.path.strip('/').split('/')

        # å…±åŒçš„è·¯å¾„æ®µ
        common_parts = set(path1_parts) & set(path2_parts)
        if common_parts:
            score += len(common_parts) * 2

        # è·¯å¾„é•¿åº¦ç›¸ä¼¼
        if abs(len(path1_parts) - len(path2_parts)) <= 1:
            score += 3

        return score

    def extract_geo_location(self, flow_data: Dict[str, Any]) -> str:
        """æå–åœ°ç†ä½ç½®"""
        # æ ¹æ®åŸŸåæ¨æ–­åœ°ç†ä½ç½®
        url = flow_data['url']

        if '.hk' in url or 'hong' in url.lower():
            return "HK"
        elif '.cn' in url or 'china' in url.lower():
            return "CN"
        elif '.in' in url or 'india' in url.lower():
            return "IN"
        else:
            return "US"  # é»˜è®¤

    def classify_api_type(self, url: str, response_content: str) -> str:
        """åˆ†ç±»APIç±»å‹"""
        url_lower = url.lower()
        content_lower = response_content.lower()

        if 'account' in url_lower or 'acc' in url_lower:
            return "account_management"
        elif 'transaction' in url_lower or 'txn' in url_lower:
            return "transaction_history"
        elif 'balance' in url_lower or 'balance' in content_lower:
            return "balance_inquiry"
        elif 'login' in url_lower or 'logon' in url_lower:
            return "authentication"
        else:
            return "general_banking"

    def extract_response_variables(self, response_matches: List[Dict], response_redactions: List[Dict]) -> List[str]:
        """æå–å“åº”å˜é‡"""
        variables = set()

        # ä»responseMatchesä¸­æå–å˜é‡
        for match in response_matches:
            value = match.get('value', '')
            # æŸ¥æ‰¾{{variable}}æ¨¡å¼
            var_matches = re.findall(r'\{\{(\w+)\}\}', value)
            variables.update(var_matches)

        # ä»responseRedactionsä¸­æå–å˜é‡
        for redaction in response_redactions:
            json_path = redaction.get('jsonPath', '')
            regex = redaction.get('regex', '')

            # æŸ¥æ‰¾å˜é‡æ¨¡å¼
            var_matches = re.findall(r'\{\{(\w+)\}\}', json_path + regex)
            variables.update(var_matches)

        return list(variables)

    def _detect_content_type(self, content: str) -> str:
        """æ£€æµ‹å†…å®¹ç±»å‹

        Args:
            content: å“åº”å†…å®¹

        Returns:
            str: å†…å®¹ç±»å‹ ('json', 'html', 'text')
        """
        if not content:
            return 'text'

        content_stripped = content.strip()

        # æ£€æŸ¥JSONæ ¼å¼
        if (content_stripped.startswith('{') and content_stripped.endswith('}')) or \
           (content_stripped.startswith('[') and content_stripped.endswith(']')):
            try:
                json.loads(content_stripped)
                return 'json'
            except json.JSONDecodeError:
                pass

        # æ£€æŸ¥HTMLæ ¼å¼
        if content_stripped.startswith('<') or \
           '<html' in content.lower() or \
           '<body' in content.lower() or \
           '<!doctype html' in content.lower():
            return 'html'

        # é»˜è®¤ä¸ºæ–‡æœ¬
        return 'text'

    def _extract_actual_currencies(self, content: str) -> List[str]:
        """ä»å“åº”å†…å®¹ä¸­æå–å®é™…å­˜åœ¨çš„è´§å¸ä»£ç 

        Args:
            content: å“åº”å†…å®¹

        Returns:
            List[str]: å®é™…å­˜åœ¨çš„è´§å¸ä»£ç åˆ—è¡¨
        """
        import re

        currencies = ['HKD', 'USD', 'CNY', 'EUR', 'GBP', 'JPY', 'AUD', 'CAD', 'SGD']
        found_currencies = []

        for currency in currencies:
            # æ£€æŸ¥è´§å¸ä»£ç æ˜¯å¦åœ¨æœ‰æ„ä¹‰çš„ä¸Šä¸‹æ–‡ä¸­å‡ºç°
            currency_patterns = [
                rf'<td[^>]*>{currency}</td>',  # è¡¨æ ¼å•å…ƒæ ¼ä¸­
                rf'<span[^>]*>{currency}</span>',  # spanæ ‡ç­¾ä¸­
                rf'{currency}\s*[0-9,]+\.?\d*',  # è´§å¸ä»£ç åè·Ÿæ•°å­—
                rf'[0-9,]+\.?\d*\s*{currency}',  # æ•°å­—åè·Ÿè´§å¸ä»£ç 
            ]

            for pattern in currency_patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    found_currencies.append(currency)
                    break

        return found_currencies

    def _extract_actual_amounts(self, content: str) -> List[str]:
        """ä»å“åº”å†…å®¹ä¸­æå–å®é™…å­˜åœ¨çš„é‡‘é¢æ ¼å¼

        Args:
            content: å“åº”å†…å®¹

        Returns:
            List[str]: å®é™…å­˜åœ¨çš„é‡‘é¢æ ¼å¼åˆ—è¡¨
        """
        import re

        amount_patterns = [
            r'\$[0-9,]+\.[0-9]{2}',  # $1,234.56
            r'[0-9,]+\.[0-9]{2}\s*(HKD|USD|CNY|EUR|GBP|JPY)',  # 1,234.56 HKD
            r'(HKD|USD|CNY|EUR|GBP|JPY)\s*[0-9,]+\.[0-9]{2}',  # HKD 1,234.56
        ]

        found_amounts = []
        for pattern in amount_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                found_amounts.extend(matches[:3])  # æœ€å¤šè®°å½•3ä¸ªç¤ºä¾‹

        return found_amounts

    def _extract_actual_accounts(self, content: str) -> List[str]:
        """ä»å“åº”å†…å®¹ä¸­æå–å®é™…å­˜åœ¨çš„è´¦æˆ·å·ç 

        Args:
            content: å“åº”å†…å®¹

        Returns:
            List[str]: å®é™…å­˜åœ¨çš„è´¦æˆ·å·ç åˆ—è¡¨
        """
        import re

        account_patterns = [
            r'\b\d{8,20}\b',  # 8-20ä½æ•°å­—
            r'\b[A-Z]{2,4}\d{8,16}\b',  # å­—æ¯+æ•°å­—æ ¼å¼
        ]

        found_accounts = []
        for pattern in account_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                # æ’é™¤æ˜æ˜¾çš„æ—¥æœŸæ ¼å¼
                if not (match.startswith('20') and len(match) == 8):  # æ’é™¤20140715è¿™æ ·çš„æ—¥æœŸ
                    if not (match.startswith('19') and len(match) == 8):  # æ’é™¤19xxå¹´ä»½
                        found_accounts.append(match)

        return found_accounts[:5]  # æœ€å¤šè¿”å›5ä¸ª

    def _deduplicate_response_matches(self, response_matches: List[Dict]) -> List[Dict]:
        """å»é™¤é‡å¤çš„responseMatchesè§„åˆ™

        Args:
            response_matches: åŸå§‹çš„responseMatchesåˆ—è¡¨

        Returns:
            List[Dict]: å»é‡åçš„responseMatchesåˆ—è¡¨
        """
        seen = set()
        deduplicated = []

        for match in response_matches:
            # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦ï¼šåŸºäºvalueå’Œtype
            identifier = (match['value'], match['type'])

            if identifier not in seen:
                seen.add(identifier)
                # é‡æ–°åˆ†é…orderï¼Œç¡®ä¿è¿ç»­
                match['order'] = len(deduplicated) + 1
                deduplicated.append(match)
            else:
                print(f"ğŸ”„ å»é™¤é‡å¤çš„responseMatch: {match['description']}")

        return deduplicated

    def _filter_response_matches_by_quality(self, response_matches: List[Dict], response_content: str, threshold: float = 6.5) -> List[Dict]:
        """æŒ‰è´¨é‡åˆ†æ•°è¿‡æ»¤ responseMatchesï¼Œä»…ä¿ç•™åˆ†æ•°>=é˜ˆå€¼çš„

        è¯„åˆ†ç»´åº¦ï¼ˆæ€»åˆ† 10ï¼‰ï¼š
        - å‘½ä¸­éªŒè¯ï¼ˆå¿…éœ€ï¼‰ï¼š3 åˆ†ï¼ˆcontains/regex çœŸæ­£å‘½ä¸­æ–‡æœ¬ï¼‰
        - ç¨³å®šæ€§ï¼š0-3 åˆ†ï¼ˆå‘½åæ•è·ç»„/å­—æ®µåå­˜åœ¨/å¸ç§ä¸é‡‘é¢åŒæ—¶å‡ºç°ç­‰æé«˜ç¨³å®šæ€§ï¼‰
        - å™ªå£°æƒ©ç½šï¼š-0~2 åˆ†ï¼ˆå‘½ä¸­ HTML æ³¨é‡Š/script/style/console ç­‰å™ªå£°åŒºåŸŸæ‰£åˆ†ï¼‰
        - ä¸Šä¸‹æ–‡çº¿ç´¢ï¼š0-2 åˆ†ï¼ˆé™„è¿‘ 120 å­—ç¬¦å†…å‡ºç° currency/amount/balance/account ç­‰é‡‘èå…³é”®è¯åŠ åˆ†ï¼‰

        Args:
            response_matches: åŸå§‹åŒ¹é…è§„åˆ™
            response_content: å“åº”æ–‡æœ¬ï¼ˆæœªå‹ç¼©ï¼‰
            threshold: è¿‡æ»¤é˜ˆå€¼

        Returns:
            è¿‡æ»¤åçš„åŒ¹é…è§„åˆ™
        """
        import re

        def is_hit(rule: Dict) -> bool:
            value = rule.get('value', '') or ''
            rtype = (rule.get('type') or 'contains').lower()
            invert = bool(rule.get('invert'))
            try:
                if rtype == 'regex':
                    ok = re.search(value, response_content) is not None
                else:
                    ok = value.strip('"') in response_content
                return (not invert and ok) or (invert and not ok)
            except Exception:
                return False

        def noise_penalty(span: tuple[int, int]) -> float:
            # ç®€æ˜“åŒºåŸŸåˆ¤æ–­ï¼šå‘½ä¸­åŒºé—´å‰åå„å– 200 å­—ç¬¦ï¼Œåˆ¤æ–­æ˜¯å¦å¤„äº script/style/æ³¨é‡Š
            start, end = span
            s = max(0, start - 200)
            e = min(len(response_content), end + 200)
            ctx = response_content[s:e]
            penalty = 0.0
            if re.search(r'<!--.*?-->', ctx, flags=re.S):
                penalty += 1.0
            if re.search(r'<script[^>]*>.*?</script>', ctx, flags=re.S|re.I):
                penalty += 1.0
            if re.search(r'<style[^>]*>.*?</style>', ctx, flags=re.S|re.I):
                penalty += 0.5
            return penalty

        def context_bonus(span: tuple[int, int]) -> float:
            start, end = span
            s = max(0, start - 120)
            e = min(len(response_content), end + 120)
            ctx = response_content[s:e].lower()
            bonus = 0.0
            for kw in ['currency', 'amount', 'balance', 'available', 'current', 'account', 'è´¦æˆ·', 'ä½™é¢', 'é‡‘é¢', 'å¸ç§']:
                if kw in ctx:
                    bonus += 0.4
            return min(bonus, 2.0)

        def find_span(rule: Dict) -> tuple[int, int] | None:
            value = rule.get('value', '') or ''
            rtype = (rule.get('type') or 'contains').lower()
            try:
                if rtype == 'regex':
                    m = re.search(value, response_content)
                    return (m.start(), m.end()) if m else None
                else:
                    val = value.strip('"')
                    idx = response_content.find(val)
                    return (idx, idx + len(val)) if idx >= 0 else None
            except Exception:
                return None

        filtered: List[Dict] = []
        for rule in response_matches:
            # å‘½ä¸­å¿…éœ€
            if not is_hit(rule):
                continue

            score = 3.0  # å‘½ä¸­åŸºç¡€åˆ†

            # ç¨³å®šæ€§ï¼šå‘½åæ•è·ç»„/å­—æ®µå/å¸ç§+é‡‘é¢å…±ç°
            value = rule.get('value', '') or ''
            rtype = (rule.get('type') or 'contains').lower()
            if rtype == 'regex' and re.search(r'\?P<\w+>', value):
                score += 1.5
            if any(key in value.lower() for key in ['currency', 'amount', 'balance', 'account', 'userName', 'account_number']):
                score += 1.0

            # æŸ¥æ‰¾å‘½ä¸­åŒºé—´
            span = find_span(rule)
            if span:
                # å™ªå£°æƒ©ç½š
                score -= noise_penalty(span)
                # ä¸Šä¸‹æ–‡çº¿ç´¢
                score += context_bonus(span)

            # æˆªæ–­åˆ° [0,10]
            score = max(0.0, min(10.0, score))

            if score >= threshold:
                filtered.append(rule)

        return filtered

    def _deduplicate_response_redactions(self, response_redactions: List[Dict]) -> List[Dict]:
        """å»é™¤é‡å¤çš„responseRedactionsè§„åˆ™

        Args:
            response_redactions: åŸå§‹çš„responseRedactionsåˆ—è¡¨

        Returns:
            List[Dict]: å»é‡åçš„responseRedactionsåˆ—è¡¨
        """
        seen = set()
        deduplicated = []

        for redaction in response_redactions:
            # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦ï¼šåŸºäºregexå’ŒjsonPath
            identifier = (redaction.get('regex', ''), redaction.get('jsonPath', ''))

            if identifier not in seen:
                seen.add(identifier)
                # é‡æ–°åˆ†é…orderï¼Œç¡®ä¿è¿ç»­
                redaction['order'] = len(deduplicated) + 1
                deduplicated.append(redaction)
            else:
                print(f"ğŸ”„ å»é™¤é‡å¤çš„responseRedaction: regex={redaction.get('regex', '')[:50]}...")

        return deduplicated

    def _is_sensitive_field(self, field_name: str) -> bool:
        """åˆ¤æ–­å­—æ®µæ˜¯å¦ä¸ºæ•æ„Ÿå­—æ®µï¼Œéœ€è¦å“ˆå¸Œå¤„ç†

        Args:
            field_name: å­—æ®µå

        Returns:
            bool: æ˜¯å¦ä¸ºæ•æ„Ÿå­—æ®µ
        """
        sensitive_keywords = [
            'account', 'password', 'token', 'id', 'number', 'card',
            'phone', 'email', 'name', 'address', 'è´¦å·', 'å¯†ç ', 'å§“å'
        ]
        field_lower = field_name.lower()
        return any(keyword in field_lower for keyword in sensitive_keywords)

    def _is_html_response(self, matched_patterns: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºHTMLå“åº”

        Args:
            matched_patterns: åŒ¹é…çš„æ¨¡å¼åˆ—è¡¨

        Returns:
            bool: æ˜¯å¦ä¸ºHTMLå“åº”
        """
        return any("html_content:" in pattern for pattern in matched_patterns)

    def _is_json_response(self, matched_patterns: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºJSONå“åº”

        Args:
            matched_patterns: åŒ¹é…çš„æ¨¡å¼åˆ—è¡¨

        Returns:
            bool: æ˜¯å¦ä¸ºJSONå“åº”
        """
        return any("json_content:" in pattern for pattern in matched_patterns)

    def _get_user_name_regex(self, matched_patterns: List[str]) -> str:
        """æ ¹æ®å“åº”ç±»å‹ç”Ÿæˆç”¨æˆ·å§“åçš„æ­£åˆ™è¡¨è¾¾å¼

        Args:
            matched_patterns: åŒ¹é…çš„æ¨¡å¼åˆ—è¡¨

        Returns:
            str: é€‚åˆçš„æ­£åˆ™è¡¨è¾¾å¼
        """
        if self._is_json_response(matched_patterns):
            # JSONæ ¼å¼ï¼šåŒ¹é…JSONå­—æ®µ
            return "\"(?:user_?name|customer_?name|holder_?name|full_?name)\":\\s*\"(?P<user_name>[^\"]+)\""
        elif self._is_html_response(matched_patterns):
            # HTMLæ ¼å¼ï¼šæ›´ç²¾ç¡®çš„åŒ¹é…ï¼Œé¿å…åŒ¹é…HTMLæ ‡ç­¾å’Œæ— å…³æ–‡æœ¬
            # åŒ¹é…è¡¨æ ¼å•å…ƒæ ¼æˆ–ç‰¹å®šä¸Šä¸‹æ–‡ä¸­çš„å§“å
            return "(?:å§“å|å®¢æˆ·|æŒæœ‰äºº|ç”¨æˆ·)[^>]*>\\s*(?P<user_name>[\\u4e00-\\u9fff]{2,4}|[A-Z][a-z]+\\s+[A-Z][a-z]+)"
        else:
            # é»˜è®¤ï¼šå°è¯•JSONæ ¼å¼
            return "\"(?:user_?name|customer_?name|holder_?name|full_?name)\":\\s*\"(?P<user_name>[^\"]+)\""

    def _get_name_component_regex(self, matched_patterns: List[str]) -> str:
        """æ ¹æ®å“åº”ç±»å‹ç”Ÿæˆå§“åç»„ä»¶çš„æ­£åˆ™è¡¨è¾¾å¼

        Args:
            matched_patterns: åŒ¹é…çš„æ¨¡å¼åˆ—è¡¨

        Returns:
            str: é€‚åˆçš„æ­£åˆ™è¡¨è¾¾å¼
        """
        if self._is_json_response(matched_patterns):
            # JSONæ ¼å¼ï¼šåŒ¹é…JSONå­—æ®µ
            return "\"(?:first_?name|last_?name|display_?name)\":\\s*\"(?P<name_component>[^\"]+)\""
        elif self._is_html_response(matched_patterns):
            # HTMLæ ¼å¼ï¼šæ›´ç²¾ç¡®çš„åŒ¹é…ï¼Œåœ¨ç‰¹å®šä¸Šä¸‹æ–‡ä¸­æŸ¥æ‰¾å§“åç»„ä»¶
            # åŒ¹é…è¡¨æ ¼æˆ–è¡¨å•ä¸­çš„å§“åå­—æ®µ
            return "(?:å|å§“)[^>]*>\\s*(?P<name_component>[\\u4e00-\\u9fff]{1,3}|[A-Z][a-z]+)"
        else:
            # é»˜è®¤ï¼šå°è¯•JSONæ ¼å¼
            return "\"(?:first_?name|last_?name|display_?name)\":\\s*\"(?P<name_component>[^\"]+)\""

    def _generate_priority_balance_rules(self, matched_patterns: List[str], response_content: str) -> List[Dict]:
        """ğŸ¯ ç”Ÿæˆä¼˜å…ˆçº§ä½™é¢åŒ¹é…è§„åˆ™ï¼šä»ä¸¥æ ¼åˆ°å®½æ¾

        Args:
            matched_patterns: åŒ¹é…çš„æ¨¡å¼åˆ—è¡¨
            response_content: å“åº”å†…å®¹

        Returns:
            List[Dict]: æŒ‰ä¼˜å…ˆçº§æ’åºçš„åŒ¹é…è§„åˆ™åˆ—è¡¨
        """
        rules = []

        if self._is_html_response(matched_patterns):
            # ğŸ¯ HTMLå“åº”ï¼šä¼˜å…ˆçº§åŒ¹é…è§„åˆ™

            # ä¼˜å…ˆçº§1ï¼šä¸¥æ ¼è§„åˆ™ï¼ˆæ—©æœŸå¤šæ¡ç‰ˆæœ¬ï¼‰ï¼šå¸ç§åœ¨å‰ä¸”åŒè¡Œé‚»è¿‘
            strict_rules = [
                {
                    "regex": "HKD.*?(?P<hkd_balance>\\d{1,3}(?:,\\d{3})*\\.\\d{2})",
                    "description": "ä¸¥æ ¼è§„åˆ™ï¼šHKDç²¾ç¡®åŒ¹é…çº¯å‡€é‡‘é¢",
                    "priority": 1,
                    "isOptional": True
                },
                {
                    "regex": "USD.*?(?P<usd_balance>\\d{1,3}(?:,\\d{3})*\\.\\d{2})",
                    "description": "ä¸¥æ ¼è§„åˆ™ï¼šUSDç²¾ç¡®åŒ¹é…çº¯å‡€é‡‘é¢",
                    "priority": 1,
                    "isOptional": True
                },
                {
                    "regex": "CNY.*?(?P<cny_balance>\\d{1,3}(?:,\\d{3})*\\.\\d{2})",
                    "description": "ä¸¥æ ¼è§„åˆ™ï¼šCNYç²¾ç¡®åŒ¹é…çº¯å‡€é‡‘é¢",
                    "priority": 1,
                    "isOptional": True
                }
            ]

            # ä¼˜å…ˆçº§2ï¼šå®½æ¾è§„åˆ™ - åŒ…å«HTMLç»“æ„çš„åŒ¹é…ï¼ˆé™çº§ä½¿ç”¨ï¼‰
            loose_rules = [
                {
                    "regex": "(?P<hkd_balance>HKD.*?>([\\d,]+\\.\\d{2}))",
                    "description": "å®½æ¾è§„åˆ™ï¼šHKDåŒ…å«HTMLç»“æ„",
                    "priority": 2,
                    "isOptional": True
                },
                {
                    "regex": "(?P<usd_balance>USD.*?>([\\d,]+\\.\\d{2}))",
                    "description": "å®½æ¾è§„åˆ™ï¼šUSDåŒ…å«HTMLç»“æ„",
                    "priority": 2,
                    "isOptional": True
                },
                {
                    "regex": "(?P<cny_balance>CNY.*?>([\\d,]+\\.\\d{2}))",
                    "description": "å®½æ¾è§„åˆ™ï¼šCNYåŒ…å«HTMLç»“æ„",
                    "priority": 2,
                    "isOptional": True
                }
            ]

            # ğŸ¯ ä¼˜å…ˆçº§åŒ¹é…é€»è¾‘ï¼šä¸¥æ ¼è§„åˆ™ä¼˜å…ˆï¼ŒæˆåŠŸåˆ™è·³è¿‡å¯¹åº”çš„å®½æ¾è§„åˆ™
            print(f"ğŸ” DEBUG: æµ‹è¯•ä¸¥æ ¼è§„åˆ™ï¼Œå“åº”å†…å®¹é•¿åº¦: {len(response_content)}")
            print(f"ğŸ” DEBUG: å“åº”å†…å®¹å‰200å­—ç¬¦: {repr(response_content[:200])}")

            # ä¼˜å…ˆçº§1ï¼šæµ‹è¯•ä¸¥æ ¼è§„åˆ™ï¼ˆå‘½ä¸­å³è¿”å›ï¼Œä¸¥æ ¼â†’å®½æ¾ï¼ŒåŒ¹é…åˆ°å°±breakï¼‰
            for rule in strict_rules:
                print(f"ğŸ” DEBUG: æµ‹è¯•ä¸¥æ ¼è§„åˆ™: {rule['description']}")
                print(f"ğŸ” DEBUG: æ­£åˆ™è¡¨è¾¾å¼: {rule['regex']}")
                if self._test_regex_match(response_content, rule["regex"]):
                    print(f"âœ… ä¸¥æ ¼è§„åˆ™æœ‰æ•ˆ: {rule['description']} -> é‡‡ç”¨å¹¶ç»“æŸä¼˜å…ˆçº§åŒ¹é…")
                    return [rule]
                else:
                    print(f"âŒ ä¸¥æ ¼è§„åˆ™æ— æ•ˆ: {rule['description']}")

            # ä¼˜å…ˆçº§2ï¼šæµ‹è¯•å®½æ¾è§„åˆ™ï¼ˆå‘½ä¸­å³è¿”å›ï¼‰
            for rule in loose_rules:
                print(f"ğŸ” DEBUG: æµ‹è¯•å®½æ¾è§„åˆ™: {rule['description']}")
                print(f"ğŸ” DEBUG: æ­£åˆ™è¡¨è¾¾å¼: {rule['regex']}")
                if self._test_regex_match(response_content, rule["regex"]):
                    print(f"âš ï¸ å®½æ¾è§„åˆ™æœ‰æ•ˆ: {rule['description']} -> é‡‡ç”¨å¹¶ç»“æŸä¼˜å…ˆçº§åŒ¹é…")
                    return [rule]
                else:
                    print(f"âŒ å®½æ¾è§„åˆ™æ— æ•ˆ: {rule['description']}")

        else:
            # JSONå“åº”ï¼šä½¿ç”¨æ ‡å‡†è§„åˆ™
            rules.append({
                "regex": "\"balance\":\\s*(?P<balance>[0-9]+)",
                "description": "JSONä½™é¢æ ‡å‡†è§„åˆ™",
                "priority": 1,
                "isOptional": False
            })

            # è‹¥å‡æœªå‘½ä¸­ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼Œç”±ä¸Šå±‚æ¸…æ´—å†³å®šæ˜¯å¦é™çº§ä½¿ç”¨é€šç”¨è§„åˆ™
            return []

    def _test_regex_match(self, content: str, regex_pattern: str) -> bool:
        """æµ‹è¯•æ­£åˆ™è¡¨è¾¾å¼æ˜¯å¦èƒ½åŒ¹é…å†…å®¹"""
        try:
            import re
            # ğŸ¯ ä½¿ç”¨DOTALLæ ‡å¿—ï¼Œè®©.åŒ¹é…æ¢è¡Œç¬¦ï¼Œå¹¶æ·»åŠ è¯¦ç»†è°ƒè¯•
            match = re.search(regex_pattern, content, re.DOTALL)
            if match:
                print(f"âœ… æ­£åˆ™åŒ¹é…æˆåŠŸ: {regex_pattern}")
                print(f"   åŒ¹é…å†…å®¹: {match.group()[:100]}...")
                if hasattr(match, 'groupdict') and match.groupdict():
                    print(f"   å‘½åç»„: {match.groupdict()}")
                return True
            else:
                print(f"âŒ æ­£åˆ™åŒ¹é…å¤±è´¥: {regex_pattern}")
                return False
        except Exception as e:
            print(f"âŒ æ­£åˆ™è¡¨è¾¾å¼æµ‹è¯•å¤±è´¥: {regex_pattern}, é”™è¯¯: {e}")
            return False

    def _get_balance_regex(self, matched_patterns: List[str]) -> str:
        """æ ¹æ®å“åº”ç±»å‹ç”Ÿæˆä½™é¢çš„æ­£åˆ™è¡¨è¾¾å¼

        ğŸ¯ ä¼˜å…ˆçº§åŒ¹é…è§„åˆ™ï¼šä»ä¸¥æ ¼åˆ°å®½æ¾
        1. ä¸¥æ ¼è§„åˆ™ï¼šç²¾ç¡®åŒ¹é…çº¯å‡€é‡‘é¢æ•°å­—
        2. å®½æ¾è§„åˆ™ï¼šåŒ…å«HTMLç»“æ„çš„åŒ¹é…ï¼ˆé™çº§ä½¿ç”¨ï¼‰
        """
        if self._is_json_response(matched_patterns):
            return "\"balance\":\\s*(?P<balance>[0-9]+)"
        elif self._is_html_response(matched_patterns):
            # ğŸ¯ HTMLå“åº”ï¼šè¿”å›å®½æ¾è§„åˆ™ï¼Œåç»­åœ¨ä¸»æµç¨‹ä¸­åº”ç”¨ä¼˜å…ˆçº§æ¸…æ´—
            return "(?P<balance>\\d{1,10}(?:\\.\\d{2})?)"
        else:
            return "\"balance\":\\s*(?P<balance>[0-9]+)"

    def _get_account_regex(self, matched_patterns: List[str]) -> str:
        """æ ¹æ®å“åº”ç±»å‹ç”Ÿæˆè´¦æˆ·çš„æ­£åˆ™è¡¨è¾¾å¼"""
        if self._is_json_response(matched_patterns):
            return "\"(?:account[^\"]*|acc[^\"]*?)\":\\s*\"(?P<account_info>[^\"]+)\""
        elif self._is_html_response(matched_patterns):
            # é¿å…ä½¿ç”¨ä¸è¢« JS å¼•æ“æ”¯æŒçš„å‰ç»è¯­æ³•ï¼Œæ”¹ä¸ºç­‰ä»·å½¢å¼
            return "(?P<account_info>[A-Z]{2,4}\\d{8,16}|\\d{8,20}[A-Z])"
        else:
            return "\"(?:account[^\"]*|acc[^\"]*?)\":\\s*\"(?P<account_info>[^\"]+)\""

    def _get_currency_regex(self, matched_patterns: List[str]) -> str:
        """æ ¹æ®å“åº”ç±»å‹ç”Ÿæˆè´§å¸çš„æ­£åˆ™è¡¨è¾¾å¼"""
        if self._is_json_response(matched_patterns):
            return "\"(?:currency|currencyCode)\":\\s*\"(?P<currency>[A-Z]{3})\""
        elif self._is_html_response(matched_patterns):
            return "(?P<currency>HKD|USD|CNY|EUR|GBP|JPY|AUD|CAD|SGD)"
        else:
            return "\"(?:currency|currencyCode)\":\\s*\"(?P<currency>[A-Z]{3})\""

    def _get_amount_regex(self, matched_patterns: List[str]) -> str:
        """æ ¹æ®å“åº”ç±»å‹ç”Ÿæˆé‡‘é¢çš„æ­£åˆ™è¡¨è¾¾å¼"""
        if self._is_json_response(matched_patterns):
            return "\"(?:amount|value)\":\\s*(?P<amount>[0-9.]+)"
        elif self._is_html_response(matched_patterns):
            return "(?P<amount>\\$?[0-9,]+(?:\\.\\d{2})?)"
        else:
            return "\"(?:amount|value)\":\\s*(?P<amount>[0-9.]+)"

    def _get_account_type_regex(self, matched_patterns: List[str]) -> str:
        """æ ¹æ®å“åº”ç±»å‹ç”Ÿæˆè´¦æˆ·ç±»å‹çš„æ­£åˆ™è¡¨è¾¾å¼"""
        if self._is_json_response(matched_patterns):
            return "\"(?:accountType|accountStatus)\":\\s*\"(?P<account_type>[^\"]+)\""
        elif self._is_html_response(matched_patterns):
            return "(?P<account_type>å‚¨è“„|æ”¯ç¥¨|å®šæœŸ|æ´»æœŸ|Savings|Checking|Fixed|Current)"
        else:
            return "\"(?:accountType|accountStatus)\":\\s*\"(?P<account_type>[^\"]+)\""

    def _get_major_currency_regex(self, matched_patterns: List[str]) -> str:
        """æ ¹æ®å“åº”ç±»å‹ç”Ÿæˆä¸»è¦è´§å¸çš„æ­£åˆ™è¡¨è¾¾å¼"""
        if self._is_json_response(matched_patterns):
            return "\"(?P<major_currency>HKD|USD|CNY|EUR|GBP|JPY|AUD|CAD|SGD)\""
        elif self._is_html_response(matched_patterns):
            return "(?P<major_currency>HKD|USD|CNY|EUR|GBP|JPY|AUD|CAD|SGD)"
        else:
            return "\"(?P<major_currency>HKD|USD|CNY|EUR|GBP|JPY|AUD|CAD|SGD)\""

    def _get_formatted_amount_regex(self, matched_patterns: List[str]) -> str:
        """æ ¹æ®å“åº”ç±»å‹ç”Ÿæˆæ ¼å¼åŒ–é‡‘é¢çš„æ­£åˆ™è¡¨è¾¾å¼"""
        if self._is_json_response(matched_patterns):
            return "(?P<formatted_amount>\\$[0-9,]+\\.\\d{2}|[0-9,]+\\.\\d{2}\\s*(?:HKD|USD|CNY))"
        elif self._is_html_response(matched_patterns):
            return "(?P<formatted_amount>\\$[0-9,]+\\.\\d{2}|[0-9,]+\\.\\d{2}\\s*(?:HKD|USD|CNY))"
        else:
            return "(?P<formatted_amount>\\$[0-9,]+\\.\\d{2}|[0-9,]+\\.\\d{2}\\s*(?:HKD|USD|CNY))"

    def _get_total_asset_regex(self, matched_patterns: List[str]) -> str:
        """æ ¹æ®å“åº”ç±»å‹ç”Ÿæˆæ€»èµ„äº§çš„æ­£åˆ™è¡¨è¾¾å¼"""
        if self._is_json_response(matched_patterns):
            return "\"(?:total_?asset|net_?worth|portfolio_?value)\":\\s*(?P<total_asset>[0-9.]+)"
        elif self._is_html_response(matched_patterns):
            return "(?P<total_asset>[0-9,]+(?:\\.\\d{2})?)"
        else:
            return "\"(?:total_?asset|net_?worth|portfolio_?value)\":\\s*(?P<total_asset>[0-9.]+)"

    def _get_market_value_regex(self, matched_patterns: List[str]) -> str:
        """æ ¹æ®å“åº”ç±»å‹ç”Ÿæˆå¸‚å€¼çš„æ­£åˆ™è¡¨è¾¾å¼"""
        if self._is_json_response(matched_patterns):
            return "\"(?:market_?value|book_?value|investment_?value)\":\\s*(?P<market_value>[0-9.]+)"
        elif self._is_html_response(matched_patterns):
            return "(?P<market_value>[0-9,]+(?:\\.\\d{2})?)"
        else:
            return "\"(?:market_?value|book_?value|investment_?value)\":\\s*(?P<market_value>[0-9.]+)"

    def _get_core_banking_regex(self, matched_patterns: List[str]) -> str:
        """æ ¹æ®å“åº”ç±»å‹ç”Ÿæˆæ ¸å¿ƒé“¶è¡Œä¸šåŠ¡çš„æ­£åˆ™è¡¨è¾¾å¼"""
        if self._is_json_response(matched_patterns):
            return "\"(?:amount|balance|value)\":\\s*(?P<balance_value>[0-9]+)"
        elif self._is_html_response(matched_patterns):
            return "(?P<balance_value>[0-9,]+(?:\\.\\d{2})?)"
        else:
            return "\"(?:amount|balance|value)\":\\s*(?P<balance_value>[0-9]+)"

    def _get_negative_patterns(self, keywords: List[str]) -> List[tuple]:
        """
        ç”Ÿæˆé€šç”¨çš„è´Ÿé¢æŒ‡æ ‡è§„åˆ™

        Args:
            keywords: è¦æ£€æŸ¥çš„å…³é”®å­—åˆ—è¡¨

        Returns:
            List[tuple]: (pattern, description, penalty) çš„åˆ—è¡¨
        """
        patterns = []
        keyword_pattern = '|'.join(re.escape(kw) for kw in keywords)

        base_patterns = [
            (rf'<!--.*?(?:{keyword_pattern}).*?-->', 'HTMLæ³¨é‡Š', -3),
            (rf'<script[^>]*>.*?(?:{keyword_pattern}).*?</script>', 'JavaScript', -3),
            (rf'<style[^>]*>.*?(?:{keyword_pattern}).*?</style>', 'CSS', -2),
            (rf'/\*.*?(?:{keyword_pattern}).*?\*/', 'CSS/JSæ³¨é‡Š', -2),
            (rf'console\.log.*?(?:{keyword_pattern})', 'Consoleæ—¥å¿—', -2),
            (rf'//.*?(?:{keyword_pattern})', 'å•è¡Œæ³¨é‡Š', -1),
            (rf'function.*?(?:{keyword_pattern}).*?\{{', 'JavaScriptå‡½æ•°', -2),
            (rf'var\s+.*?(?:{keyword_pattern}).*?=', 'JavaScriptå˜é‡', -1),
            (rf'class.*?(?:{keyword_pattern}).*?\{{', 'CSSç±»', -1)
        ]

        return base_patterns

    def _validate_regex_effectiveness(self, content: str, regex: str, field_name: str) -> bool:
        """
        éªŒè¯æ­£åˆ™è¡¨è¾¾å¼çš„æœ‰æ•ˆæ€§ï¼Œå®é™…æµ‹è¯•æ˜¯å¦èƒ½åŒ¹é…åˆ°æœ‰ä»·å€¼çš„å†…å®¹

        Args:
            content: å“åº”å†…å®¹
            regex: æ­£åˆ™è¡¨è¾¾å¼
            field_name: å­—æ®µåç§°

        Returns:
            bool: æ˜¯å¦åº”è¯¥ä¿ç•™è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼
        """
        import re

        try:
            matches = re.findall(regex, content)

            if not matches:
                print(f"âš ï¸ {field_name} æ­£åˆ™è¡¨è¾¾å¼æ— æ³•åŒ¹é…ä»»ä½•å†…å®¹ï¼Œè·³è¿‡ç”Ÿæˆ")
                return False

            # è§„åˆ™1ï¼šè´¦æˆ·å·ç  - å¤šä¸ªåŒ¹é…æ—¶æŒ‰è´¨é‡ç­›é€‰
            if 'account' in field_name.lower():
                return self._validate_account_matches(matches, field_name)

            # è§„åˆ™2ï¼šç”¨æˆ·å§“å - åŒ¹é…è¿‡å¤šæ—¶æ”¾å¼ƒ
            elif 'name' in field_name.lower():
                return self._validate_name_matches(matches, field_name)

            # å…¶ä»–å­—æ®µçš„åŸºæœ¬éªŒè¯
            else:
                if len(matches) > 100:
                    print(f"âš ï¸ {field_name} åŒ¹é…è¿‡å¤š({len(matches)}ä¸ª)ï¼Œå¯èƒ½ä¸å‡†ç¡®ï¼Œè·³è¿‡ç”Ÿæˆ")
                    return False
                return True

        except Exception as e:
            print(f"âŒ {field_name} æ­£åˆ™è¡¨è¾¾å¼æµ‹è¯•å¤±è´¥: {e}")
            return False

    def _validate_account_matches(self, matches: List[str], field_name: str) -> bool:
        """
        è§„åˆ™1ï¼šå¯¹è´¦æˆ·å·ç åŒ¹é…è¿›è¡Œè´¨é‡è¯„ä¼°
        """
        if len(matches) == 0:
            return False

        print(f"ğŸ” {field_name} æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…ï¼Œè¿›è¡Œè´¨é‡è¯„ä¼°")

        # å¯¹æ¯ä¸ªåŒ¹é…è¿›è¡Œæ‰“åˆ†
        scored_matches = []
        for match in matches:
            score = 0

            # é•¿åº¦è¯„åˆ†ï¼š8-20ä½æœ€ä½³
            if 8 <= len(match) <= 20:
                score += 3
            elif 6 <= len(match) <= 25:
                score += 1

            # å­—ç¬¦ç±»å‹è¯„åˆ†ï¼šåŒ…å«æ•°å­—å’Œå­—æ¯/è¿å­—ç¬¦
            if re.search(r'\d', match):
                score += 2
            if re.search(r'[A-Z]', match):
                score += 1
            if '-' in match:
                score += 1

            # ç´§å‡‘æ€§è¯„åˆ†ï¼šé¿å…è¿‡å¤šç©ºç™½å­—ç¬¦
            if match.count(' ') <= 1:
                score += 1

            # é¿å…æ˜æ˜¾çš„æ—¥æœŸæ ¼å¼
            if not re.match(r'^\d{8}$', match):  # é¿å…20140715è¿™ç§æ—¥æœŸ
                score += 2

            scored_matches.append((match, score))

        # æ’åºå¹¶é€‰æ‹©æœ€ä½³åŒ¹é…
        scored_matches.sort(key=lambda x: x[1], reverse=True)
        best_matches = [m for m, s in scored_matches if s >= 4]  # è‡³å°‘4åˆ†

        print(f"   è´¨é‡è¯„ä¼°ç»“æœ: {len(best_matches)} ä¸ªé«˜è´¨é‡åŒ¹é…")
        if best_matches:
            print(f"   æœ€ä½³åŒ¹é…: {best_matches[:3]}")
            return True
        else:
            print(f"   æ²¡æœ‰é«˜è´¨é‡åŒ¹é…ï¼Œè·³è¿‡ç”Ÿæˆ")
            return False

    def _validate_name_matches(self, matches: List[str], field_name: str) -> bool:
        """
        è§„åˆ™2ï¼šå¯¹ç”¨æˆ·å§“ååŒ¹é…è¿›è¡Œæ•°é‡æ§åˆ¶
        """
        print(f"ğŸ” {field_name} æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…")

        # å¦‚æœåŒ¹é…è¿‡å¤šï¼Œè¯´æ˜æ­£åˆ™è¡¨è¾¾å¼è¿‡äºå®½æ³›
        if len(matches) > 50:
            print(f"   åŒ¹é…è¿‡å¤š({len(matches)}ä¸ª)ï¼Œå¯èƒ½åŒ…å«å¤§é‡æ— å…³å†…å®¹ï¼Œè·³è¿‡ç”Ÿæˆ")
            return False

        # æ£€æŸ¥åŒ¹é…è´¨é‡
        valid_matches = []
        for match in matches:
            # è¿‡æ»¤æ˜æ˜¾çš„æ— å…³å†…å®¹
            if (len(match.strip()) < 2 or          # å¤ªçŸ­
                '\n' in match or                   # åŒ…å«æ¢è¡Œç¬¦
                'DOCTYPE' in match or              # HTMLæ ‡ç­¾
                match.isspace() or                 # åªæœ‰ç©ºç™½å­—ç¬¦
                len(match) > 20):                  # å¤ªé•¿
                continue
            valid_matches.append(match)

        print(f"   è¿‡æ»¤åæœ‰æ•ˆåŒ¹é…: {len(valid_matches)} ä¸ª")
        if len(valid_matches) > 0 and len(valid_matches) <= 10:
            print(f"   æœ‰æ•ˆåŒ¹é…ç¤ºä¾‹: {valid_matches[:3]}")
            return True
        else:
            print(f"   æœ‰æ•ˆåŒ¹é…æ•°é‡ä¸åˆç†ï¼Œè·³è¿‡ç”Ÿæˆ")
            return False

    def _validate_account_context(self, content: str) -> bool:
        """
        éªŒè¯è´¦æˆ·å…³é”®å­—çš„ä¸Šä¸‹æ–‡æ˜¯å¦ç¬¦åˆçœŸå®çš„ç”¨æˆ·ä¿¡æ¯æ ¼å¼

        Args:
            content: å“åº”å†…å®¹

        Returns:
            bool: æ˜¯å¦é€šè¿‡ä¸Šä¸‹æ–‡éªŒè¯
        """
        import re

        # æ£€æŸ¥æ˜¯å¦åŒ…å«è´¦æˆ·å…³é”®å­—
        account_keywords = ['account', 'Account', 'è´¦æˆ·', 'è´¦å·']
        if not any(keyword in content for keyword in account_keywords):
            return False

        # ä¸Šä¸‹æ–‡éªŒè¯è§„åˆ™
        validation_score = 0

        # 1. æ£€æŸ¥æ˜¯å¦æœ‰è´¦æˆ·å·ç æ¨¡å¼ï¼ˆ8-20ä½æ•°å­—æˆ–å¸¦å­—æ¯å‰ç¼€çš„è´¦å·ï¼‰
        account_number_patterns = [
            r'\b\d{8,20}\b',  # 8-20ä½çº¯æ•°å­—
            r'\b[A-Z]{2,4}\d{8,16}\b',  # å­—æ¯å‰ç¼€+æ•°å­—
            r'\b\d{4}[-\s]\d{4}[-\s]\d{4,12}\b'  # åˆ†æ®µè´¦å·
        ]

        for pattern in account_number_patterns:
            if re.search(pattern, content):
                validation_score += 2
                break

        # 2. æ£€æŸ¥æ˜¯å¦æœ‰é‡‘èç›¸å…³å­—æ®µ
        financial_keywords = [
            'balance', 'Balance', 'ä½™é¢', 'å¯ç”¨', 'available',
            'currency', 'Currency', 'è´§å¸', 'HKD', 'USD', 'CNY',
            'amount', 'Amount', 'é‡‘é¢', 'æ•°é‡'
        ]

        financial_count = sum(1 for keyword in financial_keywords if keyword in content)
        validation_score += min(financial_count, 3)  # æœ€å¤šåŠ 3åˆ†

        # 3. æ£€æŸ¥æ˜¯å¦åœ¨è¡¨æ ¼æˆ–è¡¨å•ç»“æ„ä¸­
        structure_patterns = [
            r'<table[^>]*>.*?account.*?</table>',
            r'<form[^>]*>.*?account.*?</form>',
            r'<tr[^>]*>.*?account.*?</tr>',
            r'<div[^>]*class[^>]*account[^>]*>',
            r'"account[^"]*":\s*"[^"]*"'  # JSONæ ¼å¼
        ]

        for pattern in structure_patterns:
            if re.search(pattern, content, re.IGNORECASE | re.DOTALL):
                validation_score += 2
                break

        # 4. æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·ä¿¡æ¯ç›¸å…³å­—æ®µ
        user_info_keywords = [
            'name', 'Name', 'å§“å', 'ç”¨æˆ·', 'customer', 'Customer',
            'holder', 'Holder', 'æŒæœ‰äºº', 'owner', 'Owner'
        ]

        user_info_count = sum(1 for keyword in user_info_keywords if keyword in content)
        validation_score += min(user_info_count, 2)  # æœ€å¤šåŠ 2åˆ†

        # 5. è´Ÿé¢æŒ‡æ ‡ï¼šä½¿ç”¨é€šç”¨çš„è´Ÿé¢æŒ‡æ ‡è§„åˆ™
        negative_patterns = self._get_negative_patterns(account_keywords)

        for pattern, desc, penalty in negative_patterns:
            if re.search(pattern, content, re.IGNORECASE | re.DOTALL):
                validation_score += penalty  # penaltyæ˜¯è´Ÿæ•°
                print(f"âŒ å‘ç°è´Ÿé¢æŒ‡æ ‡: {desc} (æ‰£{abs(penalty)}åˆ†)")

        # åˆ¤æ–­é˜ˆå€¼ï¼šæ€»åˆ†>=4åˆ†è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„ç”¨æˆ·ä¿¡æ¯ä¸Šä¸‹æ–‡
        threshold = 4
        is_valid = validation_score >= threshold

        print(f"ğŸ” è´¦æˆ·ä¸Šä¸‹æ–‡éªŒè¯: å¾—åˆ†={validation_score}, é˜ˆå€¼={threshold}, ç»“æœ={'é€šè¿‡' if is_valid else 'ä¸é€šè¿‡'}")

        return is_valid

    def _validate_user_info_context(self, content: str) -> bool:
        """
        éªŒè¯ç”¨æˆ·ä¿¡æ¯å…³é”®å­—çš„ä¸Šä¸‹æ–‡æ˜¯å¦ç¬¦åˆçœŸå®çš„ç”¨æˆ·ä¿¡æ¯æ ¼å¼

        Args:
            content: å“åº”å†…å®¹

        Returns:
            bool: æ˜¯å¦é€šè¿‡ä¸Šä¸‹æ–‡éªŒè¯
        """
        import re

        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç”¨æˆ·ä¿¡æ¯å…³é”®å­—
        user_keywords = ['name', 'Name', 'å§“å', 'ç”¨æˆ·', 'customer', 'Customer', 'holder', 'Holder']
        if not any(keyword in content for keyword in user_keywords):
            return False

        validation_score = 0

        # 1. æ£€æŸ¥æ˜¯å¦æœ‰çœŸå®å§“åæ¨¡å¼
        name_patterns = [
            r'[\u4e00-\u9fff]{2,4}',  # ä¸­æ–‡å§“å
            r'[A-Z][a-z]+\s+[A-Z][a-z]+',  # è‹±æ–‡å§“å
        ]

        for pattern in name_patterns:
            if re.search(pattern, content):
                validation_score += 2
                break

        # 2. æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·ä¿¡æ¯ç›¸å…³å­—æ®µ
        user_fields = ['phone', 'email', 'address', 'id', 'card']
        user_field_count = sum(1 for field in user_fields if field in content)
        validation_score += min(user_field_count, 2)

        # 3. è´Ÿé¢æŒ‡æ ‡ï¼šä½¿ç”¨é€šç”¨çš„è´Ÿé¢æŒ‡æ ‡è§„åˆ™
        negative_patterns = self._get_negative_patterns(user_keywords)

        for pattern, desc, penalty in negative_patterns:
            if re.search(pattern, content, re.IGNORECASE | re.DOTALL):
                validation_score += penalty

        threshold = 3
        is_valid = validation_score >= threshold

        print(f"ğŸ” ç”¨æˆ·ä¿¡æ¯ä¸Šä¸‹æ–‡éªŒè¯: å¾—åˆ†={validation_score}, é˜ˆå€¼={threshold}, ç»“æœ={'é€šè¿‡' if is_valid else 'ä¸é€šè¿‡'}")

        return is_valid

    def _validate_financial_context(self, content: str) -> bool:
        """
        éªŒè¯é‡‘èä¿¡æ¯å…³é”®å­—çš„ä¸Šä¸‹æ–‡æ˜¯å¦ç¬¦åˆçœŸå®çš„é‡‘èæ•°æ®æ ¼å¼

        Args:
            content: å“åº”å†…å®¹

        Returns:
            bool: æ˜¯å¦é€šè¿‡ä¸Šä¸‹æ–‡éªŒè¯
        """
        import re

        # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡‘èå…³é”®å­—
        financial_keywords = ['balance', 'Balance', 'ä½™é¢', 'amount', 'Amount', 'é‡‘é¢', 'currency', 'Currency', 'è´§å¸']
        if not any(keyword in content for keyword in financial_keywords):
            return False

        validation_score = 0

        # 1. æ£€æŸ¥æ˜¯å¦æœ‰é‡‘é¢æ•°å­—æ¨¡å¼
        amount_patterns = [
            r'\d+\.\d{2}',  # å°æ•°é‡‘é¢
            r'\d{1,3}(,\d{3})*',  # åƒåˆ†ä½æ ¼å¼
        ]

        for pattern in amount_patterns:
            if re.search(pattern, content):
                validation_score += 2
                break

        # 2. æ£€æŸ¥æ˜¯å¦æœ‰è´§å¸ç¬¦å·
        currency_symbols = ['$', 'Â¥', 'â‚¬', 'Â£', 'HKD', 'USD', 'CNY']
        currency_count = sum(1 for symbol in currency_symbols if symbol in content)
        validation_score += min(currency_count, 2)

        # 3. è´Ÿé¢æŒ‡æ ‡ï¼šä½¿ç”¨é€šç”¨çš„è´Ÿé¢æŒ‡æ ‡è§„åˆ™
        negative_patterns = self._get_negative_patterns(financial_keywords)

        for pattern, desc, penalty in negative_patterns:
            if re.search(pattern, content, re.IGNORECASE | re.DOTALL):
                validation_score += penalty

        threshold = 3
        is_valid = validation_score >= threshold

        print(f"ğŸ” é‡‘èä¿¡æ¯ä¸Šä¸‹æ–‡éªŒè¯: å¾—åˆ†={validation_score}, é˜ˆå€¼={threshold}, ç»“æœ={'é€šè¿‡' if is_valid else 'ä¸é€šè¿‡'}")

        return is_valid

    def _is_json_response(self, matched_patterns: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºJSONå“åº”

        Args:
            matched_patterns: åŒ¹é…çš„æ¨¡å¼åˆ—è¡¨

        Returns:
            bool: æ˜¯å¦ä¸ºJSONå“åº”
        """
        return any("json_content:" in pattern for pattern in matched_patterns)

    def _get_user_name_regex(self, matched_patterns: List[str]) -> str:
        """æ ¹æ®å“åº”ç±»å‹ç”Ÿæˆç”¨æˆ·å§“åçš„æ­£åˆ™è¡¨è¾¾å¼

        Args:
            matched_patterns: åŒ¹é…çš„æ¨¡å¼åˆ—è¡¨

        Returns:
            str: é€‚åˆçš„æ­£åˆ™è¡¨è¾¾å¼
        """
        if self._is_json_response(matched_patterns):
            # JSONæ ¼å¼ï¼šåŒ¹é…JSONå­—æ®µ
            return "\"(?:user_?name|customer_?name|holder_?name|full_?name)\":\\s*\"(?P<user_name>[^\"]+)\""
        elif self._is_html_response(matched_patterns):
            # HTMLæ ¼å¼ï¼šåŒ¹é…HTMLä¸­çš„å§“åæ–‡æœ¬
            return "(?P<user_name>[\\u4e00-\\u9fff]{2,4}|[A-Za-z\\s]{2,20})"
        else:
            # é»˜è®¤ï¼šå°è¯•JSONæ ¼å¼
            return "\"(?:user_?name|customer_?name|holder_?name|full_?name)\":\\s*\"(?P<user_name>[^\"]+)\""

    def generate_custom_injection(self, api_data: Dict[str, Any], flow_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆè‡ªå®šä¹‰æ³¨å…¥ä»£ç """
        # é¢„å¤„ç†å˜é‡ä»¥é¿å…f-stringè¯­æ³•é—®é¢˜
        institution = api_data.get('institution', 'Unknown')
        url = flow_data['url']
        method = flow_data['method']
        timestamp = datetime.now().isoformat()

        important_headers = self.filter_important_headers(flow_data['request_headers'])
        headers_json = json.dumps(important_headers, indent=16)
        headers_json_compact = json.dumps(important_headers, indent=12)
        geo_location = self.extract_geo_location(flow_data)

        # åŸºç¡€çš„æ³¨å…¥æ¨¡æ¿
        injection_template = f"""
// Auto-generated injection for {institution} API
// API: {url}
// Generated at: {timestamp}

const extractData = async () => {{
    try {{
        const response = await fetch("{url}", {{
            method: "{method}",
            headers: {headers_json},
            credentials: "include",
            mode: "cors"
        }});

        if (!response.ok) {{
            throw new Error(`HTTP error! status: ${{response.status}}`);
        }}

        const data = await response.json();

        // Extract relevant data based on response patterns
        const extractedData = {{
            url: "{url}",
            method: "{method}",
            headers: {headers_json_compact},
            responseBody: data,
            extractedParams: {{}},
            geoLocation: "{geo_location}",
            responseRedactions: [],
            responseMatches: [],
            witnessParameters: {{}}
        }};

        // Send extracted data
        if (window.flutter_inappwebview) {{
            window.flutter_inappwebview.callHandler("extractedData", JSON.stringify(extractedData));
        }}

    }} catch (error) {{
        console.error('Data extraction failed:', error);
    }}
}};

// Auto-trigger extraction when page is ready
if (document.readyState === 'loading') {{
    document.addEventListener('DOMContentLoaded', extractData);
}} else {{
    extractData();
}}
"""

        return injection_template.strip()

    def build_all_providers(self) -> Tuple[List[Dict], List[Dict]]:
        """æ„å»ºæ‰€æœ‰APIçš„provideré…ç½®

        Returns:
            Tuple[List[Dict], List[Dict]]: (æˆåŠŸçš„providers, å­˜ç–‘çš„APIs)
        """
        print("ğŸš€ å¼€å§‹æ„å»ºReclaim Providers...")

        successful_providers = []
        questionable_apis = []

        # è·å–åˆ†æç»“æœä¸­çš„APIæ•°æ®
        extracted_data = self.analysis_data.get('extracted_data', [])

        print(f"ğŸ“Š å…±å‘ç° {len(extracted_data)} ä¸ªæœ‰ä»·å€¼çš„API")

        # ğŸ¯ æ–°å¢ï¼šç”¨äºå»é‡çš„å­—å…¸ï¼Œkeyä¸ºURLï¼Œvalueä¸ºæœ€ä½³çš„APIæ•°æ®
        best_apis_by_url = {}

        # ğŸ¯ ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰å€¼å¾—æ„å»ºproviderçš„APIï¼Œå¹¶é€‰æ‹©æœ€ä½³ç‰ˆæœ¬
        print("ğŸ” ç¬¬ä¸€æ­¥ï¼šAPIå»é‡å’Œæœ€ä½³ç‰ˆæœ¬é€‰æ‹©...")

        def _is_resource_url(url: str) -> bool:
            ul = url.lower()
            # æ˜ç¡®èµ„æºæ‰©å±•å
            resource_exts = ('.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', '.woff', '.woff2', '.ttf', '.map')
            if any(ul.endswith(ext) for ext in resource_exts):
                return True
            # å¸¸è§èµ„æºè·¯å¾„æ®µ
            resource_paths = ['/css/', '/js/', '/assets/', '/static/', '/images/', '/img/']
            if any(p in ul for p in resource_paths):
                return True
            return False

        def _looks_like_login(url: str) -> bool:
            ul = url.lower()
            login_keywords = ['login', 'logon', 'signin', 'sign-in', 'auth', 'lgn']
            return any(k in ul for k in login_keywords)

        for i, api_data in enumerate(extracted_data, 1):
            api_category = api_data.get('api_category', 'unknown')
            provider_worthy = api_data.get('provider_worthy', False)
            url = api_data.get('url', '')

            # é¢å¤–çš„URLçº§è¿‡æ»¤ï¼ˆé˜²æ¼ï¼‰
            if _is_resource_url(url):
                questionable_apis.append({
                    'api_data': api_data,
                    'reason': 'èµ„æºç±»URLï¼ˆåç¼€/è·¯å¾„å‘½ä¸­èµ„æºç‰¹å¾ï¼‰ï¼Œåœ¨æ¸…æ´—é˜¶æ®µæ ‡è®°å¹¶åœ¨æ„å»ºé˜¶æ®µè·³è¿‡',
                    'api_category': 'resource',
                    'confidence_score': 0.0
                })
                continue

            # å°è¯•ç”¨å·²çŸ¥åˆ†ç±»å™¨å†åˆ¤ä¸€æ¬¡ç±»å‹ï¼ˆç»“åˆå“åº”å†…å®¹ï¼‰
            try:
                flow = self.flow_data_map.get(url)
                resp_content = ''
                if flow and flow.get('response_body'):
                    try:
                        resp_content = flow['response_body'].decode('utf-8', errors='ignore')
                    except Exception:
                        resp_content = ''
                api_type_guess = self.classify_api_type(url, resp_content)
            except Exception:
                api_type_guess = 'unknown'

            if _looks_like_login(url) or api_type_guess == 'authentication' or api_category in ('auth', 'resource'):
                questionable_apis.append({
                    'api_data': api_data,
                    'reason': 'éä¸šåŠ¡ç±»APIï¼ˆç™»å½•/èµ„æºï¼‰ï¼Œåœ¨æ¸…æ´—é˜¶æ®µæ ‡è®°å¹¶åœ¨æ„å»ºé˜¶æ®µè·³è¿‡',
                    'api_category': 'auth' if _looks_like_login(url) or api_type_guess == 'authentication' or api_category == 'auth' else 'resource',
                    'confidence_score': 0.0
                })
                continue

            # ğŸš« æ˜ç¡®è¿‡æ»¤éä¸šåŠ¡ç±»APIï¼šç™»å½•ç±»ä¸èµ„æºç±»ç›´æ¥æ ‡è®°å¹¶è·³è¿‡æ„å»º
            if api_category in ('auth', 'resource'):
                questionable_api = {
                    'api_data': api_data,
                    'reason': 'éä¸šåŠ¡ç±»APIï¼ˆç™»å½•/èµ„æºï¼‰ï¼Œåœ¨æ¸…æ´—é˜¶æ®µæ ‡è®°å¹¶åœ¨æ„å»ºé˜¶æ®µè·³è¿‡',
                    'api_category': api_category,
                    'confidence_score': 0.0
                }
                questionable_apis.append(questionable_api)
                continue

            if not provider_worthy:
                questionable_api = {
                    'api_data': api_data,
                    'reason': f'APIåˆ†ç±»ä¸º{api_category}ï¼Œä¸é€‚åˆç”Ÿæˆprovider',
                    'api_category': api_category,
                    'confidence_score': 0.0
                }
                questionable_apis.append(questionable_api)
                continue

            url = api_data['url']

            # ğŸ¯ å»é‡é€»è¾‘ï¼šé€‰æ‹©æœ€ä½³ç‰ˆæœ¬
            if url in best_apis_by_url:
                current_best = best_apis_by_url[url]
                if self._is_better_api_version(api_data, current_best):
                    print(f"ğŸ”„ å‘ç°æ›´ä½³ç‰ˆæœ¬: {url[:60]}...")
                    print(f"   æ›¿æ¢ç‰ˆæœ¬: {len(current_best.get('matched_patterns', []))}æ¨¡å¼ â†’ {len(api_data.get('matched_patterns', []))}æ¨¡å¼")
                    best_apis_by_url[url] = api_data
                else:
                    print(f"âš ï¸  è·³è¿‡é‡å¤API (å·²æœ‰æ›´ä½³ç‰ˆæœ¬): {url[:60]}...")
            else:
                best_apis_by_url[url] = api_data

        print(f"ğŸ“Š å»é‡åå‰©ä½™ {len(best_apis_by_url)} ä¸ªå”¯ä¸€API")

        # ğŸ¯ ç¬¬äºŒæ­¥ï¼šä¸ºé€‰ä¸­çš„æœ€ä½³APIæ„å»ºprovider
        print("ğŸ” ç¬¬äºŒæ­¥ï¼šæ„å»ºproviders...")

        for i, (url, api_data) in enumerate(best_apis_by_url.items(), 1):
            print(f"\nğŸ” å¤„ç†API {i}/{len(best_apis_by_url)}: {api_data['url']}")

            try:
                provider_config, quality_check = self.build_provider_for_api(api_data)

                if provider_config:
                    successful_providers.append(provider_config)
                    print(f"âœ… æˆåŠŸæ„å»ºprovider (ç½®ä¿¡åº¦: {quality_check.confidence_score:.2f})")
                else:
                    # æ·»åŠ åˆ°å­˜ç–‘åˆ—è¡¨
                    questionable_api = {
                        'api_data': api_data,
                        'quality_check': asdict(quality_check) if quality_check else None,
                        'reason': 'è´¨é‡æ£€æŸ¥æœªé€šè¿‡',
                        'missing_fields': quality_check.missing_fields if quality_check else ['unknown'],
                        'confidence_score': quality_check.confidence_score if quality_check else 0.0
                    }
                    questionable_apis.append(questionable_api)
                    print(f"âš ï¸  æ·»åŠ åˆ°å­˜ç–‘åˆ—è¡¨ (ç½®ä¿¡åº¦: {quality_check.confidence_score if quality_check else 0:.2f})")

            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥: {e}")
                questionable_api = {
                    'api_data': api_data,
                    'quality_check': None,
                    'reason': f'å¤„ç†å¼‚å¸¸: {str(e)}',
                    'missing_fields': ['processing_error'],
                    'confidence_score': 0.0
                }
                questionable_apis.append(questionable_api)

        print(f"\nğŸ“ˆ æ„å»ºå®Œæˆ:")
        print(f"   âœ… æˆåŠŸæ„å»º: {len(successful_providers)} ä¸ªproviders")
        print(f"   âš ï¸  å­˜ç–‘API: {len(questionable_apis)} ä¸ª")

        return successful_providers, questionable_apis

    def _is_better_api_version(self, new_api: Dict, current_best: Dict) -> bool:
        """åˆ¤æ–­æ–°çš„APIç‰ˆæœ¬æ˜¯å¦æ¯”å½“å‰æœ€ä½³ç‰ˆæœ¬æ›´å¥½

        Args:
            new_api: æ–°çš„APIæ•°æ®
            current_best: å½“å‰æœ€ä½³APIæ•°æ®

        Returns:
            bool: æ–°ç‰ˆæœ¬æ˜¯å¦æ›´å¥½
        """
        # ğŸ¯ è¯„åˆ¤æ ‡å‡†ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

        # 1. åŒ¹é…æ¨¡å¼æ•°é‡ï¼ˆæ›´å¤šæ¨¡å¼ = æ›´ä¸°å¯Œçš„ç‰¹å¾ï¼‰
        new_patterns = len(new_api.get('matched_patterns', []))
        current_patterns = len(current_best.get('matched_patterns', []))

        if new_patterns != current_patterns:
            return new_patterns > current_patterns

        # 2. ä»·å€¼è¯„åˆ†ï¼ˆæ›´é«˜è¯„åˆ† = æ›´æœ‰ä»·å€¼ï¼‰
        new_score = new_api.get('value_score', 0)
        current_score = current_best.get('value_score', 0)

        if new_score != current_score:
            return new_score > current_score

        # 3. æ•°æ®ç±»å‹æ•°é‡ï¼ˆæ›´å¤šæ•°æ®ç±»å‹ = æ›´ä¸°å¯Œçš„å†…å®¹ï¼‰
        new_data_types = len(new_api.get('data_types', []))
        current_data_types = len(current_best.get('data_types', []))

        if new_data_types != current_data_types:
            return new_data_types > current_data_types

        # 4. ä¼˜å…ˆçº§çº§åˆ«ï¼ˆcritical > high > medium > lowï¼‰
        priority_order = {'critical': 4, 'high': 3, 'medium': 2, 'low': 1, 'unknown': 0}
        new_priority = priority_order.get(new_api.get('priority_level', 'unknown'), 0)
        current_priority = priority_order.get(current_best.get('priority_level', 'unknown'), 0)

        if new_priority != current_priority:
            return new_priority > current_priority

        # 5. å¦‚æœæ‰€æœ‰æŒ‡æ ‡éƒ½ç›¸åŒï¼Œä¿æŒå½“å‰ç‰ˆæœ¬ï¼ˆå…ˆåˆ°å…ˆå¾—ï¼‰
        return False

    def save_results(self, successful_providers: List[Dict], questionable_apis: List[Dict],
                    output_dir: str = "data") -> Tuple[str, str]:
        """ä¿å­˜æ„å»ºç»“æœ

        Args:
            successful_providers: æˆåŠŸçš„providers
            questionable_apis: å­˜ç–‘çš„APIs
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            Tuple[str, str]: (providersæ–‡ä»¶è·¯å¾„, å­˜ç–‘æ–‡ä»¶è·¯å¾„)
        """
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)

        # ä½¿ç”¨æ—¥æœŸä½œä¸ºæ–‡ä»¶ååç¼€ï¼ˆæ”¯æŒåŒæ—¥è¿½åŠ åˆå¹¶ï¼‰
        date_str = datetime.now().strftime("%Y%m%d")
        # ç›®æ ‡æ–‡ä»¶ï¼ˆä»Šå¤©ï¼‰
        providers_file_today = os.path.join(output_dir, f"reclaim_providers_{date_str}.json")

        # è‹¥ä»Šå¤©æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•æ‹·è´ä¸Šä¸€æ—¥ä½œä¸ºåŸºçº¿
        if not os.path.exists(providers_file_today):
            try:
                prev_date = None
                prev_file_path = None
                for fname in os.listdir(output_dir):
                    if not (fname.startswith("reclaim_providers_") and fname.endswith(".json")):
                        continue
                    date_part = fname.replace("reclaim_providers_", "").replace(".json", "")
                    if len(date_part) == 8 and date_part.isdigit() and date_part < date_str:
                        if prev_date is None or date_part > prev_date:
                            prev_date = date_part
                            prev_file_path = os.path.join(output_dir, fname)

                if prev_file_path and os.path.exists(prev_file_path):
                    shutil.copyfile(prev_file_path, providers_file_today)
                    print(f"ğŸ“„ å·²ä»ä¸Šä¸€æ—¥ {prev_date} æ‹·è´ providers æ–‡ä»¶ä¸ºä»Šæ—¥åŸºçº¿: {providers_file_today}")
            except Exception as e:
                print(f"âš ï¸ æ‹·è´ä¸Šä¸€æ—¥ providers æ–‡ä»¶å¤±è´¥ï¼ˆå¿½ç•¥å¹¶ç»§ç»­ï¼‰ï¼š{e}")

        # ğŸ¯ è¯»å–å·²æœ‰æ–‡ä»¶ï¼ŒåŸºäº URL è¿›è¡Œâ€œè¿½åŠ åˆå¹¶â€
        def _extract_primary_url(p: Dict) -> Optional[str]:
            try:
                req_datas = p.get('providerConfig', {}).get('providerConfig', {}).get('requestData', [])
                if isinstance(req_datas, list) and req_datas:
                    return req_datas[0].get('url')
            except Exception:
                pass
            return None

        # è§„èŒƒåŒ–URLï¼šå¿½ç•¥æ˜“å˜å‚æ•°ï¼Œç”¨äºâ€œç›¸ä¼¼â€åˆ¤æ–­ä¸å»é‡é”®
        def _normalize_url_key(url: str) -> str:
            try:
                pr = urlparse(url)
                qs = parse_qs(pr.query, keep_blank_values=True)
                volatile_params = {
                    'dse_sessionId', 'mcp_timestamp', 'dse_pageId', 'sessionId',
                    'timestamp', '_t', '_ts', 'ts'
                }
                kept = []
                for k, vals in qs.items():
                    if k.lower() in {p.lower() for p in volatile_params}:
                        continue
                    for v in vals:
                        kept.append((k, v))
                kept.sort()
                norm_q = '&'.join([f"{k}={v}" for k, v in kept]) if kept else ''
                return f"{pr.netloc}{pr.path}?{norm_q}" if norm_q else f"{pr.netloc}{pr.path}"
            except Exception:
                return url

        # ç»Ÿè®¡ä¸€ä¸ªprovideræ‰€æœ‰ responseMatches çš„æ•°é‡ï¼Œç”¨äºé€‰æ‹©â€œæ›´ä¼˜â€ç‰ˆæœ¬
        def _count_response_matches(p: Dict) -> int:
            try:
                total = 0
                rds = p.get('providerConfig', {}).get('providerConfig', {}).get('requestData', []) or []
                for rd in rds:
                    rms = rd.get('responseMatches', []) or []
                    total += len(rms)
                return total
            except Exception:
                return 0

        providers_file = providers_file_today
        existing_data: Dict[str, Any] = {}
        existing_providers: Dict[str, Dict] = {}
        if os.path.exists(providers_file):
            try:
                with open(providers_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    existing_providers = existing_data.get('providers', {}) or {}
            except Exception as e:
                print(f"âš ï¸  è¯»å–å·²æœ‰providersæ–‡ä»¶å¤±è´¥ï¼Œå¿½ç•¥å¹¶é‡æ–°ç”Ÿæˆ: {e}")
                existing_data = {}
                existing_providers = {}

        # æ„å»º è§„èŒƒåŒ–URLé”® -> providerId æ˜ å°„ï¼ˆæ¥è‡ªå·²æœ‰æ–‡ä»¶ï¼ŒæŒ‰â€œæ›´ä¼˜æ¡ç›®â€å ä½ï¼‰
        key_to_provider_id: Dict[str, str] = {}
        key_to_best_prov: Dict[str, Dict] = {}
        for pid, prov in existing_providers.items():
            u = _extract_primary_url(prov)
            if not u:
                continue
            key = _normalize_url_key(u)
            if key not in key_to_best_prov or _count_response_matches(prov) > _count_response_matches(key_to_best_prov[key]):
                key_to_best_prov[key] = prov
                key_to_provider_id[key] = pid

        # åŸºäº URL åˆå¹¶ï¼š
        merged_providers: Dict[str, Dict] = dict(existing_providers)

        for new_provider in successful_providers:
            new_cfg = new_provider.get('providerConfig', {})
            new_pid = new_cfg.get('providerId')
            new_url = _extract_primary_url(new_provider)
            if not new_pid or not new_url:
                print("âš ï¸  è·³è¿‡æ— æ•ˆproviderï¼ˆç¼ºå°‘providerIdæˆ–urlï¼‰")
                continue

            key = _normalize_url_key(new_url)
            if key in key_to_provider_id:
                # ç›¸ä¼¼ï¼ˆè§„èŒƒåŒ–åï¼‰URL å·²å­˜åœ¨ï¼šå¤ç”¨å­˜é‡ providerIdï¼Œå…¶ä½™å†…å®¹ç”¨æ–°å†…å®¹è¦†ç›–
                exist_pid = key_to_provider_id[key]
                try:
                    new_provider['providerConfig']['providerId'] = exist_pid
                except Exception:
                    pass
                merged_providers[exist_pid] = new_provider
                # æ›´æ–°å½“å‰keyå¯¹åº”çš„â€œæœ€ä½³â€å ä½
                key_to_best_prov[key] = new_provider
            else:
                # æ–° URLï¼šç›´æ¥è¿½åŠ 
                merged_providers[new_pid] = new_provider
                key_to_provider_id[key] = new_pid
                key_to_best_prov[key] = new_provider

        # æ¸…ç†ï¼šç§»é™¤ responseMatches ä¸ºç©ºçš„å­˜é‡ä¸æ–°æ¡ç›®
        def _has_nonempty_matches(p: Dict) -> bool:
            try:
                req_datas = p.get('providerConfig', {}).get('providerConfig', {}).get('requestData', [])
                if not isinstance(req_datas, list) or not req_datas:
                    return False
                # è‹¥ä»»æ„ä¸€æ¡ requestData çš„ responseMatches éç©ºï¼Œåˆ™ä¿ç•™
                for rd in req_datas:
                    rms = rd.get('responseMatches', [])
                    if isinstance(rms, list) and len(rms) > 0:
                        return True
                return False
            except Exception:
                return False

        cleaned_providers: Dict[str, Dict] = {
            pid: prov for pid, prov in merged_providers.items() if _has_nonempty_matches(prov)
        }

        # è§„èŒƒåŒ–URLå»é‡ï¼ˆä¸¥æ ¼ç‰ˆï¼‰ï¼šä»…å½“ host+path å®Œå…¨ä¸€è‡´ï¼Œmethod ä¸ requestHash ä¸€è‡´æ—¶æ‰å…è®¸è¦†ç›–ï¼›å¦åˆ™å¹¶å­˜
        def _extract_host_path_method_hash(p: Dict) -> Tuple[str, str, str, str]:
            try:
                rds = p.get('providerConfig', {}).get('providerConfig', {}).get('requestData', []) or []
                rd0 = rds[0] if rds else {}
                url = rd0.get('url', '')
                pr = urlparse(url)
                method = (rd0.get('method') or '').upper()
                rhash = rd0.get('requestHash') or ''
                return pr.netloc.lower(), pr.path, method, rhash
            except Exception:
                return '', '', '', ''

        deduped: Dict[str, Dict] = {}
        for pid, prov in cleaned_providers.items():
            host, path, method, rhash = _extract_host_path_method_hash(prov)
            key = f"{host}{path}"
            if key not in deduped:
                deduped[key] = prov
            else:
                # ä»…å½“ method ä¸ requestHash éƒ½ä¸€è‡´æ—¶æ‰å…è®¸â€œæ‹©ä¼˜è¦†ç›–â€ï¼Œå¦åˆ™å¹¶å­˜ï¼ˆé¿å…è·¨ç«¯ç‚¹é”™å¹¶ï¼‰
                oh, op, om, orh = _extract_host_path_method_hash(deduped[key])
                if om == method and orh == rhash:
                    if _count_response_matches(prov) > _count_response_matches(deduped[key]):
                        deduped[key] = prov

        # æœ€ç»ˆå®‰å…¨è¿‡æ»¤ï¼šå†æ¬¡æ’é™¤ç™»å½•/èµ„æºç±»providerï¼ˆå¤šä¸€é“ä¿é™©ï¼‰
        def _is_non_business_provider(p: Dict) -> bool:
            try:
                meta = p.get('providerConfig', {}).get('providerConfig', {}).get('metadata', {})
                api_type = str(meta.get('api_type', '')).lower()
                if api_type in ('authentication', 'login', 'resource'):
                    return True
                # URLè¾…åŠ©åˆ¤æ–­
                rds = p.get('providerConfig', {}).get('providerConfig', {}).get('requestData', []) or []
                url0 = (rds[0].get('url') if rds else '') or ''
                ul = url0.lower()
                if any(ul.endswith(ext) for ext in ('.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', '.woff', '.woff2', '.ttf', '.map')):
                    return True
                if any(seg in ul for seg in ['/css/', '/js/', '/assets/', '/static/', '/images/', '/img/']):
                    return True
                if any(k in ul for k in ['login', 'logon', 'signin', 'sign-in', 'auth', 'lgn']):
                    # å¦‚æœURLå¼ºçƒˆæŒ‡ç¤ºç™»å½•ï¼Œä¸”ä¸æ˜¯æ˜ç¡®çš„ä¸šåŠ¡ç«¯ç‚¹ï¼Œè§†ä¸ºéä¸šåŠ¡
                    if not any(k in ul for k in ['overview', 'balance', 'account', 'acc', 'history', 'statement', 'transaction']):
                        return True
                return False
            except Exception:
                return False

        deduped_business_only: Dict[str, Dict] = {k: v for k, v in deduped.items() if not _is_non_business_provider(v)}

        # é‡æ–°æ„å»ºç´¢å¼•
        providers_indexed = {prov.get('providerConfig', {}).get('providerId', pid): prov for pid, prov in cleaned_providers.items() if prov in deduped_business_only.values()}
        provider_index: Dict[str, Any] = {}
        for pid, prov in providers_indexed.items():
            prov_cfg = prov.get('providerConfig', {})
            metadata = prov_cfg.get('providerConfig', {}).get('metadata', {})
            provider_index[pid] = {
                "institution": metadata.get('institution', ''),
                "api_type": metadata.get('api_type', ''),
                "priority_level": metadata.get('priority_level', 'medium'),
                "value_score": metadata.get('value_score', 0),
                "confidence_score": metadata.get('confidence_score', 0.0),
                "created_at": metadata.get('generated_at', ''),
                "config_id": prov_cfg.get('id', '')
            }

        # ä¿å­˜æˆåŠŸçš„providersï¼ˆæ–°çš„å¯ç´¢å¼•ç»“æ„ï¼‰
        providers_output = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "date": date_str,
                "total_providers": len(providers_indexed),
                "source_mitm_file": self.mitm_file_path,
                "source_analysis_file": self.analysis_result_file,
                "generator_version": "1.0.0",
                "index_structure": "providerId_based",
                "description": "Daily provider configurations indexed by providerId for efficient lookup"
            },
            "provider_index": provider_index,
            "providers": providers_indexed,
            "query_helpers": {
                "get_provider_by_id": "providers[providerId]",
                "get_provider_metadata": "provider_index[providerId]",
                "list_all_provider_ids": "Object.keys(providers)",
                "filter_by_institution": "Object.entries(provider_index).filter(([id, meta]) => meta.institution === institutionName)",
                "filter_by_api_type": "Object.entries(provider_index).filter(([id, meta]) => meta.api_type === apiType)",
                "filter_by_priority": "Object.entries(provider_index).filter(([id, meta]) => meta.priority_level === priority)"
            }
        }

        with open(providers_file, 'w', encoding='utf-8') as f:
            json.dump(providers_output, f, indent=2, ensure_ascii=False)

        # ä¿å­˜å­˜ç–‘çš„APIs
        questionable_file = os.path.join(output_dir, f"questionable_apis_{date_str}.json")
        questionable_output = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_questionable": len(questionable_apis),
                "reasons_summary": self.analyze_questionable_reasons(questionable_apis),
                "source_mitm_file": self.mitm_file_path,
                "source_analysis_file": self.analysis_result_file
            },
            "questionable_apis": questionable_apis
        }

        with open(questionable_file, 'w', encoding='utf-8') as f:
            json.dump(questionable_output, f, indent=2, ensure_ascii=False)

        return providers_file, questionable_file

    def analyze_questionable_reasons(self, questionable_apis: List[Dict]) -> Dict[str, int]:
        """åˆ†æå­˜ç–‘APIçš„åŸå› ç»Ÿè®¡"""
        reasons = {}

        for api in questionable_apis:
            missing_fields = api.get('missing_fields', [])
            for field in missing_fields:
                reasons[field] = reasons.get(field, 0) + 1

        return reasons

    @staticmethod
    def load_providers_by_date(date_str: str, data_dir: str = "data") -> Optional[Dict]:
        """æŒ‰æ—¥æœŸåŠ è½½provideré…ç½®æ–‡ä»¶

        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYYMMDD
            data_dir: æ•°æ®ç›®å½•

        Returns:
            Optional[Dict]: åŠ è½½çš„provideræ•°æ®ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨è¿”å›None
        """
        providers_file = os.path.join(data_dir, f"reclaim_providers_{date_str}.json")

        if not os.path.exists(providers_file):
            return None

        try:
            with open(providers_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½provideræ–‡ä»¶å¤±è´¥: {e}")
            return None

    @staticmethod
    def query_provider_by_id(provider_id: str, date_str: str, data_dir: str = "data") -> Optional[Dict]:
        """é€šè¿‡providerIdæŸ¥è¯¢provideré…ç½®

        Args:
            provider_id: Provider ID
            date_str: æ—¥æœŸå­—ç¬¦ä¸²
            data_dir: æ•°æ®ç›®å½•

        Returns:
            Optional[Dict]: Provideré…ç½®ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        providers_data = ReclaimProviderBuilder.load_providers_by_date(date_str, data_dir)

        if not providers_data:
            return None

        return providers_data.get('providers', {}).get(provider_id)

    @staticmethod
    def query_providers_by_institution(institution: str, date_str: str, data_dir: str = "data") -> List[Dict]:
        """é€šè¿‡æœºæ„åæŸ¥è¯¢providers

        Args:
            institution: æœºæ„å
            date_str: æ—¥æœŸå­—ç¬¦ä¸²
            data_dir: æ•°æ®ç›®å½•

        Returns:
            List[Dict]: åŒ¹é…çš„providersåˆ—è¡¨
        """
        providers_data = ReclaimProviderBuilder.load_providers_by_date(date_str, data_dir)

        if not providers_data:
            return []

        provider_index = providers_data.get('provider_index', {})
        providers = providers_data.get('providers', {})

        matching_providers = []
        for provider_id, metadata in provider_index.items():
            if metadata.get('institution', '').lower() == institution.lower():
                provider_config = providers.get(provider_id)
                if provider_config:
                    matching_providers.append({
                        'provider_id': provider_id,
                        'metadata': metadata,
                        'config': provider_config
                    })

        return matching_providers

    @staticmethod
    def list_all_provider_ids(date_str: str, data_dir: str = "data") -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰provider IDs

        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²
            data_dir: æ•°æ®ç›®å½•

        Returns:
            List[str]: Provider IDsåˆ—è¡¨
        """
        providers_data = ReclaimProviderBuilder.load_providers_by_date(date_str, data_dir)

        if not providers_data:
            return []

        return list(providers_data.get('providers', {}).keys())



def run_integration_and_build_providers(mitm_file: str, output_dir: str = "data") -> Tuple[str, str, str]:
    """è¿è¡Œå®Œæ•´çš„é›†æˆæµç¨‹ï¼šåˆ†æ + æ„å»ºproviders

    Args:
        mitm_file: mitmæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        Tuple[str, str, str]: (åˆ†æç»“æœæ–‡ä»¶, providersæ–‡ä»¶, å­˜ç–‘æ–‡ä»¶)
    """
    print("ğŸš€ å¼€å§‹å®Œæ•´çš„Provideræ„å»ºæµç¨‹...")

    # ç¬¬ä¸€æ­¥ï¼šè¿è¡Œintegrate_with_mitmproxy2swagger.pyè¿›è¡Œåˆ†æ
    print("\nğŸ“Š ç¬¬ä¸€æ­¥ï¼šè¿è¡Œç‰¹å¾åº“åˆ†æ...")

    import subprocess
    import sys

    # æ„å»ºå›ºå®šçš„åˆ†æç»“æœæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    analysis_result_file = os.path.join(output_dir, f"provider_analysis_result_{timestamp}.json")

    # è¿è¡Œåˆ†æè„šæœ¬
    script_dir = Path(__file__).parent.parent / "feature-library" / "plugins"
    cmd = [
        sys.executable,
        str(script_dir / "integrate_with_mitmproxy2swagger.py"),
        "--mode", "direct",
        "--input", mitm_file,
        "--output", analysis_result_file
    ]

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        print("âœ… ç‰¹å¾åº“åˆ†æå®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"âŒ ç‰¹å¾åº“åˆ†æå¤±è´¥: {e}")
        print(f"é”™è¯¯è¾“å‡º: {e.stderr}")
        raise

    # ç¬¬äºŒæ­¥ï¼šæ„å»ºproviders
    print("\nğŸ—ï¸  ç¬¬äºŒæ­¥ï¼šæ„å»ºReclaim Providers...")

    builder = ReclaimProviderBuilder(mitm_file, analysis_result_file)
    successful_providers, questionable_apis = builder.build_all_providers()

    # ç¬¬ä¸‰æ­¥ï¼šä¿å­˜ç»“æœ
    print("\nğŸ’¾ ç¬¬ä¸‰æ­¥ï¼šä¿å­˜æ„å»ºç»“æœ...")

    providers_file, questionable_file = builder.save_results(
        successful_providers, questionable_apis, output_dir
    )

    print(f"\nğŸ‰ å®Œæ•´æµç¨‹å®Œæˆ!")
    print(f"ğŸ“ åˆ†æç»“æœ: {analysis_result_file}")
    print(f"ğŸ“ Providers: {providers_file}")
    print(f"ğŸ“ å­˜ç–‘APIs: {questionable_file}")

    return analysis_result_file, providers_file, questionable_file


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description='Reclaim Provideræ„å»ºå™¨')
    parser.add_argument('--mitm-file', '-i', required=True, help='è¾“å…¥çš„mitmæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--analysis-file', '-a', help='ç‰¹å¾åº“åˆ†æç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›ä¼šè‡ªåŠ¨è¿è¡Œåˆ†æï¼‰')
    parser.add_argument('--output-dir', '-o', default='data', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--run-full-pipeline', '-f', action='store_true',
                       help='è¿è¡Œå®Œæ•´æµç¨‹ï¼ˆåˆ†æ+æ„å»ºï¼‰')

    args = parser.parse_args()

    try:
        if args.run_full_pipeline or not args.analysis_file:
            # è¿è¡Œå®Œæ•´æµç¨‹
            analysis_file, providers_file, questionable_file = run_integration_and_build_providers(
                args.mitm_file, args.output_dir
            )
        else:
            # åªæ„å»ºproviders
            print("ğŸ—ï¸  æ„å»ºReclaim Providers...")

            builder = ReclaimProviderBuilder(args.mitm_file, args.analysis_file)
            successful_providers, questionable_apis = builder.build_all_providers()

            providers_file, questionable_file = builder.save_results(
                successful_providers, questionable_apis, args.output_dir
            )

            print(f"\nğŸ‰ æ„å»ºå®Œæˆ!")
            print(f"ğŸ“ Providers: {providers_file}")
            print(f"ğŸ“ å­˜ç–‘APIs: {questionable_file}")

    except Exception as e:
        print(f"âŒ æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

