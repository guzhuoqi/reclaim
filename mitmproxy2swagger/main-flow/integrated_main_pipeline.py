#!/usr/bin/env python3
"""
é›†æˆä¸»æµç¨‹è„šæœ¬
Integrated Main Pipeline Script

é›†æˆç‰¹å¾åº“åˆ†æå’ŒProvideræ„å»ºçš„å®Œæ•´ä¸»æµç¨‹
ä»mitmæŠ“åŒ…æ–‡ä»¶åˆ°æœ€ç»ˆReclaim Provideré…ç½®çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆ

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ£€æµ‹mitmä»£ç†æœåŠ¡çŠ¶æ€
2. å¯¼å‡ºæŠ“åŒ…æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
3. è¿è¡Œä¸¥æ ¼ç‰¹å¾åº“åˆ†æï¼ˆintegrate_with_mitmproxy2swagger.pyï¼‰
4. æ„å»ºReclaim Provideré…ç½®ï¼ˆprovider_builder.pyï¼‰
5. ç”Ÿæˆå®Œæ•´çš„æ‰§è¡ŒæŠ¥å‘Š

ä½¿ç”¨æ–¹å¼ï¼š
- åœ¨çº¿æ¨¡å¼ï¼špython3 integrated_main_pipeline.py
- ç¦»çº¿æ¨¡å¼ï¼špython3 integrated_main_pipeline.py --offline --input-file flows.mitm
- æŒ‡å®šé…ç½®ï¼špython3 integrated_main_pipeline.py --mitm-host 10.10.10.146 --mitm-port 8082
"""

import os
import sys
import json
import time
import requests
import argparse
import subprocess
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from math import floor

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).parent
sys.path.append(str(current_dir.parent / "feature-library" / "plugins"))

# ç¡®ä¿logså’Œdataç›®å½•å­˜åœ¨
os.makedirs('logs', exist_ok=True)
os.makedirs('data', exist_ok=True)

# é…ç½®æ—¥å¿— - åªè¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œä¸åˆ›å»ºæ—¥å¿—æ–‡ä»¶
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class IntegratedMainPipeline:
    """é›†æˆä¸»æµç¨‹ç®¡é“å™¨"""

    def __init__(self, config: Dict[str, Any] = None):
        """åˆå§‹åŒ–é›†æˆæµç¨‹

        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or {}

        # é»˜è®¤é…ç½®
        self.default_config = {
            'mitm_host': '127.0.0.1',
            'mitm_port': 8080,
            'output_dir': 'data',
            'temp_dir': 'temp'
        }

        # åˆå¹¶é…ç½®
        for key, value in self.default_config.items():
            if key not in self.config:
                self.config[key] = value

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.ensure_directories()

        # æµç¨‹çŠ¶æ€è·Ÿè¸ª
        self.pipeline_state = {
            'mitm_status': 'unknown',
            'export_file': None,
            'analysis_result': None,
            'provider_result': None,
            'steps_completed': [],
            'errors': [],
            'start_time': datetime.now(),
            'output_files': {}
        }

        logger.info(f"é›†æˆä¸»æµç¨‹åˆå§‹åŒ–å®Œæˆï¼Œé…ç½®: {self.config}")

    def ensure_directories(self):
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        for dir_key in ['output_dir', 'temp_dir']:
            dir_path = self.config[dir_key]
            if not os.path.exists(dir_path):
                os.makedirs(dir_path)
                print(f"âœ… åˆ›å»ºç›®å½•: {dir_path}")

    def check_mitm_proxy_status(self) -> Tuple[bool, str]:
        """æ£€æµ‹mitmproxyæœåŠ¡çŠ¶æ€

        Returns:
            Tuple[bool, str]: (æ˜¯å¦è¿è¡Œ, çŠ¶æ€æ¶ˆæ¯)
        """
        mitm_host = self.config['mitm_host']
        mitm_port = self.config['mitm_port']

        try:
            base = f"http://{mitm_host}:{mitm_port}"
            url = f"{base}/flows/dump"
            print(f"ğŸ” æ£€æµ‹mitmä»£ç†çŠ¶æ€: {mitm_host}:{mitm_port}")

            # å…¼å®¹ mitmweb CSRF ä¿æŠ¤ï¼šå…ˆè®¿é—®é¦–é¡µæ‹¿ csrftokenï¼Œå†æºå¸¦å¤´éƒ¨è®¿é—® /flows/dump
            session = requests.Session()
            try:
                r0 = session.get(base + "/", timeout=5)
                csrf_token = session.cookies.get('csrftoken') or session.cookies.get('csrf_token')
                headers = {
                    'X-CSRFToken': csrf_token
                } if csrf_token else {}
                # è¡¥å……Refererä¸XHRå¤´ï¼Œå…¼å®¹ä¸¥æ ¼æ¨¡å¼
                headers['Referer'] = base + "/"
                headers['X-Requested-With'] = 'XMLHttpRequest'
                response = session.get(url, headers=headers, timeout=5)
            except Exception:
                # å›é€€åˆ°ç›´æ¥è¯·æ±‚
                response = requests.get(url, timeout=5)

            if response.status_code == 200:
                content_length = len(response.content)
                self.pipeline_state['mitm_status'] = 'running'
                status_msg = f"âœ… mitmä»£ç†è¿è¡Œæ­£å¸¸ï¼Œå½“å‰æµé‡æ•°æ®: {content_length} bytes"
                print(status_msg)
                logger.info(f"mitmä»£ç†çŠ¶æ€æ­£å¸¸: {content_length} bytes")
                return True, status_msg
            else:
                error_msg = f"âŒ mitmä»£ç†å“åº”å¼‚å¸¸: HTTP {response.status_code}"
                print(error_msg)
                logger.error(error_msg)
                return False, error_msg

        except requests.exceptions.ConnectionError:
            error_msg = f"âŒ æ— æ³•è¿æ¥åˆ°mitmä»£ç†: {mitm_host}:{mitm_port}"
            print(error_msg)
            logger.error(error_msg)
            self.pipeline_state['mitm_status'] = 'connection_failed'
            return False, error_msg

        except Exception as e:
            error_msg = f"âŒ mitmä»£ç†æ£€æµ‹å¤±è´¥: {e}"
            print(error_msg)
            logger.error(error_msg)
            self.pipeline_state['mitm_status'] = 'error'
            return False, error_msg

    def export_mitm_flows(self, output_file: str = None, max_download_bytes: Optional[int] = None) -> Optional[str]:
        """ä»mitmproxyå¯¼å‡ºæµé‡æ•°æ®

        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            max_download_bytes: æœ€å¤§ä¸‹è½½å­—èŠ‚æ•°ï¼Œé€šè¿‡curl --max-filesizeé™åˆ¶æˆ–mitmproxy APIæŸ¥è¯¢å‚æ•°é—´æ¥æ§åˆ¶

        Returns:
            å¯¼å‡ºçš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        if not output_file:
            date_str = datetime.now().strftime("%Y%m%d")
            output_file = os.path.join(self.config['temp_dir'], f"flows_export_{date_str}.mitm")

        mitm_host = self.config['mitm_host']
        mitm_port = self.config['mitm_port']
        
        # æ„å»ºAPI URLï¼Œå¦‚æœæŒ‡å®šäº†å¤§å°é™åˆ¶ï¼Œå°è¯•é€šè¿‡æŸ¥è¯¢å‚æ•°é—´æ¥æ§åˆ¶
        base = f"http://{mitm_host}:{mitm_port}"
        base_url = f"{base}/flows/dump"
        if max_download_bytes:
            # ä¼°ç®—æµé‡æ¡æ•°é™åˆ¶ï¼ˆå‡è®¾å¹³å‡æ¯æ¡æµé‡çº¦10KBï¼‰
            estimated_flows = max(1, max_download_bytes // (10 * 1024))
            # æ³¨æ„ï¼šmitmproxyå¯èƒ½ä¸æ”¯æŒlimitå‚æ•°ï¼Œè¿™é‡Œä½œä¸ºå°è¯•
            url = f"{base_url}?limit={estimated_flows}"
            print(f"   å°è¯•é™åˆ¶æµé‡æ¡æ•°: {estimated_flows} (åŸºäº{max_download_bytes}å­—èŠ‚ä¼°ç®—)")
        else:
            url = base_url

        try:
            print(f"ğŸ“¥ å¼€å§‹å¯¼å‡ºæµé‡æ•°æ®...")
            print(f"   æºåœ°å€: {url}")
            print(f"   ç›®æ ‡æ–‡ä»¶: {output_file}")

            # è·å– CSRF tokenï¼ˆå¦‚æœæœ‰ï¼‰
            csrf_token = None
            try:
                s = requests.Session()
                s.get(base + "/", timeout=5)
                csrf_token = s.cookies.get('csrftoken') or s.cookies.get('csrf_token')
            except Exception:
                csrf_token = None

            # ä½¿ç”¨curlå‘½ä»¤å¯¼å‡ºï¼Œå¯é€‰é™åˆ¶ä¸‹è½½å¤§å°ï¼ˆæºå¸¦ CSRF/Referer/XHRï¼‰
            curl_cmd = ['curl', '-s', url, '-H', f'Referer: {base}/', '-H', 'X-Requested-With: XMLHttpRequest']
            if csrf_token:
                curl_cmd.extend(['-H', f'X-CSRFToken: {csrf_token}', '-b', f'csrftoken={csrf_token}'])
            if max_download_bytes:
                curl_cmd.extend(['--max-filesize', str(max_download_bytes)])
                print(f"   é™åˆ¶ä¸‹è½½å¤§å°: {max_download_bytes} bytes ({max_download_bytes / 1024 / 1024:.1f}MB)")

            with open(output_file, 'wb') as f:
                result = subprocess.run(curl_cmd, stdout=f, stderr=subprocess.PIPE)

            if result.returncode == 0:
                file_size = os.path.getsize(output_file)
                if file_size > 0:
                    print(f"âœ… æˆåŠŸå¯¼å‡ºæµé‡æ•°æ®: {file_size} bytes")
                    logger.info(f"æµé‡æ•°æ®å¯¼å‡ºæˆåŠŸ: {output_file}, {file_size} bytes")

                    # è‹¥æ–‡ä»¶è¶…è¿‡5MBï¼ŒæŒ‰è§„åˆ™è£å‰ª
                    MAX_SIZE = 5 * 1024 * 1024
                    if file_size > MAX_SIZE:
                        print("âš ï¸  å¯¼å‡ºæ–‡ä»¶å¤§äº5MBï¼Œå¼€å§‹æŒ‰è§„åˆ™è£å‰ªï¼ˆæœ€è¿‘30åˆ†é’Ÿï¼Œä¸”æœ€ç»ˆä¸è¶…è¿‡5MBï¼‰...")
                        trimmed_file = self._trim_mitm_file(output_file, max_size_bytes=MAX_SIZE, window_minutes=30)
                        if trimmed_file and os.path.exists(trimmed_file):
                            output_file = trimmed_file
                            file_size = os.path.getsize(output_file)
                            print(f"âœ… è£å‰ªå®Œæˆ: {file_size} bytes -> {output_file}")
                            logger.info(f"æµé‡æ•°æ®å·²è£å‰ªè‡³<=5MB: {output_file}, {file_size} bytes")
                        else:
                            print("âš ï¸  è£å‰ªå¤±è´¥æˆ–æ— å¯ç”¨æ•°æ®ï¼Œç»§ç»­ä½¿ç”¨åŸå§‹å¯¼å‡ºæ–‡ä»¶")

                    self.pipeline_state['export_file'] = output_file
                    self.pipeline_state['steps_completed'].append('export')
                    return output_file
                else:
                    print(f"âš ï¸  å¯¼å‡ºçš„æ–‡ä»¶ä¸ºç©º")
                    logger.warning("å¯¼å‡ºçš„æ–‡ä»¶ä¸ºç©º")
                    return None
            else:
                error_msg = result.stderr.decode() if result.stderr else "æœªçŸ¥é”™è¯¯"
                # curlè¿”å›ç 63è¡¨ç¤ºæ–‡ä»¶å¤§å°è¶…è¿‡--max-filesizeé™åˆ¶
                if result.returncode == 63:
                    print(f"âš ï¸  ä¸‹è½½è¢«ä¸­æ–­ï¼šæ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ ({max_download_bytes} bytes)")
                    # æ£€æŸ¥æ˜¯å¦æœ‰éƒ¨åˆ†æ•°æ®è¢«ä¸‹è½½
                    if os.path.exists(output_file) and os.path.getsize(output_file) > 0:
                        file_size = os.path.getsize(output_file)
                        print(f"âœ… å·²ä¸‹è½½éƒ¨åˆ†æ•°æ®: {file_size} bytesï¼Œç»§ç»­ä½¿ç”¨")
                        logger.info(f"éƒ¨åˆ†ä¸‹è½½å®Œæˆ: {output_file}, {file_size} bytes")
                        self.pipeline_state['export_file'] = output_file
                        self.pipeline_state['steps_completed'].append('export')
                        return output_file
                    else:
                        print(f"âŒ æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®")
                        return None
                else:
                    print(f"âŒ curlå¯¼å‡ºå¤±è´¥: {error_msg}")
                    logger.error(f"curlå¯¼å‡ºå¤±è´¥: {error_msg}")
                    return None

        except Exception as e:
            print(f"âŒ å¯¼å‡ºæµé‡æ•°æ®å¤±è´¥: {e}")
            logger.error(f"å¯¼å‡ºæµé‡æ•°æ®å¤±è´¥: {e}")
            self.pipeline_state['errors'].append(f"å¯¼å‡ºå¤±è´¥: {e}")
            return None

    def _trim_mitm_file(self, input_file: str, max_size_bytes: int = 5 * 1024 * 1024, window_minutes: int = 30) -> Optional[str]:
        """å°†mitmæŠ“åŒ…æ–‡ä»¶æŒ‰è§„åˆ™è£å‰ªï¼šè¶…è¿‡é˜ˆå€¼æ—¶ï¼Œä»…ä¿ç•™æœ€è¿‘window_minutesåˆ†é’Ÿå†…çš„æµé‡ï¼›
        å¦‚ä»è¶…è¿‡é˜ˆå€¼ï¼Œåˆ™ä»…ä¿ç•™æœ€è¿‘çš„éƒ¨åˆ†æµé‡ï¼Œä½¿æ–‡ä»¶ä¸å¤§äºé˜ˆå€¼ã€‚

        Returns: æ–°æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            from mitmproxy import io as mitm_io
            from mitmproxy import http as mitm_http
            import time as _time

            input_size = os.path.getsize(input_file)
            if input_size <= max_size_bytes:
                return input_file

            # è¯»å–å…¨éƒ¨flowså¹¶é™„å¸¦æ—¶é—´æˆ³
            flows: List[Tuple[float, Any]] = []
            with open(input_file, 'rb') as f:
                reader = mitm_io.FlowReader(f)
                for obj in reader.stream():
                    if isinstance(obj, mitm_http.HTTPFlow) and obj.response is not None:
                        ts = getattr(obj.request, 'timestamp_start', None)
                        if ts is None:
                            # é€€åŒ–åˆ°å“åº”ç»“æŸæ—¶é—´æˆ–å½“å‰æ—¶é—´
                            ts = getattr(obj.response, 'timestamp_end', _time.time())
                        flows.append((float(ts), obj))

            if not flows:
                return None

            # ä»…åœ¨åŸå§‹æ–‡ä»¶è¶…é™æ—¶åº”ç”¨æ—¶é—´çª—å£
            latest_ts = max(ts for ts, _ in flows)
            threshold = latest_ts - window_minutes * 60
            recent_flows = [flow for ts, flow in flows if ts >= threshold]

            # è‹¥æ—¶é—´çª—å£æ— æ•°æ®ï¼Œåˆ™ä¸åº”ç”¨çª—å£ï¼Œä½¿ç”¨å…¨éƒ¨flows
            candidate_flows = recent_flows if recent_flows else [flow for _, flow in flows]

            # å¦‚å€™é€‰å†™å‡ºåä»å¯èƒ½è¶…é™ï¼Œä¼°ç®—æ¯flowå¤§å°ï¼Œç¡®å®šæœ€å¤§N
            temp_dir = self.config['temp_dir']
            os.makedirs(temp_dir, exist_ok=True)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            sample_path = os.path.join(temp_dir, f"_trim_sample_{timestamp}.mitm")

            # é‡‡æ ·å‰Kæ¡è®¡ç®—å¹³å‡size
            sample_count = min(50, len(candidate_flows))
            if sample_count == 0:
                return None
            with open(sample_path, 'wb') as wf:
                writer = mitm_io.FlowWriter(wf)
                for flow in candidate_flows[:sample_count]:
                    writer.add(flow)
            sample_size = os.path.getsize(sample_path)
            try:
                os.remove(sample_path)
            except Exception:
                pass

            avg_per_flow = max(1, sample_size // sample_count)
            max_flows = max(1, max_size_bytes // avg_per_flow)

            # é€‰æ‹©æœ€è¿‘çš„max_flowsæ¡
            candidate_flows_sorted = sorted(candidate_flows, key=lambda fl: getattr(fl.request, 'timestamp_start', 0.0) or getattr(fl.response, 'timestamp_end', 0.0) or 0.0, reverse=True)
            selected = candidate_flows_sorted[:max_flows]

            # å†™å…¥æœ€ç»ˆæ–‡ä»¶ï¼ˆæŒ‰æ—¶é—´å‡åºå†™å…¥æ›´è´´è¿‘åŸå§‹é¡ºåºï¼‰
            final_flows = list(reversed(selected))
            out_path = os.path.join(temp_dir, f"flows_recent_{timestamp}.mitm")
            with open(out_path, 'wb') as wf:
                writer = mitm_io.FlowWriter(wf)
                for flow in final_flows:
                    writer.add(flow)

            # è‹¥ä¼°ç®—å¤±è¯¯å¯¼è‡´ä»è¶…é™ï¼Œåš1-2æ¬¡ç¼©å‡é‡å†™
            retries = 2
            while os.path.getsize(out_path) > max_size_bytes and len(final_flows) > 1 and retries > 0:
                shrink_to = max(1, int(len(final_flows) * 0.85))
                final_flows = final_flows[-shrink_to:]
                with open(out_path, 'wb') as wf:
                    writer = mitm_io.FlowWriter(wf)
                    for flow in final_flows:
                        writer.add(flow)
                retries -= 1

            # æœ€ç»ˆä¿éšœä¸è¶…è¿‡é˜ˆå€¼ï¼Œè‹¥ä»è¶…é™ï¼Œè¿”å›None
            if os.path.getsize(out_path) <= max_size_bytes:
                return out_path
            return None
        except Exception as e:
            logger.warning(f"è£å‰ªmitmæ–‡ä»¶å¤±è´¥: {e}")
            return None

    def run_feature_analysis(self, mitm_file: str) -> Optional[Dict[str, Any]]:
        """è¿è¡Œä¸¥æ ¼ç‰¹å¾åº“åˆ†æ

        Args:
            mitm_file: mitmæ–‡ä»¶è·¯å¾„

        Returns:
            åˆ†æç»“æœå­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            print(f"ğŸ” å¼€å§‹è¿è¡Œä¸¥æ ¼ç‰¹å¾åº“åˆ†æ...")
            print(f"   è¾“å…¥æ–‡ä»¶: {mitm_file}")

            # æ„å»ºåˆ†æç»“æœæ–‡ä»¶åï¼ˆä½¿ç”¨æ—¥æœŸï¼Œè¦†ç›–å†™ï¼‰
            date_str = datetime.now().strftime("%Y%m%d")
            analysis_file = os.path.join(self.config['output_dir'], f"feature_analysis_{date_str}.json")

            # è¿è¡Œintegrate_with_mitmproxy2swagger.pyè„šæœ¬
            script_path = current_dir.parent / "feature-library" / "plugins" / "integrate_with_mitmproxy2swagger.py"

            cmd = [
                sys.executable,
                str(script_path),
                "--mode", "direct",
                "--input", mitm_file,
                "--output", analysis_file
            ]

            print(f"   æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            logger.info(f"æ‰§è¡Œç‰¹å¾åº“åˆ†æ: {' '.join(cmd)}")

            # æé«˜è¶…æ—¶æ—¶é—´ï¼Œé¿å…å¤§æµé‡æˆ–å¤æ‚åˆ†æå¯¼è‡´çš„è¯¯åˆ¤å¤±è´¥
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=900)

            if result.returncode == 0:
                # æ£€æŸ¥åˆ†æç»“æœæ–‡ä»¶
                if os.path.exists(analysis_file):
                    with open(analysis_file, 'r', encoding='utf-8') as f:
                        analysis_data = json.load(f)

                    valuable_apis = analysis_data.get('valuable_apis', 0)
                    processed_flows = analysis_data.get('processed_flows', 0)

                    print(f"âœ… ç‰¹å¾åº“åˆ†æå®Œæˆ:")
                    print(f"   - å¤„ç†æµé‡: {processed_flows}")
                    print(f"   - æœ‰ä»·å€¼API: {valuable_apis}")
                    print(f"   - åˆ†ææ–‡ä»¶: {analysis_file}")

                    logger.info(f"ç‰¹å¾åº“åˆ†ææˆåŠŸ: {valuable_apis}/{processed_flows} APIs")

                    self.pipeline_state['analysis_result'] = {
                        'success': True,
                        'analysis_file': analysis_file,
                        'valuable_apis': valuable_apis,
                        'processed_flows': processed_flows,
                        'analysis_data': analysis_data
                    }
                    self.pipeline_state['output_files']['analysis'] = analysis_file
                    self.pipeline_state['steps_completed'].append('analysis')

                    return self.pipeline_state['analysis_result']
                else:
                    print(f"âŒ åˆ†æç»“æœæ–‡ä»¶æœªç”Ÿæˆ: {analysis_file}")
                    logger.error(f"åˆ†æç»“æœæ–‡ä»¶æœªç”Ÿæˆ: {analysis_file}")
                    return None
            else:
                error_msg = result.stderr if result.stderr else result.stdout
                print(f"âŒ ç‰¹å¾åº“åˆ†æå¤±è´¥:")
                print(f"   è¿”å›ç : {result.returncode}")
                print(f"   é”™è¯¯ä¿¡æ¯: {error_msg}")
                logger.error(f"ç‰¹å¾åº“åˆ†æå¤±è´¥: {result.returncode}, {error_msg}")
                return None

        except subprocess.TimeoutExpired:
            # è¶…æ—¶å…œåº•ï¼šè‹¥è¾“å‡ºæ–‡ä»¶å·²ç”Ÿæˆä¸”å¯è¯»ï¼Œåˆ™æŒ‰æˆåŠŸå¤„ç†
            print(f"âŒ ç‰¹å¾åº“åˆ†æè¶…æ—¶ï¼ˆ15åˆ†é’Ÿï¼‰")
            logger.error("ç‰¹å¾åº“åˆ†æè¶…æ—¶")
            try:
                date_str = datetime.now().strftime("%Y%m%d")
                analysis_file = os.path.join(self.config['output_dir'], f"feature_analysis_{date_str}.json")
                if os.path.exists(analysis_file) and os.path.getsize(analysis_file) > 0:
                    with open(analysis_file, 'r', encoding='utf-8') as f:
                        analysis_data = json.load(f)

                    valuable_apis = analysis_data.get('valuable_apis', 0)
                    processed_flows = analysis_data.get('processed_flows', 0)

                    print(f"âš ï¸  ä½¿ç”¨è¶…æ—¶å‰å·²ç”Ÿæˆçš„åˆ†æç»“æœç»§ç»­æµç¨‹")
                    print(f"   - å¤„ç†æµé‡: {processed_flows}")
                    print(f"   - æœ‰ä»·å€¼API: {valuable_apis}")
                    print(f"   - åˆ†ææ–‡ä»¶: {analysis_file}")

                    self.pipeline_state['analysis_result'] = {
                        'success': True,
                        'analysis_file': analysis_file,
                        'valuable_apis': valuable_apis,
                        'processed_flows': processed_flows,
                        'analysis_data': analysis_data
                    }
                    self.pipeline_state['output_files']['analysis'] = analysis_file
                    self.pipeline_state['steps_completed'].append('analysis')
                    return self.pipeline_state['analysis_result']
            except Exception as _:
                pass
            return None
        except Exception as e:
            print(f"âŒ ç‰¹å¾åº“åˆ†æå¼‚å¸¸: {e}")
            logger.error(f"ç‰¹å¾åº“åˆ†æå¼‚å¸¸: {e}")
            self.pipeline_state['errors'].append(f"ç‰¹å¾åº“åˆ†æå¼‚å¸¸: {e}")
            return None

    def run_simple_enhanced_learning(self, mitm_file: str) -> Optional[Dict[str, Any]]:
        """è¿è¡Œç®€åŒ–çš„å¢å¼ºå­¦ä¹ 

        Args:
            mitm_file: mitmæ–‡ä»¶è·¯å¾„

        Returns:
            å¢å¼ºå­¦ä¹ ç»“æœ
        """
        try:
            print(f"ğŸ§  å¼€å§‹å¢å¼ºå­¦ä¹ : {mitm_file}")

            # æ„å»ºå¢å¼ºå­¦ä¹ å‘½ä»¤
            learning_script = os.path.join(
                os.path.dirname(__file__),
                "..",
                "feature-library",
                "learning_engine",
                "enhanced_learning_pipeline.py"
            )

            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            learning_output = os.path.join(
                self.config['output_dir'],
                f"enhanced_learning_report_{timestamp}.json"
            )

            # æ„å»ºå‘½ä»¤
            cmd = [
                sys.executable,
                learning_script,
                "--input", mitm_file,
                "--output", learning_output
            ]

            print(f"ğŸ”§ æ‰§è¡Œå¢å¼ºå­¦ä¹ ...")

            # æ‰§è¡Œå¢å¼ºå­¦ä¹ 
            start_time = time.time()
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=180  # 3åˆ†é’Ÿè¶…æ—¶
            )

            execution_time = time.time() - start_time

            if result.returncode == 0:
                print(f"âœ… å¢å¼ºå­¦ä¹ å®Œæˆï¼Œè€—æ—¶: {execution_time:.1f}ç§’")

                # è¯»å–å­¦ä¹ ç»“æœ
                if os.path.exists(learning_output):
                    with open(learning_output, 'r', encoding='utf-8') as f:
                        learning_data = json.load(f)

                    # æå–å…³é”®ç»Ÿè®¡ä¿¡æ¯
                    stats = learning_data.get('pipeline_stats', {})
                    extraction = learning_data.get('extraction_summary', {})

                    learning_summary = {
                        'success': True,
                        'execution_time': execution_time,
                        'api_count': extraction.get('validated_count', 0),
                        'institution_count': len(extraction.get('institution_stats', {})),
                        'feature_library_updates': stats.get('feature_library_updates', 0),
                        'output_file': learning_output
                    }

                    print(f"ğŸ“Š å­¦ä¹ ç»Ÿè®¡: {stats.get('candidates_discovered', 0)} ä¸ªå€™é€‰API")
                    print(f"ğŸ“Š éªŒè¯é€šè¿‡: {extraction.get('validated_count', 0)} ä¸ªAPI")
                    print(f"ğŸ“Š ç‰¹å¾åº“æ›´æ–°: {stats.get('feature_library_updates', 0)} é¡¹")

                    return learning_summary
                else:
                    print("âš ï¸  å¢å¼ºå­¦ä¹ è¾“å‡ºæ–‡ä»¶æœªç”Ÿæˆ")
                    return None
            else:
                print(f"âš ï¸  å¢å¼ºå­¦ä¹ æ‰§è¡Œå¤±è´¥: {result.stderr}")
                return None

        except subprocess.TimeoutExpired:
            print("âš ï¸  å¢å¼ºå­¦ä¹ æ‰§è¡Œè¶…æ—¶ï¼ˆ3åˆ†é’Ÿï¼‰")
            return None

        except Exception as e:
            print(f"âš ï¸  å¢å¼ºå­¦ä¹ å¼‚å¸¸: {e}")
            return None

    def run_provider_builder(self, mitm_file: str, analysis_file: str) -> Optional[Dict[str, Any]]:
        """è¿è¡ŒProvideræ„å»ºå™¨

        Args:
            mitm_file: mitmæ–‡ä»¶è·¯å¾„
            analysis_file: åˆ†æç»“æœæ–‡ä»¶è·¯å¾„

        Returns:
            æ„å»ºç»“æœå­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            print(f"ğŸ—ï¸  å¼€å§‹è¿è¡ŒProvideræ„å»ºå™¨...")
            print(f"   mitmæ–‡ä»¶: {mitm_file}")
            print(f"   åˆ†ææ–‡ä»¶: {analysis_file}")

            # è¿è¡Œprovider_builder.pyè„šæœ¬
            script_path = current_dir / "provider_builder.py"

            cmd = [
                sys.executable,
                str(script_path),
                "--mitm-file", mitm_file,
                "--analysis-file", analysis_file,
                "--output-dir", self.config['output_dir']
            ]

            print(f"   æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            logger.info(f"æ‰§è¡ŒProvideræ„å»º: {' '.join(cmd)}")

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)

            if result.returncode == 0:
                # è§£æè¾“å‡ºï¼ŒæŸ¥æ‰¾ç”Ÿæˆçš„æ–‡ä»¶
                output_lines = result.stdout.split('\n')
                providers_file = None
                questionable_file = None

                for line in output_lines:
                    if 'Providers:' in line and '.json' in line:
                        providers_file = line.split('Providers:')[-1].strip()
                    elif 'å­˜ç–‘APIs:' in line and '.json' in line:
                        questionable_file = line.split('å­˜ç–‘APIs:')[-1].strip()

                # éªŒè¯æ–‡ä»¶å­˜åœ¨
                providers_count = 0
                questionable_count = 0

                if providers_file and os.path.exists(providers_file):
                    try:
                        with open(providers_file, 'r', encoding='utf-8') as f:
                            providers_data = json.load(f)
                        providers_count = providers_data.get('metadata', {}).get('total_providers', 0)
                    except Exception as e:
                        logger.warning(f"è¯»å–providersæ–‡ä»¶å¤±è´¥: {e}")

                if questionable_file and os.path.exists(questionable_file):
                    try:
                        with open(questionable_file, 'r', encoding='utf-8') as f:
                            questionable_data = json.load(f)
                        questionable_count = questionable_data.get('metadata', {}).get('total_questionable', 0)
                    except Exception as e:
                        logger.warning(f"è¯»å–questionableæ–‡ä»¶å¤±è´¥: {e}")

                print(f"âœ… Provideræ„å»ºå®Œæˆ:")
                print(f"   - æˆåŠŸæ„å»º: {providers_count} ä¸ªproviders")
                print(f"   - å­˜ç–‘API: {questionable_count} ä¸ª")
                print(f"   - Providersæ–‡ä»¶: {providers_file}")
                print(f"   - å­˜ç–‘æ–‡ä»¶: {questionable_file}")

                logger.info(f"Provideræ„å»ºæˆåŠŸ: {providers_count} providers, {questionable_count} questionable")

                self.pipeline_state['provider_result'] = {
                    'success': True,
                    'providers_file': providers_file,
                    'questionable_file': questionable_file,
                    'providers_count': providers_count,
                    'questionable_count': questionable_count
                }

                if providers_file:
                    self.pipeline_state['output_files']['providers'] = providers_file
                if questionable_file:
                    self.pipeline_state['output_files']['questionable'] = questionable_file

                self.pipeline_state['steps_completed'].append('provider_build')

                return self.pipeline_state['provider_result']
            else:
                error_msg = result.stderr if result.stderr else result.stdout
                print(f"âŒ Provideræ„å»ºå¤±è´¥:")
                print(f"   è¿”å›ç : {result.returncode}")
                print(f"   é”™è¯¯ä¿¡æ¯: {error_msg}")
                logger.error(f"Provideræ„å»ºå¤±è´¥: {result.returncode}, {error_msg}")
                return None

        except subprocess.TimeoutExpired:
            print(f"âŒ Provideræ„å»ºè¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰")
            logger.error("Provideræ„å»ºè¶…æ—¶")
            return None
        except Exception as e:
            print(f"âŒ Provideræ„å»ºå¼‚å¸¸: {e}")
            logger.error(f"Provideræ„å»ºå¼‚å¸¸: {e}")
            self.pipeline_state['errors'].append(f"Provideræ„å»ºå¼‚å¸¸: {e}")
            return None

    def generate_pipeline_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´çš„æµç¨‹æ‰§è¡ŒæŠ¥å‘Š"""
        end_time = datetime.now()
        execution_duration = (end_time - self.pipeline_state['start_time']).total_seconds()

        # åŸºç¡€æŠ¥å‘Šä¿¡æ¯
        report = {
            "pipeline_metadata": {
                "execution_start": self.pipeline_state['start_time'].isoformat(),
                "execution_end": end_time.isoformat(),
                "execution_duration_seconds": execution_duration,
                "pipeline_version": "2.0.0",
                "integrated_scripts": [
                    "integrate_with_mitmproxy2swagger.py",
                    "provider_builder.py"
                ]
            },
            "configuration": {
                "mitm_host": self.config['mitm_host'],
                "mitm_port": self.config['mitm_port'],
                "output_dir": self.config['output_dir'],
                "temp_dir": self.config['temp_dir']
            },
            "execution_status": {
                "mitm_status": self.pipeline_state['mitm_status'],
                "steps_completed": self.pipeline_state['steps_completed'],
                "steps_total": ["export", "analysis", "provider_build"],
                "success_rate": len(self.pipeline_state['steps_completed']) / 3,
                "errors": self.pipeline_state['errors']
            },
            "results": {
                "export_file": self.pipeline_state['export_file'],
                "output_files": self.pipeline_state['output_files']
            }
        }

        # æ·»åŠ åˆ†æç»“æœ - å®‰å…¨æ£€æŸ¥
        if self.pipeline_state and self.pipeline_state.get('analysis_result'):
            analysis = self.pipeline_state['analysis_result']
            report["analysis_results"] = {
                "success": analysis.get('success', False),
                "processed_flows": analysis.get('processed_flows', 0),
                "valuable_apis": analysis.get('valuable_apis', 0),
                "analysis_file": analysis.get('analysis_file', ''),
                "identification_rate": (analysis.get('valuable_apis', 0) / max(analysis.get('processed_flows', 1), 1)) * 100
            }

        # æ·»åŠ Provideræ„å»ºç»“æœ - å®‰å…¨æ£€æŸ¥
        if self.pipeline_state and self.pipeline_state.get('provider_result'):
            provider = self.pipeline_state['provider_result']
            report["provider_results"] = {
                "success": provider.get('success', False),
                "providers_count": provider.get('providers_count', 0),
                "questionable_count": provider.get('questionable_count', 0),
                "providers_file": provider.get('providers_file', ''),
                "questionable_file": provider.get('questionable_file', ''),
                "success_rate": provider.get('providers_count', 0) / max(
                    provider.get('providers_count', 0) + provider.get('questionable_count', 0), 1
                ) * 100
            }

        # è®¡ç®—æ•´ä½“æˆåŠŸçŠ¶æ€ - å®‰å…¨æ£€æŸ¥pipeline_state
        if self.pipeline_state:
            steps_completed = len(self.pipeline_state.get('steps_completed', []))
            errors_count = len(self.pipeline_state.get('errors', []))
            provider_result = self.pipeline_state.get('provider_result', {})
            provider_success = provider_result.get('success', False) if provider_result else False

            overall_success = (
                steps_completed >= 2 and  # è‡³å°‘å®Œæˆåˆ†æå’Œæ„å»º
                errors_count == 0 and
                provider_success
            )

            completion_percentage = (steps_completed / 3) * 100
            final_providers_generated = provider_result.get('providers_count', 0) if provider_result else 0
        else:
            overall_success = False
            completion_percentage = 0
            final_providers_generated = 0

        report["overall_status"] = {
            "success": overall_success,
            "completion_percentage": completion_percentage,
            "final_providers_generated": final_providers_generated
        }

        return report

    def save_pipeline_report(self, report: Dict[str, Any]) -> str:
        """ä¿å­˜æµç¨‹æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        date_str = datetime.now().strftime("%Y%m%d")
        report_file = os.path.join(self.config['output_dir'], f"integrated_pipeline_report_{date_str}.json")

        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)

            print(f"ğŸ“Š æµç¨‹æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            logger.info(f"æµç¨‹æŠ¥å‘Šä¿å­˜æˆåŠŸ: {report_file}")
            return report_file

        except Exception as e:
            print(f"âš ï¸  ä¿å­˜æµç¨‹æŠ¥å‘Šå¤±è´¥: {e}")
            logger.error(f"ä¿å­˜æµç¨‹æŠ¥å‘Šå¤±è´¥: {e}")
            return ""

    def print_final_summary(self, report: Dict[str, Any]):
        """æ‰“å°æœ€ç»ˆæ‰§è¡Œæ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ‰ é›†æˆä¸»æµç¨‹æ‰§è¡Œå®Œæˆ!")
        print("=" * 80)

        # æ‰§è¡ŒçŠ¶æ€
        overall = report.get('overall_status', {})
        if overall.get('success'):
            print(f"âœ… æ•´ä½“çŠ¶æ€: æˆåŠŸ")
        else:
            print(f"âš ï¸  æ•´ä½“çŠ¶æ€: éƒ¨åˆ†æˆåŠŸæˆ–å¤±è´¥")

        print(f"ğŸ“Š å®Œæˆåº¦: {overall.get('completion_percentage', 0):.1f}%")
        print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {report.get('pipeline_metadata', {}).get('execution_duration_seconds', 0):.1f}ç§’")

        # åˆ†æç»“æœ
        analysis = report.get('analysis_results', {})
        if analysis:
            print(f"\nğŸ” ç‰¹å¾åº“åˆ†æç»“æœ:")
            print(f"   - å¤„ç†æµé‡: {analysis.get('processed_flows', 0)}")
            print(f"   - æœ‰ä»·å€¼API: {analysis.get('valuable_apis', 0)}")
            print(f"   - è¯†åˆ«ç‡: {analysis.get('identification_rate', 0):.1f}%")

        # Provideræ„å»ºç»“æœ
        provider = report.get('provider_results', {})
        if provider:
            print(f"\nğŸ—ï¸  Provideræ„å»ºç»“æœ:")
            print(f"   - æˆåŠŸæ„å»º: {provider.get('providers_count', 0)} ä¸ª")
            print(f"   - å­˜ç–‘API: {provider.get('questionable_count', 0)} ä¸ª")
            print(f"   - æˆåŠŸç‡: {provider.get('success_rate', 0):.1f}%")

        # è¾“å‡ºæ–‡ä»¶
        output_files = report.get('results', {}).get('output_files', {})
        if output_files:
            print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
            for file_type, file_path in output_files.items():
                print(f"   - {file_type}: {file_path}")

        # é”™è¯¯ä¿¡æ¯
        errors = report.get('execution_status', {}).get('errors', [])
        if errors:
            print(f"\nâš ï¸  æ‰§è¡Œè¿‡ç¨‹ä¸­çš„é”™è¯¯:")
            for error in errors:
                print(f"   - {error}")

        print("=" * 80)

    def run_full_pipeline(self, offline_mode: bool = False, input_file: str = None) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„é›†æˆä¸»æµç¨‹

        Args:
            offline_mode: ç¦»çº¿æ¨¡å¼ï¼Œä¸æ£€æµ‹mitmä»£ç†
            input_file: æŒ‡å®šè¾“å…¥æ–‡ä»¶ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰

        Returns:
            æµç¨‹æ‰§è¡Œç»“æœ
        """
        print("ğŸš€ å¯åŠ¨é›†æˆä¸»æµç¨‹ - ç‰¹å¾åº“åˆ†æ + Provideræ„å»º")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.config['output_dir']}")
        print("=" * 80)

        logger.info("é›†æˆä¸»æµç¨‹å¼€å§‹æ‰§è¡Œ")

        # æ­¥éª¤1: æ£€æµ‹mitmä»£ç†å’Œå¯¼å‡ºæ•°æ®ï¼ˆå¦‚æœä¸æ˜¯ç¦»çº¿æ¨¡å¼ï¼‰
        mitm_file = None

        if not offline_mode:
            print("ğŸ“¡ æ­¥éª¤1: æ£€æµ‹mitmä»£ç†çŠ¶æ€")
            mitm_running, status_msg = self.check_mitm_proxy_status()

            if not mitm_running:
                print(f"ğŸ’¡ æç¤º: å¦‚éœ€ç¦»çº¿æ¨¡å¼ï¼Œè¯·ä½¿ç”¨ --offline --input-file <file>")
                return {
                    "success": False,
                    "message": "mitmä»£ç†æœªè¿è¡Œï¼Œæ— æ³•å¯¼å‡ºæ•°æ®",
                    "report": self.generate_pipeline_report()
                }

            print("\nğŸ“¥ æ­¥éª¤2: å¯¼å‡ºæµé‡æ•°æ®")
            mitm_file = self.export_mitm_flows()
            if not mitm_file:
                return {
                    "success": False,
                    "message": "æµé‡æ•°æ®å¯¼å‡ºå¤±è´¥",
                    "report": self.generate_pipeline_report()
                }
        else:
            # ç¦»çº¿æ¨¡å¼ï¼Œä½¿ç”¨æŒ‡å®šçš„è¾“å…¥æ–‡ä»¶
            if not input_file or not os.path.exists(input_file):
                return {
                    "success": False,
                    "message": f"ç¦»çº¿æ¨¡å¼ä¸‹æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}",
                    "report": self.generate_pipeline_report()
                }

            mitm_file = input_file
            print(f"ğŸ“„ ç¦»çº¿æ¨¡å¼ï¼Œä½¿ç”¨è¾“å…¥æ–‡ä»¶: {mitm_file}")
            self.pipeline_state['export_file'] = mitm_file
            self.pipeline_state['steps_completed'].append('export')

        print("\n" + "-" * 80)

        # æ­¥éª¤2.5: å‰ç½®å¢å¼ºå­¦ä¹ ï¼ˆæ–°å¢ï¼‰
        print("ğŸ§  æ­¥éª¤2.5: å‰ç½®å¢å¼ºå­¦ä¹  - å­¦ä¹ æŠ“åŒ…æ–‡ä»¶ä¸­çš„APIæ¨¡å¼")
        learning_result = self.run_simple_enhanced_learning(mitm_file)
        if learning_result and learning_result.get('success'):
            print(f"âœ… å¢å¼ºå­¦ä¹ å®Œæˆ: å‘ç° {learning_result.get('api_count', 0)} ä¸ªAPIï¼Œæ›´æ–°ç‰¹å¾åº“")
        else:
            print("âš ï¸  å¢å¼ºå­¦ä¹ å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸæœ‰ç‰¹å¾åº“")

        print("\n" + "-" * 80)

        # æ­¥éª¤3: è¿è¡Œç‰¹å¾åº“åˆ†æ
        print("ğŸ” æ­¥éª¤3: è¿è¡Œä¸¥æ ¼ç‰¹å¾åº“åˆ†æ")
        analysis_result = self.run_feature_analysis(mitm_file)
        if not analysis_result:
            return {
                "success": False,
                "message": "ç‰¹å¾åº“åˆ†æå¤±è´¥",
                "report": self.generate_pipeline_report()
            }

        print("\n" + "-" * 80)

        # æ­¥éª¤3/4: è¿è¡ŒProvideræ„å»º
        print("ğŸ—ï¸  æ­¥éª¤4: è¿è¡ŒProvideræ„å»ºå™¨")
        provider_result = self.run_provider_builder(mitm_file, analysis_result['analysis_file'])
        if not provider_result:
            return {
                "success": False,
                "message": "Provideræ„å»ºå¤±è´¥",
                "report": self.generate_pipeline_report()
            }

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = self.generate_pipeline_report()
        report_file = self.save_pipeline_report(final_report)

        # æ·»åŠ æŠ¥å‘Šæ–‡ä»¶åˆ°è¾“å‡ºæ–‡ä»¶åˆ—è¡¨
        if report_file:
            self.pipeline_state['output_files']['report'] = report_file
            final_report['results']['output_files']['report'] = report_file

        # æ‰“å°æœ€ç»ˆæ‘˜è¦
        self.print_final_summary(final_report)

        logger.info("é›†æˆä¸»æµç¨‹æ‰§è¡Œå®Œæˆ")

        return {
            "success": True,
            "message": "é›†æˆä¸»æµç¨‹æ‰§è¡ŒæˆåŠŸ",
            "report": final_report,
            "output_files": self.pipeline_state['output_files']
        }


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(
        description='é›†æˆä¸»æµç¨‹ - ç‰¹å¾åº“åˆ†æ + Provideræ„å»º',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åœ¨çº¿æ¨¡å¼ï¼ˆè‡ªåŠ¨æ£€æµ‹mitmä»£ç†å¹¶å¯¼å‡ºæ•°æ®ï¼‰
  python3 integrated_main_pipeline.py

  # æŒ‡å®šmitmä»£ç†åœ°å€
  python3 integrated_main_pipeline.py --mitm-host 10.10.10.146 --mitm-port 8082

  # ç¦»çº¿æ¨¡å¼ï¼ˆä½¿ç”¨ç°æœ‰çš„mitmæ–‡ä»¶ï¼‰
  python3 integrated_main_pipeline.py --offline --input-file flows.mitm

  # æŒ‡å®šè¾“å‡ºç›®å½•
  python3 integrated_main_pipeline.py --output-dir ./results
        """
    )

    # åŸºç¡€å‚æ•°
    parser.add_argument('--mitm-host', default='127.0.0.1',
                       help='mitmproxyä¸»æœºåœ°å€ (é»˜è®¤: 127.0.0.1)')
    parser.add_argument('--mitm-port', type=int, default=8080,
                       help='mitmproxyç«¯å£ (é»˜è®¤: 8080)')
    parser.add_argument('--output-dir', default='data',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: data)')
    parser.add_argument('--temp-dir', default='temp',
                       help='ä¸´æ—¶ç›®å½• (é»˜è®¤: temp)')

    # ç¦»çº¿æ¨¡å¼å‚æ•°
    parser.add_argument('--offline', action='store_true',
                       help='ç¦»çº¿æ¨¡å¼ï¼Œä¸æ£€æµ‹mitmä»£ç†')
    parser.add_argument('--input-file', '-i',
                       help='ç¦»çº¿æ¨¡å¼ä¸‹çš„è¾“å…¥mitmæ–‡ä»¶è·¯å¾„')

    # å…¶ä»–é€‰é¡¹
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='è¯¦ç»†è¾“å‡ºæ¨¡å¼')

    args = parser.parse_args()

    # éªŒè¯å‚æ•°
    if args.offline and not args.input_file:
        print("âŒ ç¦»çº¿æ¨¡å¼ä¸‹å¿…é¡»æŒ‡å®š --input-file å‚æ•°")
        parser.print_help()
        sys.exit(1)

    if args.offline and args.input_file and not os.path.exists(args.input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
        sys.exit(1)

    # æ„å»ºé…ç½®
    config = {
        'mitm_host': args.mitm_host,
        'mitm_port': args.mitm_port,
        'output_dir': args.output_dir,
        'temp_dir': args.temp_dir
    }

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    try:
        # åˆ›å»ºå¹¶è¿è¡Œé›†æˆæµç¨‹
        pipeline = IntegratedMainPipeline(config)
        result = pipeline.run_full_pipeline(
            offline_mode=args.offline,
            input_file=args.input_file
        )

        # æ ¹æ®ç»“æœè®¾ç½®é€€å‡ºç 
        if result['success']:
            print(f"\nğŸ‰ é›†æˆä¸»æµç¨‹æ‰§è¡ŒæˆåŠŸ!")
            if 'output_files' in result:
                print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
                for file_type, file_path in result['output_files'].items():
                    print(f"   - {file_type}: {file_path}")
            sys.exit(0)
        else:
            print(f"\nâŒ é›†æˆä¸»æµç¨‹æ‰§è¡Œå¤±è´¥: {result['message']}")
            sys.exit(1)

    except KeyboardInterrupt:
        print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(130)
    except Exception as e:
        print(f"\nğŸ’¥ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        logger.error(f"æ‰§è¡Œå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
